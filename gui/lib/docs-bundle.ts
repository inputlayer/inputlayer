// AUTO-GENERATED — do not edit. Run "node scripts/bundle-docs.mjs" to regenerate.

export interface TocEntry {
  level: number
  text: string
  id: string
}

export interface NavItem {
  key: string
  label: string
  slug: string
  href: string
  children: NavItem[]
}

export interface DocPage {
  title: string
  content: string
  toc: TocEntry[]
}

export const docsNavigation: NavItem[] = [
  {
    "key": "index",
    "label": "Overview",
    "slug": "index",
    "href": "/docs/index",
    "children": []
  },
  {
    "key": "guides",
    "label": "Guides",
    "slug": "guides",
    "href": "/docs/guides",
    "children": [
      {
        "key": "quickstart",
        "label": "Quick Start",
        "slug": "guides/quickstart",
        "href": "/docs/guides/quickstart",
        "children": []
      },
      {
        "key": "installation",
        "label": "Installation",
        "slug": "guides/installation",
        "href": "/docs/guides/installation",
        "children": []
      },
      {
        "key": "first-program",
        "label": "Your First Program",
        "slug": "guides/first-program",
        "href": "/docs/guides/first-program",
        "children": []
      },
      {
        "key": "core-concepts",
        "label": "Data Modeling",
        "slug": "guides/core-concepts",
        "href": "/docs/guides/core-concepts",
        "children": []
      },
      {
        "key": "repl",
        "label": "REPL Guide",
        "slug": "guides/repl",
        "href": "/docs/guides/repl",
        "children": []
      },
      {
        "key": "recursion",
        "label": "Recursion",
        "slug": "guides/recursion",
        "href": "/docs/guides/recursion",
        "children": []
      },
      {
        "key": "vectors",
        "label": "Vector Search",
        "slug": "guides/vectors",
        "href": "/docs/guides/vectors",
        "children": []
      },
      {
        "key": "indexing",
        "label": "Indexing",
        "slug": "guides/indexing",
        "href": "/docs/guides/indexing",
        "children": []
      },
      {
        "key": "temporal",
        "label": "Temporal Reasoning",
        "slug": "guides/temporal",
        "href": "/docs/guides/temporal",
        "children": []
      },
      {
        "key": "persistence",
        "label": "Persistence",
        "slug": "guides/persistence",
        "href": "/docs/guides/persistence",
        "children": []
      },
      {
        "key": "configuration",
        "label": "Configuration",
        "slug": "guides/configuration",
        "href": "/docs/guides/configuration",
        "children": []
      },
      {
        "key": "rest-api",
        "label": "REST API",
        "slug": "guides/rest-api",
        "href": "/docs/guides/rest-api",
        "children": []
      },
      {
        "key": "python-sdk",
        "label": "Python SDK",
        "slug": "guides/python-sdk",
        "href": "/docs/guides/python-sdk",
        "children": []
      },
      {
        "key": "deployment",
        "label": "Deployment",
        "slug": "guides/deployment",
        "href": "/docs/guides/deployment",
        "children": []
      },
      {
        "key": "authentication",
        "label": "Authentication",
        "slug": "guides/authentication",
        "href": "/docs/guides/authentication",
        "children": []
      },
      {
        "key": "websocket-api",
        "label": "WebSocket API",
        "slug": "guides/websocket-api",
        "href": "/docs/guides/websocket-api",
        "children": []
      },
      {
        "key": "migrations",
        "label": "Migrations",
        "slug": "guides/migrations",
        "href": "/docs/guides/migrations",
        "children": []
      },
      {
        "key": "troubleshooting",
        "label": "Troubleshooting",
        "slug": "guides/troubleshooting",
        "href": "/docs/guides/troubleshooting",
        "children": []
      }
    ]
  },
  {
    "key": "reference",
    "label": "Reference",
    "slug": "reference",
    "href": "/docs/reference",
    "children": [
      {
        "key": "index",
        "label": "Overview",
        "slug": "reference/index",
        "href": "/docs/reference/index",
        "children": []
      },
      {
        "key": "commands",
        "label": "Commands",
        "slug": "reference/commands",
        "href": "/docs/reference/commands",
        "children": []
      },
      {
        "key": "functions",
        "label": "Functions",
        "slug": "reference/functions",
        "href": "/docs/reference/functions",
        "children": []
      },
      {
        "key": "syntax",
        "label": "Syntax",
        "slug": "reference/syntax",
        "href": "/docs/reference/syntax",
        "children": []
      },
      {
        "key": "syntax-cheatsheet",
        "label": "Syntax Cheatsheet",
        "slug": "reference/syntax-cheatsheet",
        "href": "/docs/reference/syntax-cheatsheet",
        "children": []
      },
      {
        "key": "aggregations",
        "label": "Aggregations",
        "slug": "reference/aggregations",
        "href": "/docs/reference/aggregations",
        "children": []
      }
    ]
  },
  {
    "key": "spec",
    "label": "Specification",
    "slug": "spec",
    "href": "/docs/spec",
    "children": [
      {
        "key": "index",
        "label": "Overview",
        "slug": "spec/index",
        "href": "/docs/spec/index",
        "children": []
      },
      {
        "key": "basic-concepts",
        "label": "Basic Concepts",
        "slug": "spec/basic-concepts",
        "href": "/docs/spec/basic-concepts",
        "children": []
      },
      {
        "key": "types",
        "label": "Types",
        "slug": "spec/types",
        "href": "/docs/spec/types",
        "children": []
      },
      {
        "key": "rules",
        "label": "Rules",
        "slug": "spec/rules",
        "href": "/docs/spec/rules",
        "children": []
      },
      {
        "key": "queries",
        "label": "Queries",
        "slug": "spec/queries",
        "href": "/docs/spec/queries",
        "children": []
      },
      {
        "key": "grammar",
        "label": "Grammar",
        "slug": "spec/grammar",
        "href": "/docs/spec/grammar",
        "children": []
      },
      {
        "key": "errors",
        "label": "Errors",
        "slug": "spec/errors",
        "href": "/docs/spec/errors",
        "children": []
      }
    ]
  },
  {
    "key": "internals",
    "label": "Internals",
    "slug": "internals",
    "href": "/docs/internals",
    "children": [
      {
        "key": "index",
        "label": "Overview",
        "slug": "internals/index",
        "href": "/docs/internals/index",
        "children": []
      },
      {
        "key": "architecture",
        "label": "Architecture",
        "slug": "internals/architecture",
        "href": "/docs/internals/architecture",
        "children": []
      },
      {
        "key": "coding-standards",
        "label": "Coding Standards",
        "slug": "internals/coding-standards",
        "href": "/docs/internals/coding-standards",
        "children": []
      },
      {
        "key": "type-system",
        "label": "Type System",
        "slug": "internals/type-system",
        "href": "/docs/internals/type-system",
        "children": []
      },
      {
        "key": "validation",
        "label": "Validation",
        "slug": "internals/validation",
        "href": "/docs/internals/validation",
        "children": []
      },
      {
        "key": "record-syntax",
        "label": "Record Syntax",
        "slug": "internals/record-syntax",
        "href": "/docs/internals/record-syntax",
        "children": []
      },
      {
        "key": "roadmap",
        "label": "Roadmap",
        "slug": "internals/roadmap",
        "href": "/docs/internals/roadmap",
        "children": []
      }
    ]
  }
]

export const docsPages: Record<string, DocPage> = {
  "guides/authentication": {
    "title": "Authentication",
    "content": "# Authentication\n\nInputLayer requires authentication for all connections. The server bootstraps an admin user on first start.\n\n## Bootstrap Admin\n\nOn first launch, the server creates a default admin account:\n\n- **Username**: `admin`\n- **Password**: Set via `INPUTLAYER_ADMIN_PASSWORD` environment variable, or auto-generated (printed to stderr) if unset\n\n> **Important**: Always set `INPUTLAYER_ADMIN_PASSWORD` in production deployments.\n\n```bash\nINPUTLAYER_ADMIN_PASSWORD=my-secure-password inputlayer-server\n```\n\nOr in configuration:\n\n```toml\n[http.auth]\nbootstrap_admin_password = \"my-secure-password\"\n```\n\n## User Management\n\nManage users via REPL commands (requires admin privileges):\n\n### Create User\n\n```\n.user create alice secretpassword viewer\n```\n\nRoles: `admin`, `editor`, `viewer`\n\n### List Users\n\n```\n.user list\n```\n\n### Delete User\n\n```\n.user drop alice\n```\n\n### Change Password\n\n```\n.user password alice new-secret\n```\n\n### Change Role\n\n```\n.user role alice editor\n```\n\n## Knowledge Graph Access Control\n\nControl which users can access which knowledge graphs.\n\n### Grant Access\n\n```\n.kg acl grant mydb alice viewer\n.kg acl grant mydb alice editor\n.kg acl grant mydb alice owner\n```\n\n### Revoke Access\n\n```\n.kg acl revoke mydb alice\n```\n\n### List ACLs\n\n```\n.kg acl list mydb\n```\n\n### Permission Levels\n\n| Level | Capabilities |\n|-------|-------------|\n| `viewer` | Query data, list relations and rules |\n| `editor` | Insert/delete facts, create/modify rules, manage indexes |\n| `owner` | Drop knowledge graphs, manage ACLs for this KG |\n\nPermissions are cumulative — `editor` includes `viewer`, `owner` includes `editor`.\n\n## WebSocket Authentication\n\nEvery WebSocket connection must authenticate before sending queries.\n\n### Auth Handshake\n\nAfter connecting to `ws://host:port/ws`, the first message must be an auth request:\n\n```json\n{\n  \"type\": \"login\",\n  \"username\": \"admin\",\n  \"password\": \"admin\"\n}\n```\n\nOr with an API key:\n\n```json\n{\n  \"type\": \"authenticate\",\n  \"api_key\": \"your-api-key\"\n}\n```\n\nThe server responds with:\n\n```json\n{\n  \"type\": \"authenticated\",\n  \"session_id\": \"a1b2c3d4\",\n  \"knowledge_graph\": \"default\",\n  \"version\": \"0.1.0\",\n  \"role\": \"admin\"\n}\n```\n\nOr on failure:\n\n```json\n{\n  \"type\": \"auth_error\",\n  \"message\": \"Invalid credentials\"\n}\n```\n\nThe server allows 30 seconds for authentication. Any non-auth message before authentication results in disconnection.\n\n## REST API Authentication\n\nREST endpoints (except health/live/ready) require a Bearer token:\n\n```bash\ncurl -H \"Authorization: Bearer your-api-key\" http://localhost:8080/metrics\n```\n\n## API Keys\n\nAPI keys provide machine-to-machine authentication without passwords.\n\n### Create API Key\n\n```\n.apikey create my-service-key\n```\n\n### List API Keys\n\n```\n.apikey list\n```\n\n### Revoke API Key\n\n```\n.apikey revoke my-service-key\n```\n\nYou can also set an API key for the CLI client via environment variable:\n\n```bash\nexport INPUTLAYER_API_KEY=your-api-key\ninputlayer-client\n```\n\n## Python SDK Authentication\n\n```python\nfrom inputlayer import InputLayer\n\n# Username/password\nasync with InputLayer(\"ws://localhost:8080/ws\", username=\"admin\", password=\"admin\") as il:\n    ...\n\n# API key\nasync with InputLayer(\"ws://localhost:8080/ws\", api_key=\"your-key\") as il:\n    ...\n```\n\n## Security Best Practices\n\n1. **Change the default admin password** immediately after deployment\n2. **Use TLS** via a reverse proxy (see [Deployment](deployment)) — credentials are sent in plaintext over WebSocket\n3. **Create per-user accounts** rather than sharing the admin account\n4. **Use API keys** for automated services and CI/CD pipelines\n5. **Grant minimum required permissions** — use `viewer` access for dashboards, `editor` for ETL jobs\n6. **Bind to localhost** when using a reverse proxy: set `host = \"127.0.0.1\"` in config\n7. **Rotate API keys** periodically and revoke unused ones",
    "toc": [
      {
        "level": 2,
        "text": "Bootstrap Admin",
        "id": "bootstrap-admin"
      },
      {
        "level": 2,
        "text": "User Management",
        "id": "user-management"
      },
      {
        "level": 3,
        "text": "Create User",
        "id": "create-user"
      },
      {
        "level": 3,
        "text": "List Users",
        "id": "list-users"
      },
      {
        "level": 3,
        "text": "Delete User",
        "id": "delete-user"
      },
      {
        "level": 3,
        "text": "Change Password",
        "id": "change-password"
      },
      {
        "level": 3,
        "text": "Change Role",
        "id": "change-role"
      },
      {
        "level": 2,
        "text": "Knowledge Graph Access Control",
        "id": "knowledge-graph-access-control"
      },
      {
        "level": 3,
        "text": "Grant Access",
        "id": "grant-access"
      },
      {
        "level": 3,
        "text": "Revoke Access",
        "id": "revoke-access"
      },
      {
        "level": 3,
        "text": "List ACLs",
        "id": "list-acls"
      },
      {
        "level": 3,
        "text": "Permission Levels",
        "id": "permission-levels"
      },
      {
        "level": 2,
        "text": "WebSocket Authentication",
        "id": "websocket-authentication"
      },
      {
        "level": 3,
        "text": "Auth Handshake",
        "id": "auth-handshake"
      },
      {
        "level": 2,
        "text": "REST API Authentication",
        "id": "rest-api-authentication"
      },
      {
        "level": 2,
        "text": "API Keys",
        "id": "api-keys"
      },
      {
        "level": 3,
        "text": "Create API Key",
        "id": "create-api-key"
      },
      {
        "level": 3,
        "text": "List API Keys",
        "id": "list-api-keys"
      },
      {
        "level": 3,
        "text": "Revoke API Key",
        "id": "revoke-api-key"
      },
      {
        "level": 2,
        "text": "Python SDK Authentication",
        "id": "python-sdk-authentication"
      },
      {
        "level": 2,
        "text": "Security Best Practices",
        "id": "security-best-practices"
      }
    ]
  },
  "guides/configuration": {
    "title": "Configuration Guide",
    "content": "# Configuration Guide\n\nInputLayer uses a hierarchical configuration system with multiple sources:\n\n1. `config.toml` - Default configuration file\n2. `config.local.toml` - Local overrides (git-ignored)\n3. Environment variables (`INPUTLAYER_*` prefix)\n\n## Configuration File Locations\n\nInputLayer looks for config files in this order:\n1. `./config.toml` (current directory)\n2. `./config.local.toml` (local overrides, git-ignored)\n3. `--config <path>` (CLI flag for custom path)\n\nEnvironment variables (`INPUTLAYER_*` prefix) are merged on top of file configuration.\n\n## Complete Configuration Reference\n\n```toml\n# =============================================================================\n# STORAGE CONFIGURATION\n# =============================================================================\n[storage]\n# Base directory for all knowledge graph data\ndata_dir = \"./data\"\n\n# Default knowledge graph name (created on startup)\ndefault_knowledge_graph = \"default\"\n\n# Automatically create knowledge graphs when accessed\nauto_create_knowledge_graphs = false\n\n# Maximum number of knowledge graphs (default: 1000)\nmax_knowledge_graphs = 1000\n\n# -----------------------------------------------------------------------------\n# Legacy Persistence Settings\n# -----------------------------------------------------------------------------\n[storage.persistence]\n# Storage format: parquet, csv, bincode\n# - parquet: Columnar format, good compression, recommended for production\n# - csv: Human-readable, no compression, good for debugging\n# - bincode: Binary format, Rust-specific, fast serialization\nformat = \"parquet\"\n\n# Compression: snappy, gzip, none\n# - snappy: Fast compression/decompression, good ratio\n# - gzip: Better compression ratio, slower\n# - none: No compression\ncompression = \"snappy\"\n\n# Auto-save interval in seconds (0 = manual save only)\nauto_save_interval = 0\n\n# Enable write-ahead logging for crash recovery\nenable_wal = true\n\n# -----------------------------------------------------------------------------\n# DD-Native Persist Layer (Recommended)\n# -----------------------------------------------------------------------------\n[storage.persist]\n# Enable the DD-native persistence layer\nenabled = true\n\n# Buffer size before flushing to disk (number of updates)\nbuffer_size = 10000\n\n# Durability mode: immediate, batched, async\n# - immediate: Sync to disk on each write (safest, slowest)\n# - batched: Periodic sync (balanced performance/safety)\n# - async: Fire-and-forget (fastest, may lose recent data on crash)\ndurability_mode = \"immediate\"\n\n# Compaction window (reserved, not yet implemented; 0 = keep all)\ncompaction_window = 0\n\n# Maximum WAL size in bytes before forced flush (default: 64 MB)\nmax_wal_size_bytes = 67108864\n\n# Auto-compact when a shard accumulates this many batch files (0 = disabled)\nauto_compact_threshold = 10\n\n# How often to check for auto-compaction, in seconds (0 = disabled)\nauto_compact_interval_secs = 300\n\n# -----------------------------------------------------------------------------\n# Performance Tuning\n# -----------------------------------------------------------------------------\n[storage.performance]\n# Initial capacity for in-memory collections\ninitial_capacity = 10000\n\n# Batch size for bulk operations\nbatch_size = 1000\n\n# Enable async I/O operations\nasync_io = true\n\n# Number of worker threads (0 = use all CPU cores)\nnum_threads = 0\n\n# Query timeout in milliseconds (0 = no timeout)\nquery_timeout_ms = 30000\n\n# Maximum query text size in bytes\nmax_query_size_bytes = 1048576\n\n# Maximum tuples per single insert\nmax_insert_tuples = 10000\n\n# Maximum string value size in bytes\nmax_string_value_bytes = 65536\n\n# Maximum rows returned per query result (0 = unlimited)\nmax_result_rows = 100000\n\n# Log queries slower than this (ms, 0 = disabled)\nslow_query_log_ms = 5000\n\n# Maximum query cost budget (0 = unlimited)\nmax_query_cost = 0\n\n# =============================================================================\n# QUERY OPTIMIZATION\n# =============================================================================\n[optimization]\n# Enable join order planning\nenable_join_planning = true\n\n# Enable SIP (Sideways Information Passing) rewriting\nenable_sip_rewriting = true\n\n# Enable subplan sharing across rules\nenable_subplan_sharing = true\n\n# Enable boolean specialization optimizations\nenable_boolean_specialization = true\n\n# Enable Magic Sets demand-driven rewriting for recursive queries\nenable_magic_sets = true\n\n# =============================================================================\n# LOGGING\n# =============================================================================\n[logging]\n# Log level: trace, debug, info, warn, error\nlevel = \"info\"\n\n# Log format: text, json\nformat = \"text\"\n\n# =============================================================================\n# HTTP SERVER (WebSocket API)\n# =============================================================================\n[http]\n# Enable HTTP server for WebSocket API access\nenabled = true\n\n# Bind address\nhost = \"127.0.0.1\"\n\n# Port number\nport = 8080\n\n# CORS allowed origins (empty = same-origin only, unless cors_allow_all is true)\ncors_origins = []\n\n# Allow all CORS origins (use only in development)\ncors_allow_all = false\n\n# WebSocket idle timeout in milliseconds (default: 5 minutes)\nws_idle_timeout_ms = 300000\n\n# Graceful shutdown timeout in seconds (default: 30)\nshutdown_timeout_secs = 30\n\n# Stats endpoint timeout in seconds (default: 5)\nstats_timeout_secs = 5\n\n# -----------------------------------------------------------------------------\n# Web GUI Dashboard\n# -----------------------------------------------------------------------------\n[http.gui]\n# Enable web-based GUI dashboard\nenabled = true\n\n# Directory containing GUI static files\nstatic_dir = \"./gui/dist\"\n\n# -----------------------------------------------------------------------------\n# Authentication (always enabled)\n# -----------------------------------------------------------------------------\n[http.auth]\n# On first boot, an admin user and bootstrap API key are created.\n# Set a known password, or omit to auto-generate one (printed to stderr).\n# bootstrap_admin_password = \"your-secure-password\"\n\n# Session timeout in seconds (default: 24 hours)\nsession_timeout_secs = 86400\n\n# =============================================================================\n# RATE LIMITING\n# =============================================================================\n[http.rate_limit]\n# Maximum total connections (0 = unlimited)\nmax_connections = 10000\n\n# Maximum WebSocket connections (0 = unlimited)\nmax_ws_connections = 5000\n\n# Maximum WebSocket messages per second per connection (0 = unlimited)\nws_max_messages_per_sec = 1000\n\n# Maximum WebSocket connection lifetime in seconds (0 = unlimited, default: 24h)\nws_max_lifetime_secs = 86400\n\n# Maximum HTTP requests per second per IP address (0 = unlimited)\nper_ip_max_rps = 0\n\n# Notification ring buffer size for reconnect replay\nnotification_buffer_size = 4096\n```\n\n## Environment Variables\n\nAll config options can be overridden with environment variables using the `INPUTLAYER_` prefix:\n\n```bash\n# Storage settings\nexport INPUTLAYER_STORAGE__DATA_DIR=/var/lib/inputlayer/data\nexport INPUTLAYER_STORAGE__DEFAULT_KNOWLEDGE_GRAPH=mydb\n\n# Persistence\nexport INPUTLAYER_STORAGE__PERSIST__DURABILITY_MODE=batched\nexport INPUTLAYER_STORAGE__PERSIST__BUFFER_SIZE=50000\n\n# HTTP Server\nexport INPUTLAYER_HTTP__ENABLED=true\nexport INPUTLAYER_HTTP__PORT=9090\n\n# Authentication\nexport INPUTLAYER_ADMIN_PASSWORD=your-secure-password   # Admin password (first boot only)\nexport INPUTLAYER_API_KEY=your-api-key                  # CLI client authentication\n\n# Performance limits\nexport INPUTLAYER_STORAGE__PERFORMANCE__MAX_RESULT_ROWS=50000\n\n# Rate limiting\nexport INPUTLAYER_HTTP__RATE_LIMIT__WS_MAX_MESSAGES_PER_SEC=500\n\n# Logging\nexport INPUTLAYER_LOGGING__LEVEL=debug\n```\n\n**Note:** Use double underscores (`__`) to separate nested config sections.\n\n## Common Configurations\n\n### Development (Fast Iteration)\n\n```toml\n[storage]\ndata_dir = \"./dev-data\"\n\n[storage.persist]\ndurability_mode = \"async\"  # Fast writes, less safe\n\n[logging]\nlevel = \"debug\"\n```\n\n### Production (Safe & Durable)\n\n```toml\n[storage]\ndata_dir = \"/var/lib/inputlayer/data\"\n\n[storage.persistence]\nformat = \"parquet\"\ncompression = \"snappy\"\nenable_wal = true\n\n[storage.persist]\ndurability_mode = \"immediate\"\nbuffer_size = 10000\n\n[storage.performance]\nnum_threads = 0  # Use all cores\n\n[logging]\nlevel = \"warn\"\nformat = \"json\"\n\n[http]\nhost = \"0.0.0.0\"\nport = 8080\n\n[http.auth]\n# Set a known admin password for production deployments\nbootstrap_admin_password = \"your-secure-password-here\"\n\n[http.rate_limit]\nws_max_messages_per_sec = 500\nper_ip_max_rps = 100\n```\n\n### High-Throughput Ingestion\n\n```toml\n[storage.persist]\ndurability_mode = \"batched\"\nbuffer_size = 100000\n\n[storage.performance]\nbatch_size = 10000\nasync_io = true\nnum_threads = 0\n```\n\n### Memory-Constrained Environment\n\n```toml\n[storage.performance]\ninitial_capacity = 1000\nbatch_size = 100\n\n[storage.persist]\nbuffer_size = 1000\n```\n\n## Durability Modes Explained\n\n| Mode | Write Latency | Crash Safety | Use Case |\n|------|--------------|--------------|----------|\n| `immediate` | High | Full | Financial data, critical records |\n| `batched` | Medium | Partial | Most production workloads |\n| `async` | Low | Minimal | Development, analytics pipelines |\n\n### Immediate Mode\n- Every write syncs to disk before returning\n- Zero data loss on crash\n- Highest latency\n\n### Batched Mode\n- Writes buffer in memory\n- Periodic sync to disk\n- May lose last batch on crash\n\n### Async Mode\n- Writes return immediately\n- Background persistence\n- May lose recent updates on crash\n- Best for high-throughput ingestion where some loss is acceptable\n\n## Storage Formats\n\n| Format | Size | Speed | Use Case |\n|--------|------|-------|----------|\n| `parquet` | Smallest | Fast reads | Production, analytics |\n| `csv` | Largest | Slow | Debugging, interop |\n| `bincode` | Small | Fastest | Rust-only deployments |\n\n## Verifying Configuration\n\nCheck your effective configuration in the REPL:\n\n```\n.status\n```",
    "toc": [
      {
        "level": 2,
        "text": "Configuration File Locations",
        "id": "configuration-file-locations"
      },
      {
        "level": 2,
        "text": "Complete Configuration Reference",
        "id": "complete-configuration-reference"
      },
      {
        "level": 2,
        "text": "Environment Variables",
        "id": "environment-variables"
      },
      {
        "level": 2,
        "text": "Common Configurations",
        "id": "common-configurations"
      },
      {
        "level": 3,
        "text": "Development (Fast Iteration)",
        "id": "development-fast-iteration"
      },
      {
        "level": 3,
        "text": "Production (Safe & Durable)",
        "id": "production-safe-durable"
      },
      {
        "level": 3,
        "text": "High-Throughput Ingestion",
        "id": "high-throughput-ingestion"
      },
      {
        "level": 3,
        "text": "Memory-Constrained Environment",
        "id": "memory-constrained-environment"
      },
      {
        "level": 2,
        "text": "Durability Modes Explained",
        "id": "durability-modes-explained"
      },
      {
        "level": 3,
        "text": "Immediate Mode",
        "id": "immediate-mode"
      },
      {
        "level": 3,
        "text": "Batched Mode",
        "id": "batched-mode"
      },
      {
        "level": 3,
        "text": "Async Mode",
        "id": "async-mode"
      },
      {
        "level": 2,
        "text": "Storage Formats",
        "id": "storage-formats"
      },
      {
        "level": 2,
        "text": "Verifying Configuration",
        "id": "verifying-configuration"
      }
    ]
  },
  "guides/core-concepts": {
    "title": "Data Modeling Guide",
    "content": "# Data Modeling Guide\n\nIdentity semantics, schema options, and update patterns.\n\n## Identity Model\n\nInputLayer uses **pure multiset semantics** by default, where the **entire tuple is the identity**. This is the native model for Differential Dataflow (DD).\n\n### Default Behavior (No Schema)\n\nWithout an explicit schema, tuples are identified by all their values:\n\n```datalog\n+person(\"alice\", 30)     // Insert tuple (\"alice\", 30)\n+person(\"alice\", 31)     // Insert different tuple (\"alice\", 31)\n```\n\nBoth tuples coexist because they are different values. There is no concept of \"alice\" as an entity with a mutable \"age\" attribute.\n\n### Implications\n\n| Aspect | Behavior |\n|--------|----------|\n| Tuple identity | ALL columns (entire tuple) |\n| Duplicate handling | Multiset - same tuple can exist multiple times |\n| Updates | Must know all column values to delete |\n\n## Schema Declarations\n\nSchemas define the structure and constraints for relations.\n\n### Basic Schema\n\nDeclare a schema using typed arguments:\n\n```datalog\n+person(id: int, name: string, age: int)\n```\n\n## Update Patterns\n\n### Pattern 1: Exact Delete (Know All Values)\n\nWhen you know the exact tuple to delete:\n\n```datalog\n-person(\"alice\", 30)\n+person(\"alice\", 31)\n```\n\n### Pattern 2: Conditional Delete (Unknown Values)\n\nWhen you don't know all column values, use a conditional delete:\n\n```datalog\n// Delete alice regardless of age\n-person(\"alice\", Age) <- person(\"alice\", Age)\n+person(\"alice\", 31)\n```\n\n### Pattern 3: Atomic Update\n\nCombine delete and insert in one atomic operation:\n\n```datalog\n-person(Name, OldAge), +person(Name, NewAge) <-\n  person(Name, OldAge),\n  Name = \"alice\",\n  NewAge = OldAge + 1\n```\n\nThis executes at the same logical timestamp, ensuring atomicity.\n\n## Deletion Patterns\n\n### Delete Specific Tuple\n\n```datalog\n-edge(1, 2)\n```\n\n### Delete All Matching Tuples\n\n```datalog\n// Delete all edges from node 5\n-edge(5, Y) <- edge(5, Y)\n\n// Delete all high earners\n-employee(Name, Dept, Salary) <-\n  employee(Name, Dept, Salary),\n  Salary > 100000\n```\n\n### Delete a Rule\n\nTo delete a persistent rule:\n\n```datalog\n-reachable\n```\n\nThis drops the rule named `reachable`. To delete all facts from a relation, use a conditional delete:\n\n```datalog\n-person(X, Y, Z) <- person(X, Y, Z)  // Delete all tuples\n```\n\nTo drop a relation entirely (schema + data), use the meta command:\n\n```datalog\n.rel drop person\n```\n\n## Schema Inference\n\nWhen no schema is declared, it's inferred from the first insert:\n\n```datalog\n+person(\"alice\", 30)          // Inferred: person(string, int)\n+person(\"bob\", 25)            // OK: matches inferred schema\n+person(\"charlie\", \"young\")   // ERROR: type mismatch (string vs int)\n```\n\n## Transient vs Persistent\n\n### Persistent Schema (`+` prefix)\n\nStored in the database catalog:\n\n```datalog\n+person(id: int, name: string, age: int)\n```\n\n### Transient Schema (no prefix)\n\nSession-only, cleared on database switch:\n\n```datalog\ntemp(x: int, y: int)\ntemp(1, 2)\ntemp(3, 4)\n// Cleared when switching databases\n```\n\nUse transient schemas for:\n- REPL exploration with type safety\n- Temporary working data\n- Testing schema designs before persisting\n\n## Rules (Views)\n\n### Rule Identity\n\nA **view** (derived relation) is identified by its **head predicate name**. A view contains one or more rules:\n\n```datalog\n+reachable(X, Y) <- edge(X, Y)                   // Creates view, adds rule 1\n+reachable(X, Y) <- reachable(X, Z), edge(Z, Y)  // Adds rule 2 to same view\n```\n\n### Deleting Views\n\nDelete an entire view with:\n\n```datalog\n-reachable\n```\n\nIndividual rule clauses can be removed using `.rule remove`:\n\n```datalog\n.rule remove reachable 1   // Remove first clause of 'reachable' rule\n.rule drop reachable       // Remove entire 'reachable' rule (all clauses)\n```\n\nOr use file-based workflow:\n\n```datalog\n.rule clear reachable\n.load views/reachable.idl\n```\n\n### Session Rules\n\nRules without `+` are transient:\n\n```datalog\ntemp(X, Y) <- edge(X, Y), X < Y\n```\n\nSession rules:\n- Are not persisted\n- Are cleared on database switch\n- Support recursion (full fixed-point iteration)\n\n## File-Based Workflow\n\nFor complex views with many rules, use `.idl` script files:\n\n```datalog\n// views/reachable.idl\n+reachable(X, Y) <- edge(X, Y)\n+reachable(X, Y) <- reachable(X, Z), edge(Z, Y)\n```\n\n### Example Workflow\n\n```datalog\n// Initial load\n.load views/access_control.idl\n\n// After modifying the file, clear rules first then reload\n.rule clear access_control\n.load views/access_control.idl\n```\n\n## Best Practices\n\n### 1. Use Explicit Schemas\n\nExplicit schemas catch type errors early:\n\n```datalog\n+employee(id: int, name: string, salary: float)\n```\n\n### 2. Use Conditional Deletes for Unknown Values\n\n```datalog\n// Update all employees in a department\n-employee(Id, OldDept, Name), +employee(Id, \"Engineering\", Name) <-\n  employee(Id, OldDept, Name),\n  OldDept = \"Legacy\"\n```\n\n### 3. Use File-Based Workflow for Complex Rules\n\nKeep rule definitions in version-controlled files:\n\n```\nviews/\n  access_control.idl\n  graph_analysis.idl\n  reporting.idl\n```\n\n### 4. Use Persistent Rules for Automatic Materialization\n\nPersistent rules are automatically materialized and updated when base data changes:\n\n```datalog\n// Session rules compute fresh each query:\nreachable(X, Y) <- edge(X, Y)\nreachable(X, Y) <- reachable(X, Z), edge(Z, Y)\n\n// Persistent rules materialize and cache results:\n+reachable(X, Y) <- edge(X, Y)\n+reachable(X, Y) <- reachable(X, Z), edge(Z, Y)\n```\n\nBoth session and persistent rules support full recursion with fixed-point iteration.",
    "toc": [
      {
        "level": 2,
        "text": "Identity Model",
        "id": "identity-model"
      },
      {
        "level": 3,
        "text": "Default Behavior (No Schema)",
        "id": "default-behavior-no-schema"
      },
      {
        "level": 3,
        "text": "Implications",
        "id": "implications"
      },
      {
        "level": 2,
        "text": "Schema Declarations",
        "id": "schema-declarations"
      },
      {
        "level": 3,
        "text": "Basic Schema",
        "id": "basic-schema"
      },
      {
        "level": 2,
        "text": "Update Patterns",
        "id": "update-patterns"
      },
      {
        "level": 3,
        "text": "Pattern 1: Exact Delete (Know All Values)",
        "id": "pattern-1-exact-delete-know-all-values"
      },
      {
        "level": 3,
        "text": "Pattern 2: Conditional Delete (Unknown Values)",
        "id": "pattern-2-conditional-delete-unknown-values"
      },
      {
        "level": 3,
        "text": "Pattern 3: Atomic Update",
        "id": "pattern-3-atomic-update"
      },
      {
        "level": 2,
        "text": "Deletion Patterns",
        "id": "deletion-patterns"
      },
      {
        "level": 3,
        "text": "Delete Specific Tuple",
        "id": "delete-specific-tuple"
      },
      {
        "level": 3,
        "text": "Delete All Matching Tuples",
        "id": "delete-all-matching-tuples"
      },
      {
        "level": 3,
        "text": "Delete a Rule",
        "id": "delete-a-rule"
      },
      {
        "level": 2,
        "text": "Schema Inference",
        "id": "schema-inference"
      },
      {
        "level": 2,
        "text": "Transient vs Persistent",
        "id": "transient-vs-persistent"
      },
      {
        "level": 3,
        "text": "Persistent Schema (+ prefix)",
        "id": "persistent-schema-prefix"
      },
      {
        "level": 3,
        "text": "Transient Schema (no prefix)",
        "id": "transient-schema-no-prefix"
      },
      {
        "level": 2,
        "text": "Rules (Views)",
        "id": "rules-views"
      },
      {
        "level": 3,
        "text": "Rule Identity",
        "id": "rule-identity"
      },
      {
        "level": 3,
        "text": "Deleting Views",
        "id": "deleting-views"
      },
      {
        "level": 3,
        "text": "Session Rules",
        "id": "session-rules"
      },
      {
        "level": 2,
        "text": "File-Based Workflow",
        "id": "file-based-workflow"
      },
      {
        "level": 3,
        "text": "Example Workflow",
        "id": "example-workflow"
      },
      {
        "level": 2,
        "text": "Best Practices",
        "id": "best-practices"
      },
      {
        "level": 3,
        "text": "1. Use Explicit Schemas",
        "id": "1-use-explicit-schemas"
      },
      {
        "level": 3,
        "text": "2. Use Conditional Deletes for Unknown Values",
        "id": "2-use-conditional-deletes-for-unknown-values"
      },
      {
        "level": 3,
        "text": "3. Use File-Based Workflow for Complex Rules",
        "id": "3-use-file-based-workflow-for-complex-rules"
      },
      {
        "level": 3,
        "text": "4. Use Persistent Rules for Automatic Materialization",
        "id": "4-use-persistent-rules-for-automatic-materialization"
      }
    ]
  },
  "guides/deployment": {
    "title": "Deployment",
    "content": "# Deployment\n\nRun InputLayer in production with Docker, systemd, reverse proxies, and Kubernetes.\n\n## Quick Reference\n\n| Setting | Default | CLI Flag | Env Variable | Config Key |\n|---|---|---|---|---|\n| Host | `127.0.0.1` | `--host` | `INPUTLAYER_HTTP__HOST` | `http.host` |\n| Port | `8080` | `--port` | `INPUTLAYER_HTTP__PORT` | `http.port` |\n| Data directory | `./data` | `--data-dir` | `INPUTLAYER_STORAGE__DATA_DIR` | `storage.data_dir` |\n| Log level | `info` | -- | `IL_TRACE_LEVEL` | `logging.level` |\n| Log format | `text` | -- | `IL_TRACE_JSON=1` | `logging.format` |\n| Admin password | auto-generated | -- | `INPUTLAYER_ADMIN_PASSWORD` | `http.auth.bootstrap_admin_password` |\n| Config file | `config.toml` | `--config` | -- | -- |\n\n## Environment Variables\n\nInputLayer uses the `INPUTLAYER_` prefix with double-underscore separators for nested keys.\nEnvironment variables override values from config files.\n\n```bash\n# Server binding\nexport INPUTLAYER_HTTP__HOST=0.0.0.0\nexport INPUTLAYER_HTTP__PORT=8080\n\n# Storage\nexport INPUTLAYER_STORAGE__DATA_DIR=/var/lib/inputlayer/data\nexport INPUTLAYER_STORAGE__AUTO_CREATE_KNOWLEDGE_GRAPHS=true\n\n# Persistence\nexport INPUTLAYER_STORAGE__PERSIST__ENABLED=true\nexport INPUTLAYER_STORAGE__PERSIST__DURABILITY_MODE=immediate\n\n# Logging (via tracing)\nexport IL_TRACE_LEVEL=info          # trace, debug, info, warn, error\nexport IL_TRACE_JSON=1              # JSON structured logging\nexport IL_TRACE_FILE=/var/log/inputlayer/server.log  # Log to file instead of stderr\n\n# Authentication\nexport INPUTLAYER_ADMIN_PASSWORD=your-secure-password\n\n# Performance\nexport INPUTLAYER_STORAGE__PERFORMANCE__NUM_THREADS=4\nexport INPUTLAYER_STORAGE__PERFORMANCE__QUERY_TIMEOUT_MS=30000\n```\n\n## Configuration File\n\nCreate a `config.toml` for persistent configuration. Pass a custom path with `--config`:\n\n```bash\ninputlayer-server --config /etc/inputlayer/config.toml\n```\n\nExample production configuration:\n\n```toml\n[storage]\ndata_dir = \"/var/lib/inputlayer/data\"\ndefault_knowledge_graph = \"default\"\nauto_create_knowledge_graphs = false\nmax_knowledge_graphs = 100\n\n[storage.persist]\nenabled = true\ndurability_mode = \"immediate\"\nbuffer_size = 10000\nmax_wal_size_bytes = 67108864       # 64 MB\n\n[storage.performance]\nnum_threads = 0                     # 0 = all available CPU cores\nquery_timeout_ms = 30000            # 30 seconds\nmax_query_size_bytes = 1048576      # 1 MB\nmax_insert_tuples = 10000\nmax_result_rows = 100000\nslow_query_log_ms = 5000\n\n[logging]\nlevel = \"info\"\nformat = \"json\"\n\n[http]\nhost = \"0.0.0.0\"\nport = 8080\nshutdown_timeout_secs = 30\n\n[http.auth]\nsession_timeout_secs = 86400        # 24 hours\n\n[http.rate_limit]\nmax_connections = 10000\nmax_ws_connections = 5000\nws_max_messages_per_sec = 1000\nper_ip_max_rps = 100\n\n[http.gui]\nenabled = true\nstatic_dir = \"/var/lib/inputlayer/gui/dist\"\n```\n\n## Health Checks\n\nInputLayer exposes three probe endpoints. All bypass authentication.\n\n| Endpoint | Method | Purpose | Healthy Response |\n|---|---|---|---|\n| `/health` | GET | General health check | `200 OK` |\n| `/live` | GET | Kubernetes liveness probe | `200 OK` |\n| `/ready` | GET | Kubernetes readiness probe | `200 OK` (or `503` if storage is unavailable) |\n\nAll endpoints are also available under the `/v1/` prefix (e.g., `/v1/health`).\n\nExample health check:\n\n```bash\ncurl -sf http://localhost:8080/health\n```\n\n## Docker\n\nInputLayer provides a multi-stage Dockerfile that produces a minimal Debian-based image.\n\n### Build the Image\n\n```bash\ndocker build -t inputlayer:latest .\n```\n\n### Run a Container\n\n```bash\ndocker run -d \\\n  --name inputlayer \\\n  -p 8080:8080 \\\n  -v inputlayer-data:/var/lib/inputlayer/data \\\n  -e INPUTLAYER_ADMIN_PASSWORD=changeme \\\n  inputlayer:latest\n```\n\nThe image includes a built-in `HEALTHCHECK` that polls `/health` every 10 seconds:\n\n```dockerfile\nHEALTHCHECK --interval=10s --timeout=3s --start-period=5s --retries=3 \\\n    CMD curl -sf http://localhost:8080/health || exit 1\n```\n\n### Image Details\n\nThe image runs as a non-root `inputlayer` user with the following defaults:\n\n| Setting | Value |\n|---|---|\n| Binary | `/usr/local/bin/inputlayer-server` |\n| Data volume | `/var/lib/inputlayer/data` |\n| GUI assets | `/var/lib/inputlayer/gui/dist/` |\n| Exposed port | `8080` |\n| User | `inputlayer` (non-root) |\n\n### Custom Configuration in Docker\n\nMount a config file and pass it via the CLI:\n\n```bash\ndocker run -d \\\n  --name inputlayer \\\n  -p 8080:8080 \\\n  -v inputlayer-data:/var/lib/inputlayer/data \\\n  -v ./config.toml:/etc/inputlayer/config.toml:ro \\\n  inputlayer:latest \\\n  --config /etc/inputlayer/config.toml\n```\n\n## Docker Compose\n\n### Basic Setup\n\n```yaml\nversion: \"3.8\"\n\nservices:\n  inputlayer:\n    image: inputlayer:latest\n    build: .\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - inputlayer-data:/var/lib/inputlayer/data\n    environment:\n      INPUTLAYER_HTTP__HOST: \"0.0.0.0\"\n      INPUTLAYER_HTTP__PORT: \"8080\"\n      INPUTLAYER_STORAGE__DATA_DIR: \"/var/lib/inputlayer/data\"\n      INPUTLAYER_STORAGE__PERSIST__ENABLED: \"true\"\n      INPUTLAYER_STORAGE__PERSIST__DURABILITY_MODE: \"immediate\"\n      INPUTLAYER_ADMIN_PASSWORD: \"${INPUTLAYER_ADMIN_PASSWORD}\"\n      IL_TRACE_LEVEL: \"info\"\n      IL_TRACE_JSON: \"1\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-sf\", \"http://localhost:8080/health\"]\n      interval: 10s\n      timeout: 3s\n      start_period: 5s\n      retries: 3\n    restart: unless-stopped\n\nvolumes:\n  inputlayer-data:\n```\n\n### With Traefik Reverse Proxy\n\n```yaml\nversion: \"3.8\"\n\nservices:\n  traefik:\n    image: traefik:v3.2\n    command:\n      - \"--providers.docker=true\"\n      - \"--providers.docker.exposedByDefault=false\"\n      - \"--entrypoints.web.address=:80\"\n      - \"--entrypoints.websecure.address=:443\"\n      - \"--certificatesresolvers.le.acme.httpchallenge.entrypoint=web\"\n      - \"--certificatesresolvers.le.acme.email=admin@example.com\"\n      - \"--certificatesresolvers.le.acme.storage=/letsencrypt/acme.json\"\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - letsencrypt:/letsencrypt\n\n  inputlayer:\n    image: inputlayer:latest\n    build: .\n    volumes:\n      - inputlayer-data:/var/lib/inputlayer/data\n    environment:\n      INPUTLAYER_HTTP__HOST: \"0.0.0.0\"\n      INPUTLAYER_HTTP__PORT: \"8080\"\n      INPUTLAYER_STORAGE__DATA_DIR: \"/var/lib/inputlayer/data\"\n      INPUTLAYER_ADMIN_PASSWORD: \"${INPUTLAYER_ADMIN_PASSWORD}\"\n      IL_TRACE_JSON: \"1\"\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.inputlayer.rule=Host(`inputlayer.example.com`)\"\n      - \"traefik.http.routers.inputlayer.entrypoints=websecure\"\n      - \"traefik.http.routers.inputlayer.tls.certresolver=le\"\n      - \"traefik.http.services.inputlayer.loadbalancer.server.port=8080\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-sf\", \"http://localhost:8080/health\"]\n      interval: 10s\n      timeout: 3s\n      start_period: 5s\n      retries: 3\n    restart: unless-stopped\n\nvolumes:\n  inputlayer-data:\n  letsencrypt:\n```\n\n## systemd\n\nCreate a systemd service for running InputLayer on a bare-metal or VM deployment.\n\n### Create a System User\n\n```bash\nsudo useradd -r -s /bin/false -m -d /var/lib/inputlayer inputlayer\nsudo mkdir -p /var/lib/inputlayer/data\nsudo chown inputlayer:inputlayer /var/lib/inputlayer/data\n```\n\n### Install the Binary\n\n```bash\nsudo cp target/release/inputlayer-server /usr/local/bin/\nsudo chmod 755 /usr/local/bin/inputlayer-server\n```\n\n### Create the Service File\n\nWrite the following to `/etc/systemd/system/inputlayer.service`:\n\n```ini\n[Unit]\nDescription=InputLayer Knowledge Graph Database\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\nUser=inputlayer\nGroup=inputlayer\nExecStart=/usr/local/bin/inputlayer-server \\\n    --host 0.0.0.0 \\\n    --port 8080 \\\n    --config /etc/inputlayer/config.toml \\\n    --data-dir /var/lib/inputlayer/data\nRestart=on-failure\nRestartSec=5\nLimitNOFILE=65536\n\n# Security hardening\nProtectSystem=strict\nProtectHome=true\nReadWritePaths=/var/lib/inputlayer/data\nPrivateTmp=true\nNoNewPrivileges=true\nProtectKernelTunables=true\nProtectControlGroups=true\n\n# Environment\nEnvironment=IL_TRACE_LEVEL=info\nEnvironment=IL_TRACE_JSON=1\nEnvironment=IL_TRACE_FILE=/var/log/inputlayer/server.log\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### Enable and Start\n\n```bash\nsudo mkdir -p /var/log/inputlayer\nsudo chown inputlayer:inputlayer /var/log/inputlayer\n\nsudo systemctl daemon-reload\nsudo systemctl enable inputlayer\nsudo systemctl start inputlayer\n\n# Check status\nsudo systemctl status inputlayer\nsudo journalctl -u inputlayer -f\n```\n\n## TLS and Reverse Proxies\n\nInputLayer does not terminate TLS natively. Use a reverse proxy for TLS termination in production.\n\n### Nginx\n\n```nginx\nserver {\n    listen 443 ssl;\n    server_name inputlayer.example.com;\n    ssl_certificate     /etc/ssl/certs/inputlayer.crt;\n    ssl_certificate_key /etc/ssl/private/inputlayer.key;\n    ssl_protocols       TLSv1.2 TLSv1.3;\n    location / {\n        proxy_pass http://127.0.0.1:8080;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_read_timeout 300s;\n        proxy_send_timeout 300s;\n    }\n}\n```\n\nThe `proxy_read_timeout` and `proxy_send_timeout` values of 300 seconds accommodate long-running WebSocket connections. The `Upgrade` and `Connection` headers are required for WebSocket support.\n\nFor HTTP-to-HTTPS redirect, add a companion server block:\n\n```nginx\nserver {\n    listen 80;\n    server_name inputlayer.example.com;\n    return 301 https://$host$request_uri;\n}\n```\n\n### Caddy\n\nCaddy automatically provisions and renews TLS certificates via Let's Encrypt:\n\n```caddyfile\ninputlayer.example.com {\n    reverse_proxy 127.0.0.1:8080 {\n        header_up X-Real-IP {remote_host}\n    }\n}\n```\n\nNo additional TLS configuration is needed. Caddy handles certificates, HTTPS redirect, and WebSocket upgrades automatically.\n\n## Kubernetes\n\n### Deployment\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inputlayer\n  labels:\n    app: inputlayer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inputlayer\n  template:\n    metadata:\n      labels:\n        app: inputlayer\n    spec:\n      containers:\n        - name: inputlayer\n          image: inputlayer:latest\n          ports:\n            - containerPort: 8080\n              name: http\n          env:\n            - name: INPUTLAYER_HTTP__HOST\n              value: \"0.0.0.0\"\n            - name: INPUTLAYER_HTTP__PORT\n              value: \"8080\"\n            - name: INPUTLAYER_STORAGE__DATA_DIR\n              value: \"/var/lib/inputlayer/data\"\n            - name: INPUTLAYER_STORAGE__PERSIST__ENABLED\n              value: \"true\"\n            - name: INPUTLAYER_STORAGE__PERSIST__DURABILITY_MODE\n              value: \"immediate\"\n            - name: IL_TRACE_LEVEL\n              value: \"info\"\n            - name: IL_TRACE_JSON\n              value: \"1\"\n            - name: INPUTLAYER_ADMIN_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: inputlayer-secrets\n                  key: admin-password\n          volumeMounts:\n            - name: data\n              mountPath: /var/lib/inputlayer/data\n          livenessProbe:\n            httpGet:\n              path: /live\n              port: http\n            initialDelaySeconds: 5\n            periodSeconds: 10\n            timeoutSeconds: 3\n          readinessProbe:\n            httpGet:\n              path: /ready\n              port: http\n            initialDelaySeconds: 3\n            periodSeconds: 5\n            timeoutSeconds: 3\n          resources:\n            requests:\n              cpu: \"500m\"\n              memory: \"512Mi\"\n            limits:\n              cpu: \"4\"\n              memory: \"4Gi\"\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: inputlayer-data\n```\n\n### Service\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: inputlayer\nspec:\n  selector:\n    app: inputlayer\n  ports:\n    - port: 8080\n      targetPort: http\n      name: http\n  type: ClusterIP\n```\n\n### PersistentVolumeClaim\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: inputlayer-data\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n```\n\n### Secret\n\n```bash\nkubectl create secret generic inputlayer-secrets \\\n  --from-literal=admin-password='your-secure-password'\n```\n\n### Ingress\n\nUsing the nginx ingress controller with TLS:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: inputlayer\n  annotations:\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"60\"\n    # Required for WebSocket support\n    nginx.ingress.kubernetes.io/websocket-services: \"inputlayer\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  tls:\n    - hosts:\n        - inputlayer.example.com\n      secretName: inputlayer-tls\n  rules:\n    - host: inputlayer.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: inputlayer\n                port:\n                  number: 8080\n```\n\n## Resource Sizing\n\n### Memory\n\nInputLayer holds the active working set in memory via Differential Dataflow. Memory usage is proportional to the number of facts, rules, and active computations.\n\n| Workload | Facts | Rules | Recommended Memory |\n|---|---|---|---|\n| Development | < 100K | < 50 | 512 MB |\n| Small production | 100K - 1M | 50 - 200 | 2 - 4 GB |\n| Medium production | 1M - 10M | 200 - 1000 | 8 - 16 GB |\n| Large production | 10M+ | 1000+ | 32 GB+ |\n\n### CPU\n\nDifferential Dataflow benefits from multiple cores for parallel dataflow computation. The `num_threads` setting (default: `0`, meaning all available cores) controls worker parallelism.\n\n| Workload | Recommended CPU |\n|---|---|\n| Development | 2 cores |\n| Small production | 4 cores |\n| Medium production | 8 cores |\n| Large production | 16+ cores |\n\n### Disk\n\nThe data directory stores WAL files and compacted batch files. Disk usage depends on fact volume and the WAL rotation settings.\n\n- WAL files rotate at 64 MB by default (`max_wal_size_bytes`)\n- Batch files are compacted when a shard exceeds 10 files (`auto_compact_threshold`)\n- Plan for 2-3x the raw data size to account for WAL overhead and compaction headroom\n\nUse fast local storage (SSD/NVMe) for the data directory. Network-attached storage works but increases write latency, which impacts durability mode `immediate`.\n\n### File Descriptors\n\nFor production deployments with many concurrent WebSocket connections, increase the file descriptor limit:\n\n```bash\n# In systemd service\nLimitNOFILE=65536\n\n# Or system-wide in /etc/security/limits.conf\ninputlayer    soft    nofile    65536\ninputlayer    hard    nofile    65536\n```\n\n## Security Recommendations\n\n### Authentication\n\nInputLayer requires WebSocket authentication for all data operations. On first boot, if no admin password is configured, a random password is generated and printed to stderr. For production:\n\n1. Always set `INPUTLAYER_ADMIN_PASSWORD` explicitly\n2. Use Kubernetes Secrets or Docker secrets for the password\n3. Rotate credentials periodically\n\n### Network\n\n1. Bind the server to `127.0.0.1` when behind a reverse proxy (the default)\n2. Use a reverse proxy for TLS termination -- InputLayer does not terminate TLS natively\n3. Enable rate limiting via `http.rate_limit.per_ip_max_rps` for public-facing deployments\n4. Restrict CORS origins in production (`http.cors_origins` instead of `http.cors_allow_all`)\n\n### Filesystem\n\n1. Run as a dedicated non-root user\n2. Use `ProtectSystem=strict` in systemd to prevent writes outside allowed paths\n3. Set appropriate permissions on the data directory (`750` or stricter)\n4. Back up the data directory regularly -- the WAL and batch files are the source of truth\n\n### Durability\n\nFor production, always use `durability_mode = \"immediate\"`. The server warns on startup if a weaker mode is configured:\n\n```\nWARNING: Durability mode is Batched (not Immediate).\nRecent writes may be lost on crash.\n```\n\nAlso ensure `storage.persist.enabled = true`. Without it, all data is lost on restart:\n\n```\nWARNING: DD-native persist layer is DISABLED.\nData will NOT survive restarts.\n```",
    "toc": [
      {
        "level": 2,
        "text": "Quick Reference",
        "id": "quick-reference"
      },
      {
        "level": 2,
        "text": "Environment Variables",
        "id": "environment-variables"
      },
      {
        "level": 2,
        "text": "Configuration File",
        "id": "configuration-file"
      },
      {
        "level": 2,
        "text": "Health Checks",
        "id": "health-checks"
      },
      {
        "level": 2,
        "text": "Docker",
        "id": "docker"
      },
      {
        "level": 3,
        "text": "Build the Image",
        "id": "build-the-image"
      },
      {
        "level": 3,
        "text": "Run a Container",
        "id": "run-a-container"
      },
      {
        "level": 3,
        "text": "Image Details",
        "id": "image-details"
      },
      {
        "level": 3,
        "text": "Custom Configuration in Docker",
        "id": "custom-configuration-in-docker"
      },
      {
        "level": 2,
        "text": "Docker Compose",
        "id": "docker-compose"
      },
      {
        "level": 3,
        "text": "Basic Setup",
        "id": "basic-setup"
      },
      {
        "level": 3,
        "text": "With Traefik Reverse Proxy",
        "id": "with-traefik-reverse-proxy"
      },
      {
        "level": 2,
        "text": "systemd",
        "id": "systemd"
      },
      {
        "level": 3,
        "text": "Create a System User",
        "id": "create-a-system-user"
      },
      {
        "level": 3,
        "text": "Install the Binary",
        "id": "install-the-binary"
      },
      {
        "level": 3,
        "text": "Create the Service File",
        "id": "create-the-service-file"
      },
      {
        "level": 3,
        "text": "Enable and Start",
        "id": "enable-and-start"
      },
      {
        "level": 2,
        "text": "TLS and Reverse Proxies",
        "id": "tls-and-reverse-proxies"
      },
      {
        "level": 3,
        "text": "Nginx",
        "id": "nginx"
      },
      {
        "level": 3,
        "text": "Caddy",
        "id": "caddy"
      },
      {
        "level": 2,
        "text": "Kubernetes",
        "id": "kubernetes"
      },
      {
        "level": 3,
        "text": "Deployment",
        "id": "deployment"
      },
      {
        "level": 3,
        "text": "Service",
        "id": "service"
      },
      {
        "level": 3,
        "text": "PersistentVolumeClaim",
        "id": "persistentvolumeclaim"
      },
      {
        "level": 3,
        "text": "Secret",
        "id": "secret"
      },
      {
        "level": 3,
        "text": "Ingress",
        "id": "ingress"
      },
      {
        "level": 2,
        "text": "Resource Sizing",
        "id": "resource-sizing"
      },
      {
        "level": 3,
        "text": "Memory",
        "id": "memory"
      },
      {
        "level": 3,
        "text": "CPU",
        "id": "cpu"
      },
      {
        "level": 3,
        "text": "Disk",
        "id": "disk"
      },
      {
        "level": 3,
        "text": "File Descriptors",
        "id": "file-descriptors"
      },
      {
        "level": 2,
        "text": "Security Recommendations",
        "id": "security-recommendations"
      },
      {
        "level": 3,
        "text": "Authentication",
        "id": "authentication"
      },
      {
        "level": 3,
        "text": "Network",
        "id": "network"
      },
      {
        "level": 3,
        "text": "Filesystem",
        "id": "filesystem"
      },
      {
        "level": 3,
        "text": "Durability",
        "id": "durability"
      }
    ]
  },
  "guides/first-program": {
    "title": "Your First Program",
    "content": "# Your First Program\n\nWrite a social-network reachability query in under 5 minutes.\n\n## What You'll Build\n\nA simple social network that tracks who follows whom, and computes who can reach whom through any chain of follows.\n\n## Step 1: Start the Client\n\n```bash\ninputlayer-client\n```\n\nYou'll see:\n```\nConnecting to server at http://127.0.0.1:8080...\nConnected!\n\nServer status: healthy\nAuthenticated as: admin\nCurrent knowledge graph: default\n```\n\n## Step 2: Create a Knowledge Graph\n\nEvery InputLayer program runs in a knowledge graph. Let's create one:\n\n```datalog\n.kg create social\n```\n\nOutput:\n```\nKnowledge graph 'social' created.\nSwitched to knowledge graph: social\n```\n\nYou can verify with:\n```datalog\n.kg\n```\n\nOutput:\n```\nCurrent knowledge graph: social\n```\n\n## Step 3: Add Facts\n\nFacts are the base data in your knowledge graph. Let's add some \"follows\" relationships:\n\n```datalog\n+follows(1, 2)\n```\n\nOutput:\n```\nInserted 1 fact(s) into 'follows'.\n```\n\nLet's add more facts using bulk insert:\n\n```datalog\n+follows[(2, 3), (3, 4), (1, 3)]\n```\n\nOutput:\n```\nInserted 3 fact(s) into 'follows'.\n```\n\n## Step 4: Query Facts\n\nUse `?` to query data:\n\n```datalog\n?follows(1, X)\n```\n\nOutput:\n\n| 1 | X |\n|---|---|\n| 1 | 2 |\n| 1 | 3 |\n\n*2 rows*\n\nThis shows everyone that user 1 directly follows.\n\n## Step 5: Define a Rule\n\nRules derive new data from existing facts. Let's define \"reachable\" - who can you reach through any chain of follows?\n\n```datalog\n+reachable(X, Y) <- follows(X, Y)\n```\n\nOutput:\n```\nRule 'reachable' registered.\n```\n\nThis says: \"X can reach Y if X follows Y directly.\"\n\nBut we also want transitive reachability. Add another clause:\n\n```datalog\n+reachable(X, Z) <- follows(X, Y), reachable(Y, Z)\n```\n\nOutput:\n```\nRule 'reachable' registered.\n```\n\nThis says: \"X can reach Z if X follows someone Y who can reach Z.\"\n\n## Step 6: Query the Rule\n\nNow query the derived relation:\n\n```datalog\n?reachable(1, X)\n```\n\nOutput:\n\n| 1 | X |\n|---|---|\n| 1 | 2 |\n| 1 | 3 |\n| 1 | 4 |\n\n*3 rows*\n\nUser 1 can reach users 2, 3, and 4! Even though user 1 doesn't directly follow user 4, they can reach them through the chain: 1 -> 2 -> 3 -> 4.\n\n## Step 7: View Your Rules\n\nSee what rules are defined:\n\n```datalog\n.rule\n```\n\nOutput:\n```\nRules:\n  reachable (2 clause(s))\n```\n\nSee the rule definition:\n\n```datalog\n.rule def reachable\n```\n\nOutput:\n```\nRule: reachable\nClauses:\n  1. reachable(X, Y) <- follows(X, Y)\n  2. reachable(X, Z) <- follows(X, Y), reachable(Y, Z)\n```\n\n## Step 8: Add More Data and See Incremental Updates\n\nAdd a new follows relationship:\n\n```datalog\n+follows(4, 5)\n```\n\nNow query reachable again:\n\n```datalog\n?reachable(1, X)\n```\n\nOutput:\n\n| 1 | X |\n|---|---|\n| 1 | 2 |\n| 1 | 3 |\n| 1 | 4 |\n| 1 | 5 |\n\n*4 rows*\n\nUser 1 can now reach user 5! InputLayer automatically recomputed the derived relation when you added new data.\n\n## Complete Program\n\nHere's everything we did in one script:\n\n```datalog\n// Create and use knowledge graph\n.kg create social\n.kg use social\n\n// Add base facts\n+follows[(1, 2), (2, 3), (3, 4), (1, 3)]\n\n// Define transitive reachability\n+reachable(X, Y) <- follows(X, Y)\n+reachable(X, Z) <- follows(X, Y), reachable(Y, Z)\n\n// Query: who can user 1 reach?\n?reachable(1, X)\n```\n\nYou can save this to a file `social.idl` and run it:\n\n```bash\ninputlayer-client --script social.idl\n```\n\n## Key Takeaways\n\n1. **Facts** (`+relation(...)`) are base data you insert\n2. **Rules** (`+head(...) <- body`) derive new data from existing data\n3. **Queries** (`?pattern`) ask questions about your data\n4. **Incremental** - When you add/remove facts, derived data updates automatically\n5. **Persistent** - Facts and rules are saved to disk\n\n## What's Different from SQL?\n\n| SQL | InputLayer (Datalog) |\n|-----|---------------------|\n| `INSERT INTO follows VALUES (1, 2)` | `+follows(1, 2)` |\n| `CREATE VIEW` (limited recursion) | `+rule(...) <- body` (full recursion) |\n| `SELECT * FROM follows WHERE a = 1` | `?follows(1, X)` |\n| Explicit JOINs | Implicit joins via shared variables |\n\n## Next Steps\n\n- **[Core Concepts](core-concepts)** - Deeper understanding of facts, rules, and queries\n- **[REPL Guide](repl)** - All the commands available\n- **[Recursion](recursion)** - Learn about recursive queries and graph traversal",
    "toc": [
      {
        "level": 2,
        "text": "What You'll Build",
        "id": "what-youll-build"
      },
      {
        "level": 2,
        "text": "Step 1: Start the Client",
        "id": "step-1-start-the-client"
      },
      {
        "level": 2,
        "text": "Step 2: Create a Knowledge Graph",
        "id": "step-2-create-a-knowledge-graph"
      },
      {
        "level": 2,
        "text": "Step 3: Add Facts",
        "id": "step-3-add-facts"
      },
      {
        "level": 2,
        "text": "Step 4: Query Facts",
        "id": "step-4-query-facts"
      },
      {
        "level": 2,
        "text": "Step 5: Define a Rule",
        "id": "step-5-define-a-rule"
      },
      {
        "level": 2,
        "text": "Step 6: Query the Rule",
        "id": "step-6-query-the-rule"
      },
      {
        "level": 2,
        "text": "Step 7: View Your Rules",
        "id": "step-7-view-your-rules"
      },
      {
        "level": 2,
        "text": "Step 8: Add More Data and See Incremental Updates",
        "id": "step-8-add-more-data-and-see-incremental-updates"
      },
      {
        "level": 2,
        "text": "Complete Program",
        "id": "complete-program"
      },
      {
        "level": 2,
        "text": "Key Takeaways",
        "id": "key-takeaways"
      },
      {
        "level": 2,
        "text": "What's Different from SQL?",
        "id": "whats-different-from-sql"
      },
      {
        "level": 2,
        "text": "Next Steps",
        "id": "next-steps"
      }
    ]
  },
  "guides/indexing": {
    "title": "Indexing Guide",
    "content": "# Indexing Guide\n\nInputLayer provides HNSW (Hierarchical Navigable Small World) indexes for fast approximate nearest neighbor search on vector data.\n\n## Why Use Indexes?\n\nWithout an index, vector similarity queries perform a linear scan:\n- **10K vectors**: ~10ms\n- **100K vectors**: ~100ms\n- **1M vectors**: ~1s\n\nWith an HNSW index:\n- **10K vectors**: ~1ms\n- **100K vectors**: ~5ms\n- **1M vectors**: ~10ms\n\n**Trade-off**: Indexes use memory and may return approximate (not exact) results.\n\n---\n\n## Creating an Index\n\n### Basic Syntax\n\n```\n.index create <name> on <relation>(<column>) [options]\n```\n\n### Simple Example\n\n```datalog\n// Create a documents table with embeddings\n+documents(id: int, title: string, embedding: vector)\n\n// Insert some documents\n+documents(1, \"Introduction to ML\", [0.1, 0.2, 0.3, 0.4])\n+documents(2, \"Vector Databases\", [0.15, 0.25, 0.28, 0.42])\n+documents(3, \"Graph Theory\", [0.8, 0.1, 0.05, 0.05])\n```\n\n```\n.index create doc_emb_idx on documents(embedding)\n```\n\n### With Options\n\n```\n.index create doc_emb_idx on documents(embedding) metric cosine m 16 ef_search 50\n```\n\n---\n\n## Index Options\n\n### Distance Metrics\n\n| Metric | Aliases | Use Case |\n|--------|---------|----------|\n| `cosine` | `cos` | Text embeddings (most common) |\n| `euclidean` | `l2`, `euclid` | Image embeddings |\n| `dot` | `dotproduct`, `inner` | When vectors have meaningful magnitude |\n| `manhattan` | `l1`, `taxicab` | Sparse vectors |\n\n**Default**: `cosine`\n\n```\n.index create my_idx on vectors(embedding) metric l2\n.index create my_idx on vectors(embedding) metric cosine\n.index create my_idx on vectors(embedding) metric dot\n```\n\n### HNSW Parameters\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `m` | 16 | Max connections per node (higher = better recall, more memory) |\n| `ef_construction` | 200 | Construction-time ef (higher = better quality, slower build) |\n| `ef_search` | 50 | Search-time ef (higher = better recall, slower search) |\n\n```\n.index create my_idx on vectors(embedding) m 32 ef_search 100\n```\n\n### Parameter Tuning\n\n**For higher recall (more accurate results):**\n```\n.index create my_idx on vectors(embedding) m 32 ef_construction 400 ef_search 100\n```\n\n**For faster search (lower recall):**\n```\n.index create my_idx on vectors(embedding) m 8 ef_search 20\n```\n\n**For large datasets (millions of vectors):**\n```\n.index create my_idx on vectors(embedding) m 48 ef_construction 500 ef_search 200\n```\n\n---\n\n## Managing Indexes\n\n### List All Indexes\n\n```\n.index\n```\n\nor\n\n```\n.index list\n```\n\n**Output:**\n\n| Name | Relation | Column | Type | Metric | Valid |\n|---|---|---|---|---|---|\n| doc_emb_idx | documents | embedding | hnsw | cosine | yes |\n\n### View Index Statistics\n\n```\n.index stats doc_emb_idx\n```\n\n**Output:**\n```\nIndex: doc_emb_idx\n  Relation:   documents\n  Column:     embedding\n  Type:       hnsw\n  Metric:     cosine\n  Vectors:    10000\n  Dimension:  768\n  Valid:      yes\n  Tombstones: 0\n  Built:      2024-01-15 10:30:00\n```\n\n### Rebuild an Index\n\nAfter many insertions/deletions, an index may become fragmented. Rebuild to optimize:\n\n```\n.index rebuild doc_emb_idx\n```\n\n### Drop an Index\n\n```\n.index drop doc_emb_idx\n```\n\n---\n\n## Using Indexes in Queries\n\nUse the `hnsw_nearest()` predicate in query bodies to perform fast approximate nearest-neighbor search via an HNSW index.\n\n### Syntax\n\n```\nhnsw_nearest(\"index_name\", QueryVec, K, IdVar, DistVar [, EfSearch])\n```\n\n- `index_name` — String literal naming the HNSW index\n- `QueryVec` — Variable bound to a vector, or a vector literal `[1.0, 2.0]`\n- `K` — Integer: number of nearest neighbors\n- `IdVar` — Variable to bind result tuple IDs\n- `DistVar` — Variable to bind distances\n- `EfSearch` — Optional integer: override ef_search for this query\n\n### Examples\n\n```datalog\n// Find 5 nearest documents to a literal query vector\n? hnsw_nearest(\"doc_emb_idx\", [0.11, 0.21, 0.29], 5, Id, Dist)\n\n// Use a bound query vector and join with base data\n? query_vec(QV), hnsw_nearest(\"doc_emb_idx\", QV, 10, Id, Dist), documents(Id, Title, _)\n\n// Override ef_search for higher recall\n? hnsw_nearest(\"doc_emb_idx\", [0.11, 0.21, 0.29], 5, Id, Dist, 200)\n```\n\n**Note:** Indexes are not used automatically — you must explicitly call `hnsw_nearest()` to use an HNSW index.\n\n---\n\n## Index Lifecycle\n\n### Build Phase\n\nWhen you create an index, vectors are inserted incrementally:\n\n1. Index is registered with metadata\n2. Existing vectors are added to the HNSW structure\n3. Index is marked as valid\n\n### Invalidation\n\nIndexes are automatically invalidated when:\n- Base relation is modified (insert/delete)\n- Schema changes\n\n```\n.index stats my_idx\n  Valid: no  ← Index needs rebuild\n```\n\n### Rebuild\n\nInvalid indexes are rebuilt on:\n- Explicit `.index rebuild` command\n- Next query that uses the index\n\n---\n\n## Index Architecture\n\n### HNSW Structure\n\n```\nLayer 3:  *-------------*\n          |             |\nLayer 2:  *---*-----*---*---*\n          |   |     |   |   |\nLayer 1:  *-*-*-*-*-*-*-*-*-*-*\n          | | | | | | | | | | |\nLayer 0:  *********************** (all nodes)\n```\n\nEach layer has fewer nodes. Search starts at top layer and descends:\n1. Find nearest nodes in current layer\n2. Use those as entry points for next layer\n3. Repeat until layer 0\n4. Return k nearest neighbors\n\n### Memory Usage\n\nHNSW indexes use approximately:\n\n```\nmemory approx n * (d * 4 + m * 8) bytes\n```\n\nWhere:\n- `n` = number of vectors\n- `d` = vector dimension\n- `m` = max connections parameter\n\n**Example**: 1M vectors × 768 dimensions × m=16:\n```\n1M × (768 × 4 + 16 × 8) = ~3.2 GB\n```\n\n---\n\n## Tombstones and Compaction\n\nWhen vectors are deleted, they're marked with a tombstone rather than removed immediately:\n\n```\n.index stats my_idx\n  Vectors:    10000\n  Tombstones: 500  ← Deleted entries not yet cleaned up\n```\n\n### Automatic Compaction\n\nWhen the tombstone ratio exceeds 30%, the index is automatically rebuilt inline during the delete operation.\n\n### Manual Compaction\n\nForce a rebuild to remove tombstones:\n\n```\n.index rebuild my_idx\n```\n\n---\n\n## Best Practices\n\n### 1. Choose the Right Metric\n\n| Embedding Type | Recommended Metric |\n|---------------|-------------------|\n| OpenAI embeddings | `cosine` |\n| BERT/Sentence-BERT | `cosine` |\n| Image embeddings (CLIP) | `cosine` |\n| Raw feature vectors | `euclidean` |\n| Pre-normalized vectors | `dot` (fastest) |\n\n### 2. Tune Parameters for Your Use Case\n\n**Discovery/Exploration** (higher recall matters):\n```\n.index create my_idx on docs(emb) m 32 ef_search 100\n```\n\n**Production/Speed** (latency matters):\n```\n.index create my_idx on docs(emb) m 16 ef_search 30\n```\n\n### 3. Monitor Index Health\n\nRegularly check:\n```\n.index stats my_idx\n```\n\nRebuild if:\n- Tombstone ratio > 30%\n- Search quality degrades\n- After bulk insertions\n\n### 4. Create Indexes Before Bulk Load\n\nFor large initial loads, create the index first:\n\n```datalog\n.index create my_idx on docs(emb)\n\n// Then bulk insert\n+docs[(1, \"...\", [0.1, ...]),\n      (2, \"...\", [0.2, ...]),\n      ...]\n```\n\n### 5. Use Appropriate Vector Dimensions\n\nCommon dimensions:\n- OpenAI text-embedding-3-small: 1536\n- Cohere embed-english-v3: 1024\n- all-MiniLM-L6-v2: 384\n- CLIP ViT-B/32: 512\n\nHigher dimensions = more memory, slower search.\n\n---\n\n## Troubleshooting\n\n### Index Shows \"Invalid\"\n\n**Cause**: Base relation was modified.\n\n**Solution**:\n```\n.index rebuild my_idx\n```\n\n### Search Returns No Results\n\n**Possible causes**:\n1. Index not yet built\n2. Query vector dimension mismatch\n3. No vectors in relation\n\n**Debug**:\n```datalog\n.index stats my_idx\n? documents(Id, _, V)  // Check if data exists\n```\n\n### Poor Search Quality\n\n**Causes**:\n1. Wrong distance metric for embedding type\n2. ef_search too low\n3. Many tombstones\n\n**Solutions**:\n```datalog\n// Check metric matches embedding type\n.index stats my_idx\n\n// Increase ef_search\n.index create my_idx on docs(emb) ef_search 100\n\n// Rebuild to remove tombstones\n.index rebuild my_idx\n```\n\n### High Memory Usage\n\n**Solutions**:\n1. Reduce `m` parameter (trades recall for memory)\n2. Use vector quantization (see [Vectors Guide](vectors))\n3. Consider approximate embeddings with lower dimensions\n\n---\n\n## Next Steps\n\n- [Vector Search Tutorial](vectors) - Distance functions and semantic search\n- [Configuration Guide](configuration) - Index persistence settings",
    "toc": [
      {
        "level": 2,
        "text": "Why Use Indexes?",
        "id": "why-use-indexes"
      },
      {
        "level": 2,
        "text": "Creating an Index",
        "id": "creating-an-index"
      },
      {
        "level": 3,
        "text": "Basic Syntax",
        "id": "basic-syntax"
      },
      {
        "level": 3,
        "text": "Simple Example",
        "id": "simple-example"
      },
      {
        "level": 3,
        "text": "With Options",
        "id": "with-options"
      },
      {
        "level": 2,
        "text": "Index Options",
        "id": "index-options"
      },
      {
        "level": 3,
        "text": "Distance Metrics",
        "id": "distance-metrics"
      },
      {
        "level": 3,
        "text": "HNSW Parameters",
        "id": "hnsw-parameters"
      },
      {
        "level": 3,
        "text": "Parameter Tuning",
        "id": "parameter-tuning"
      },
      {
        "level": 2,
        "text": "Managing Indexes",
        "id": "managing-indexes"
      },
      {
        "level": 3,
        "text": "List All Indexes",
        "id": "list-all-indexes"
      },
      {
        "level": 3,
        "text": "View Index Statistics",
        "id": "view-index-statistics"
      },
      {
        "level": 3,
        "text": "Rebuild an Index",
        "id": "rebuild-an-index"
      },
      {
        "level": 3,
        "text": "Drop an Index",
        "id": "drop-an-index"
      },
      {
        "level": 2,
        "text": "Using Indexes in Queries",
        "id": "using-indexes-in-queries"
      },
      {
        "level": 3,
        "text": "Syntax",
        "id": "syntax"
      },
      {
        "level": 3,
        "text": "Examples",
        "id": "examples"
      },
      {
        "level": 2,
        "text": "Index Lifecycle",
        "id": "index-lifecycle"
      },
      {
        "level": 3,
        "text": "Build Phase",
        "id": "build-phase"
      },
      {
        "level": 3,
        "text": "Invalidation",
        "id": "invalidation"
      },
      {
        "level": 3,
        "text": "Rebuild",
        "id": "rebuild"
      },
      {
        "level": 2,
        "text": "Index Architecture",
        "id": "index-architecture"
      },
      {
        "level": 3,
        "text": "HNSW Structure",
        "id": "hnsw-structure"
      },
      {
        "level": 3,
        "text": "Memory Usage",
        "id": "memory-usage"
      },
      {
        "level": 2,
        "text": "Tombstones and Compaction",
        "id": "tombstones-and-compaction"
      },
      {
        "level": 3,
        "text": "Automatic Compaction",
        "id": "automatic-compaction"
      },
      {
        "level": 3,
        "text": "Manual Compaction",
        "id": "manual-compaction"
      },
      {
        "level": 2,
        "text": "Best Practices",
        "id": "best-practices"
      },
      {
        "level": 3,
        "text": "1. Choose the Right Metric",
        "id": "1-choose-the-right-metric"
      },
      {
        "level": 3,
        "text": "2. Tune Parameters for Your Use Case",
        "id": "2-tune-parameters-for-your-use-case"
      },
      {
        "level": 3,
        "text": "3. Monitor Index Health",
        "id": "3-monitor-index-health"
      },
      {
        "level": 3,
        "text": "4. Create Indexes Before Bulk Load",
        "id": "4-create-indexes-before-bulk-load"
      },
      {
        "level": 3,
        "text": "5. Use Appropriate Vector Dimensions",
        "id": "5-use-appropriate-vector-dimensions"
      },
      {
        "level": 2,
        "text": "Troubleshooting",
        "id": "troubleshooting"
      },
      {
        "level": 3,
        "text": "Index Shows \"Invalid\"",
        "id": "index-shows-invalid"
      },
      {
        "level": 3,
        "text": "Search Returns No Results",
        "id": "search-returns-no-results"
      },
      {
        "level": 3,
        "text": "Poor Search Quality",
        "id": "poor-search-quality"
      },
      {
        "level": 3,
        "text": "High Memory Usage",
        "id": "high-memory-usage"
      },
      {
        "level": 2,
        "text": "Next Steps",
        "id": "next-steps"
      }
    ]
  },
  "guides/installation": {
    "title": "Installation",
    "content": "# Installation\n\nInstall and run InputLayer on your system.\n\n## Prerequisites\n\n- **Rust 1.88+** - InputLayer is written in Rust\n- **Cargo** - Rust's package manager (comes with Rust)\n\n### Installing Rust\n\nIf you don't have Rust installed, use [rustup](https://rustup.rs/):\n\n```bash\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\nAfter installation, restart your terminal or run:\n```bash\nsource $HOME/.cargo/env\n```\n\nVerify installation:\n```bash\nrustc --version  # Should show 1.88.0 or higher\ncargo --version\n```\n\n## Installation Options\n\n### Option 1: Install from crates.io (Recommended)\n\n```bash\ncargo install inputlayer --bin inputlayer-client\n```\n\nThis installs the `inputlayer-client` binary to `~/.cargo/bin/`.\n\n### Option 2: Build from Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/inputlayer/inputlayer.git\ncd inputlayer\n\n# Build in release mode\ncargo build --release\n\n# The binaries are in target/release/\n./target/release/inputlayer-server   # Start server first\n./target/release/inputlayer-client   # Then connect with client\n```\n\n### Option 3: Run without Installing\n\nFor quick testing without installing:\n\n```bash\ncargo run --release --bin inputlayer-server &\ncargo run --release --bin inputlayer-client\n```\n\n## Verify Installation\n\nStart the server, then connect with the client:\n\n```bash\ninputlayer-server &\ninputlayer-client\n```\n\nYou should see:\n\n```\nConnecting to server at http://127.0.0.1:8080...\nConnected!\n\nServer status: healthy\nAuthenticated as: admin\nCurrent knowledge graph: default\n```\n\nTry a simple command:\n\n```\n> .kg list\nKnowledge graphs:\n  default\n```\n\nType `.quit` to exit.\n\n## Configuration (Optional)\n\nInputLayer works out of the box with sensible defaults. For customization, create a config file:\n\n**Location options:**\n1. `./config.toml` (current directory, auto-loaded)\n2. `./config.local.toml` (local overrides, git-ignored, auto-loaded)\n3. Custom path via `--config <path>` CLI flag\n\n**Example configuration:**\n\n```toml\n[storage]\ndata_dir = \"./data\"\ndefault_knowledge_graph = \"default\"\n\n[storage.performance]\nnum_threads = 4\n\n[storage.persist]\nbuffer_size = 10000\ndurability_mode = \"batched\"\n```\n\n## Data Directory\n\nBy default, InputLayer stores data in `./data/` (current working directory). Set `data_dir` in the config file to customize.\n\nThe data directory contains:\n```\n./data/\n├── default/           # Default knowledge graph\n│   └── rules/         # Persistent rule definitions (catalog.json)\n├── persist/           # DD-native persist layer\n│   ├── shards/        # Shard metadata\n│   ├── batches/       # Compacted batch files\n│   └── wal/           # Write-ahead log\n└── metadata/          # System metadata\n```\n\n## Next Steps\n\nNow that you have InputLayer installed:\n\n1. **[Your First Program](first-program)** - Write your first Datalog program\n2. **[Core Concepts](core-concepts)** - Understand facts, rules, and queries\n3. **[REPL Guide](repl)** - Master the interactive environment\n\n## Troubleshooting\n\n### Command not found: inputlayer-client\n\nMake sure `~/.cargo/bin` is in your PATH:\n\n```bash\nexport PATH=\"$HOME/.cargo/bin:$PATH\"\n```\n\nAdd this line to your `~/.bashrc` or `~/.zshrc` for persistence.\n\n### Build fails with missing dependencies\n\nOn some Linux systems, you may need development libraries:\n\n```bash\n# Ubuntu/Debian\nsudo apt-get install build-essential pkg-config libssl-dev\n\n# Fedora\nsudo dnf install gcc pkg-config openssl-devel\n\n# macOS (if needed)\nxcode-select --install\n```\n\n### Permission denied when writing data\n\nEnsure you have write permissions to the data directory:\n\n```bash\nmkdir -p ./data\nchmod 755 ./data\n```",
    "toc": [
      {
        "level": 2,
        "text": "Prerequisites",
        "id": "prerequisites"
      },
      {
        "level": 3,
        "text": "Installing Rust",
        "id": "installing-rust"
      },
      {
        "level": 2,
        "text": "Installation Options",
        "id": "installation-options"
      },
      {
        "level": 3,
        "text": "Option 1: Install from crates.io (Recommended)",
        "id": "option-1-install-from-cratesio-recommended"
      },
      {
        "level": 3,
        "text": "Option 2: Build from Source",
        "id": "option-2-build-from-source"
      },
      {
        "level": 3,
        "text": "Option 3: Run without Installing",
        "id": "option-3-run-without-installing"
      },
      {
        "level": 2,
        "text": "Verify Installation",
        "id": "verify-installation"
      },
      {
        "level": 2,
        "text": "Configuration (Optional)",
        "id": "configuration-optional"
      },
      {
        "level": 2,
        "text": "Data Directory",
        "id": "data-directory"
      },
      {
        "level": 2,
        "text": "Next Steps",
        "id": "next-steps"
      },
      {
        "level": 2,
        "text": "Troubleshooting",
        "id": "troubleshooting"
      },
      {
        "level": 3,
        "text": "Command not found: inputlayer-client",
        "id": "command-not-found-inputlayer-client"
      },
      {
        "level": 3,
        "text": "Build fails with missing dependencies",
        "id": "build-fails-with-missing-dependencies"
      },
      {
        "level": 3,
        "text": "Permission denied when writing data",
        "id": "permission-denied-when-writing-data"
      }
    ]
  },
  "guides/migrations": {
    "title": "Migrations",
    "content": "# Migrations\n\nThe migration system provides Django-style schema versioning for InputLayer. Generate numbered migration files from model diffs, apply them to a server, revert if needed, and track what's deployed.\n\n## Why Migrations?\n\nWithout migrations, `define()` and `define_rules()` fire Datalog commands blindly:\n\n- **No idempotency** — calling `define_rules()` twice duplicates clauses\n- **No rollback** — can't revert a bad deploy\n- **No state tracking** — no record of what's deployed\n\nMigrations solve all of these by providing a versioned, reversible record of schema changes.\n\n## Quick Start\n\n### 1. Define Models\n\n```python\nfrom inputlayer import Relation, Derived, Vector, From\n\nclass Employee(Relation):\n    id: int\n    name: str\n    department: str\n    salary: float\n\nclass Senior(Derived):\n    name: str\n    rules = [\n        From(Employee)\n            .where(lambda e: e.salary > 100000)\n            .select(name=Employee.name),\n    ]\n```\n\n### 2. Generate Migration\n\n```bash\ninputlayer-migrate makemigrations --models myapp.models\n# → creates migrations/0001_initial.py\n```\n\n### 3. Apply\n\n```bash\ninputlayer-migrate migrate --url ws://localhost:8080/ws --kg production\n```\n\n### 4. Iterate\n\nChange your models, then generate and apply again:\n\n```bash\ninputlayer-migrate makemigrations --models myapp.models\n# → creates migrations/0002_auto.py\n\ninputlayer-migrate migrate --url ws://localhost:8080/ws --kg production\n```\n\n### 5. Rollback\n\nIf something goes wrong:\n\n```bash\ninputlayer-migrate revert --url ws://localhost:8080/ws --kg production 0001_initial\n```\n\n### 6. Check Status\n\n```bash\ninputlayer-migrate showmigrations --url ws://localhost:8080/ws --kg production\n# [X] 0001_initial\n# [ ] 0002_auto\n```\n\n## CLI Commands\n\n| Command | Description |\n|---------|-------------|\n| `makemigrations` | Generate a migration from model diffs |\n| `migrate` | Apply pending migrations |\n| `revert` | Rollback to a specific migration |\n| `showmigrations` | Show applied/pending status |\n\n### Common Options\n\n| Option | Description |\n|--------|-------------|\n| `--url` | WebSocket URL (`ws://host:port/ws`) |\n| `--kg` | Target knowledge graph |\n| `--models` | Python module path containing model definitions |\n| `--migrations-dir` | Directory for migration files (default: `migrations/`) |\n\n## Migration File Anatomy\n\nEach migration is a self-contained Python file:\n\n```python\n# migrations/0001_initial.py\nfrom inputlayer.migrations import Migration\nfrom inputlayer.migrations import operations as ops\n\nclass M(Migration):\n    dependencies = []\n\n    operations = [\n        ops.CreateRelation(\n            name=\"employee\",\n            columns=[(\"id\", \"int\"), (\"name\", \"string\"), (\"salary\", \"float\")],\n        ),\n        ops.CreateRule(\n            name=\"senior\",\n            clauses=[\"+senior(Name) <- employee(_, Name, Salary), Salary > 100000\"],\n        ),\n    ]\n\n    state = {\n        \"relations\": {\n            \"employee\": [(\"id\", \"int\"), (\"name\", \"string\"), (\"salary\", \"float\")],\n        },\n        \"rules\": {\n            \"senior\": [\"+senior(Name) <- employee(_, Name, Salary), Salary > 100000\"],\n        },\n        \"indexes\": {},\n    }\n```\n\n### Key Fields\n\n- **`dependencies`**: List of migration names that must be applied first\n- **`operations`**: Ordered list of forward operations\n- **`state`**: Full snapshot of the model state after this migration — used for diffing\n\n## Operations Reference\n\n| Operation | Forward | Backward |\n|-----------|---------|----------|\n| `CreateRelation(name, columns)` | Schema definition | `.rel drop` |\n| `DropRelation(name, columns)` | `.rel drop` | Schema definition |\n| `CreateRule(name, clauses)` | Rule clauses | `.rule drop` |\n| `DropRule(name, clauses)` | `.rule drop` | Rule clauses |\n| `ReplaceRule(name, old, new)` | Drop + new clauses | Drop + old clauses |\n| `CreateIndex(name, ...)` | `.index create` | `.index drop` |\n| `DropIndex(name, ...)` | `.index drop` | `.index create` |\n| `RunDatalog(fwd, bwd)` | Custom commands | Custom commands |\n\n### Custom Operations\n\nUse `RunDatalog` for operations not covered by built-in ops:\n\n```python\nops.RunDatalog(\n    forward=[\"+edge[(1,2), (2,3), (3,4)]\"],\n    backward=[\"-edge(X, Y) <- edge(X, Y)\"],\n)\n```\n\n## Development Workflow\n\n### Local Development\n\n```bash\n# Start local server\ninputlayer-server\n\n# Generate initial migration\ninputlayer-migrate makemigrations --models myapp.models\n\n# Apply locally\ninputlayer-migrate migrate --url ws://localhost:8080/ws --kg dev\n\n# Make changes to models...\n# Generate diff migration\ninputlayer-migrate makemigrations --models myapp.models\n\n# Apply changes\ninputlayer-migrate migrate --url ws://localhost:8080/ws --kg dev\n```\n\n### Team Workflow\n\n1. Migrations are committed to version control alongside model changes\n2. Each developer runs `migrate` against their local server\n3. In code review, check that migrations match model changes\n4. Never edit or delete an applied migration — always create new ones\n\n## CI/CD Integration\n\n### GitHub Actions Example\n\n```yaml\n- name: Apply migrations\n  run: |\n    inputlayer-migrate migrate \\\n      --url ${{ secrets.IL_WS_URL }} \\\n      --kg production \\\n      --username admin \\\n      --password ${{ secrets.IL_ADMIN_PASSWORD }}\n```\n\n### Pre-deploy Validation\n\nCheck that migrations are valid before deploying:\n\n```bash\n# Dry run — validates without applying\ninputlayer-migrate migrate --url ws://staging:8080/ws --kg staging --dry-run\n```\n\n## Programmatic Usage\n\nUse the migration system from Python code:\n\n```python\nfrom inputlayer.migrations.autodetector import detect_changes\nfrom inputlayer.migrations.state import ModelState\nfrom inputlayer.migrations.writer import generate_migration\n\n# Build state from current models\nstate = ModelState.from_models(\n    relations=[Employee, Department],\n    derived=[Reachable],\n    indexes=[doc_idx],\n)\n\n# Diff against empty → initial migration\nops = detect_changes(ModelState(), state)\nfilename, content = generate_migration(1, ops, state.to_dict(), [])\n```\n\n## Best Practices\n\n1. **One migration per logical change** — don't mix unrelated schema changes\n2. **Never modify applied migrations** — create new ones instead\n3. **Test migrations** against a staging environment before production\n4. **Include backward operations** — always ensure migrations can be reverted\n5. **Commit migrations** alongside the model changes that generated them\n6. **Use descriptive names** — rename auto-generated files to describe the change",
    "toc": [
      {
        "level": 2,
        "text": "Why Migrations?",
        "id": "why-migrations"
      },
      {
        "level": 2,
        "text": "Quick Start",
        "id": "quick-start"
      },
      {
        "level": 3,
        "text": "1. Define Models",
        "id": "1-define-models"
      },
      {
        "level": 3,
        "text": "2. Generate Migration",
        "id": "2-generate-migration"
      },
      {
        "level": 3,
        "text": "3. Apply",
        "id": "3-apply"
      },
      {
        "level": 3,
        "text": "4. Iterate",
        "id": "4-iterate"
      },
      {
        "level": 3,
        "text": "5. Rollback",
        "id": "5-rollback"
      },
      {
        "level": 3,
        "text": "6. Check Status",
        "id": "6-check-status"
      },
      {
        "level": 2,
        "text": "CLI Commands",
        "id": "cli-commands"
      },
      {
        "level": 3,
        "text": "Common Options",
        "id": "common-options"
      },
      {
        "level": 2,
        "text": "Migration File Anatomy",
        "id": "migration-file-anatomy"
      },
      {
        "level": 3,
        "text": "Key Fields",
        "id": "key-fields"
      },
      {
        "level": 2,
        "text": "Operations Reference",
        "id": "operations-reference"
      },
      {
        "level": 3,
        "text": "Custom Operations",
        "id": "custom-operations"
      },
      {
        "level": 2,
        "text": "Development Workflow",
        "id": "development-workflow"
      },
      {
        "level": 3,
        "text": "Local Development",
        "id": "local-development"
      },
      {
        "level": 3,
        "text": "Team Workflow",
        "id": "team-workflow"
      },
      {
        "level": 2,
        "text": "CI/CD Integration",
        "id": "cicd-integration"
      },
      {
        "level": 3,
        "text": "GitHub Actions Example",
        "id": "github-actions-example"
      },
      {
        "level": 3,
        "text": "Pre-deploy Validation",
        "id": "pre-deploy-validation"
      },
      {
        "level": 2,
        "text": "Programmatic Usage",
        "id": "programmatic-usage"
      },
      {
        "level": 2,
        "text": "Best Practices",
        "id": "best-practices"
      }
    ]
  },
  "guides/persistence": {
    "title": "Persistence Guide",
    "content": "# Persistence Guide\n\nInputLayer provides durable storage with crash recovery, using a combination of Write-Ahead Logging (WAL) and Parquet batch files.\n\n## Architecture Overview\n\n```\nInsert/Delete\n    ↓\nUpdate{data, time, diff}\n    ↓\nWAL (immediate durability)\n    ↓\nIn-memory buffer\n    ↓ (when buffer full)\nBatch file (Parquet)\n```\n\n### Recovery Flow\n\nOn startup, InputLayer:\n1. Loads shard metadata from disk\n2. Reads batch files (Parquet)\n3. Replays WAL (uncommitted updates)\n4. Consolidates to get current state\n\n---\n\n## Directory Structure\n\n```\ndata/\n├── persist/\n│   ├── shards/           # Shard metadata (JSON)\n│   │   ├── default_edge.json\n│   │   └── default_node.json\n│   ├── batches/          # Data files (Parquet)\n│   │   ├── 1.parquet\n│   │   └── 2.parquet\n│   └── wal/              # Write-ahead log\n│       └── current.wal\n```\n\n---\n\n## Durability Modes\n\nInputLayer offers three durability modes, configurable via `config.toml`:\n\n### Immediate Mode (Default)\n\nEvery write syncs to disk before returning:\n\n```toml\n[storage.persist]\ndurability_mode = \"immediate\"\n```\n\n| Property | Value |\n|----------|-------|\n| Write Latency | Highest |\n| Crash Safety | Full (zero data loss) |\n| Use Case | Financial data, critical records |\n\n### Batched Mode\n\nWrites buffer in memory with periodic sync:\n\n```toml\n[storage.persist]\ndurability_mode = \"batched\"\nbuffer_size = 10000\n```\n\n| Property | Value |\n|----------|-------|\n| Write Latency | Medium |\n| Crash Safety | Partial (may lose last batch) |\n| Use Case | Most production workloads |\n\n### Async Mode\n\nWrites return immediately; background persistence:\n\n```toml\n[storage.persist]\ndurability_mode = \"async\"\n```\n\n| Property | Value |\n|----------|-------|\n| Write Latency | Lowest |\n| Crash Safety | Minimal (may lose recent updates) |\n| Use Case | Analytics pipelines, high-throughput ingestion |\n\n---\n\n## Write-Ahead Log (WAL)\n\nThe WAL provides O(1) append-only persistence with immediate durability.\n\n### WAL Entry Format\n\nEach entry is a checksummed JSON line (format: `<crc32hex>:<json>`):\n\n```json\n{\"shard\":\"default:edge\",\"update\":{\"data\":[1,2],\"time\":1,\"diff\":1}}\n{\"shard\":\"default:edge\",\"update\":{\"data\":[1,2],\"time\":2,\"diff\":-1}}\n```\n\n### WAL Entry Fields\n\n| Field | Description |\n|-------|-------------|\n| `shard` | Shard name in `\"kg:relation\"` format |\n| `update.data` | The tuple data |\n| `update.time` | Logical timestamp (monotonically increasing) |\n| `update.diff` | Multiplicity change: `+1` for insert, `-1` for delete |\n\nEach line is prefixed with a CRC32 checksum for integrity verification during recovery.\n\n### Automatic Compaction\n\nWAL entries are compacted to Parquet when:\n- Buffer reaches configured size (`buffer_size`)\n- Manual flush is triggered\n- Server shutdown (clean)\n\nAfter compaction, the WAL is archived and cleared.\n\n---\n\n## Batch Files (Parquet)\n\nData is stored in columnar Parquet format for efficient queries.\n\n### Parquet Schema\n\nEach batch file contains:\n- **Data columns**: Your tuple fields\n- **time**: Update timestamp (UInt64)\n- **diff**: +1 for insert, -1 for delete (Int64)\n\n### Compression\n\nSnappy compression is used by default (fast decompression, good ratio).\n\n### Example File\n\n```\nbatches/1.parquet\n├── col0: Int32 [1, 3, 5]\n├── col1: Int32 [2, 4, 6]\n├── time: UInt64 [10, 20, 30]\n└── diff: Int64 [1, 1, 1]\n```\n\n---\n\n## Shards\n\nEach relation is stored as a separate \"shard\" with its own:\n- Metadata file (JSON)\n- Batch files (Parquet)\n- WAL entries\n\n### Shard Metadata\n\n```json\n{\n  \"name\": \"default:edge\",\n  \"since\": 0,\n  \"upper\": 100,\n  \"batches\": [\n    {\n      \"id\": \"1\",\n      \"path\": \"batches/1.parquet\",\n      \"lower\": 0,\n      \"upper\": 50,\n      \"len\": 100\n    }\n  ]\n}\n```\n\n| Field | Description |\n|-------|-------------|\n| `name` | Shard identifier (kg:relation) |\n| `since` | Lower bound frontier (history discarded before this) |\n| `upper` | Upper bound frontier (latest update time + 1) |\n| `batches` | List of batch file references |\n\n---\n\n## Compaction\n\nCompaction consolidates history and reclaims space.\n\n### Manual Compaction\n\n```datalog\n.compact\n```\n\nThis:\n1. Flushes all pending writes\n2. Merges batch files\n3. Removes historical entries before `since` frontier\n4. Clears WAL\n\n### Automatic Compaction\n\nCompaction is triggered automatically when a shard accumulates more than `auto_compact_threshold` batch files (default: 10). The check runs every `auto_compact_interval_secs` seconds (default: 300).\n\n```toml\n[storage.persist]\nauto_compact_threshold = 10      # Compact after this many batch files\nauto_compact_interval_secs = 300  # Check interval (seconds)\n```\n\n---\n\n## Configuration Reference\n\n```toml\n[storage]\n# Base directory for all data\ndata_dir = \"./data\"\n\n[storage.persist]\n# Enable DD-native persistence\nenabled = true\n\n# Buffer size before flushing to Parquet\nbuffer_size = 10000\n\n# Durability: immediate, batched, async\ndurability_mode = \"immediate\"\n\n# Auto-compact when this many batch files accumulate (0 = disabled)\nauto_compact_threshold = 10\n\n# Check interval for auto-compaction in seconds (0 = disabled)\nauto_compact_interval_secs = 300\n```\n\n---\n\n## Best Practices\n\n### Development\n\n```toml\n[storage.persist]\ndurability_mode = \"async\"\nbuffer_size = 1000\n```\n\nFast iteration, acceptable data loss on crashes.\n\n### Production\n\n```toml\n[storage.persist]\ndurability_mode = \"immediate\"\nbuffer_size = 10000\n```\n\nMaximum safety, reasonable performance.\n\n### High-Throughput Ingestion\n\n```toml\n[storage.persist]\ndurability_mode = \"batched\"\nbuffer_size = 100000\n\n[storage.performance]\nbatch_size = 10000\nasync_io = true\n```\n\nBalance between throughput and safety.\n\n### Memory-Constrained\n\n```toml\n[storage.persist]\nbuffer_size = 1000\nauto_compact_threshold = 5\n\n[storage.performance]\ninitial_capacity = 1000\nbatch_size = 100\n```\n\nFrequent flushes, aggressive compaction.\n\n---\n\n## Monitoring\n\n### Check Storage Status\n\n```bash\n.status\n```\n\nShows:\n- Data directory location\n- Number of shards\n- WAL size\n- Buffer status\n\n### Check Shard Info\n\n```datalog\n.rel\n```\n\nLists all relations with row counts.\n\n---\n\n## Recovery Scenarios\n\n### Normal Startup\n\n1. Load shard metadata\n2. Read Parquet batch files\n3. Replay WAL entries\n4. Consolidate to current state\n\n### Crash Recovery\n\nSame as normal startup. WAL ensures all committed writes are recovered.\n\n### Corrupted Parquet\n\nIf a batch file is corrupted:\n1. WAL entries for that batch may still be available\n2. Manually remove corrupted `.parquet` file\n3. Restart to trigger WAL replay\n\n### Corrupted WAL\n\nIf WAL is corrupted:\n1. Data in Parquet files is safe\n2. Uncommitted writes since last flush are lost\n3. Rename/remove corrupted WAL file\n4. Restart\n\n---\n\n## Differential Updates\n\nInputLayer uses differential dataflow semantics internally:\n\n```rust\nUpdate {\n    data: Tuple,    // The actual data\n    time: u64,      // Logical timestamp\n    diff: i64,      // +1 = insert, -1 = delete\n}\n```\n\n### Consolidation\n\nMultiple updates to the same tuple are consolidated:\n\n| Updates | Consolidated |\n|---------|-------------|\n| +1, +1 | +2 (duplicate insert) |\n| +1, -1 | 0 (cancelled out) |\n| +1, -1, +1 | +1 (net insert) |\n\nThis enables:\n- Efficient delta storage\n- Time-travel queries (if history preserved)\n- Incremental computation\n\n---\n\n## Troubleshooting\n\n### High Write Latency\n\n**Cause**: Immediate durability mode with slow disk\n\n**Solutions**:\n1. Switch to `batched` durability mode\n2. Use faster storage (SSD)\n3. Increase `buffer_size` to batch more writes\n\n### High Memory Usage\n\n**Cause**: Large buffer, many shards\n\n**Solutions**:\n1. Reduce `buffer_size`\n2. Lower `auto_compact_threshold` for more frequent compaction\n3. Flush more frequently\n\n### Slow Startup\n\n**Cause**: Large WAL, many batch files\n\n**Solutions**:\n1. Run `.compact` before shutdown\n2. Enable compaction window\n3. Flush buffers before shutdown\n\n### Missing Data After Crash\n\n**Cause**: Async durability mode\n\n**Solutions**:\n1. Switch to `immediate` or `batched` mode\n2. Accept trade-off for async mode\n\n---\n\n## Next Steps\n\n- [Configuration Guide](configuration) - Full configuration reference\n- [REST API Guide](rest-api) - Programmatic access\n- [Temporal Functions](temporal) - Time-based queries on persisted data",
    "toc": [
      {
        "level": 2,
        "text": "Architecture Overview",
        "id": "architecture-overview"
      },
      {
        "level": 3,
        "text": "Recovery Flow",
        "id": "recovery-flow"
      },
      {
        "level": 2,
        "text": "Directory Structure",
        "id": "directory-structure"
      },
      {
        "level": 2,
        "text": "Durability Modes",
        "id": "durability-modes"
      },
      {
        "level": 3,
        "text": "Immediate Mode (Default)",
        "id": "immediate-mode-default"
      },
      {
        "level": 3,
        "text": "Batched Mode",
        "id": "batched-mode"
      },
      {
        "level": 3,
        "text": "Async Mode",
        "id": "async-mode"
      },
      {
        "level": 2,
        "text": "Write-Ahead Log (WAL)",
        "id": "write-ahead-log-wal"
      },
      {
        "level": 3,
        "text": "WAL Entry Format",
        "id": "wal-entry-format"
      },
      {
        "level": 3,
        "text": "WAL Entry Fields",
        "id": "wal-entry-fields"
      },
      {
        "level": 3,
        "text": "Automatic Compaction",
        "id": "automatic-compaction"
      },
      {
        "level": 2,
        "text": "Batch Files (Parquet)",
        "id": "batch-files-parquet"
      },
      {
        "level": 3,
        "text": "Parquet Schema",
        "id": "parquet-schema"
      },
      {
        "level": 3,
        "text": "Compression",
        "id": "compression"
      },
      {
        "level": 3,
        "text": "Example File",
        "id": "example-file"
      },
      {
        "level": 2,
        "text": "Shards",
        "id": "shards"
      },
      {
        "level": 3,
        "text": "Shard Metadata",
        "id": "shard-metadata"
      },
      {
        "level": 2,
        "text": "Compaction",
        "id": "compaction"
      },
      {
        "level": 3,
        "text": "Manual Compaction",
        "id": "manual-compaction"
      },
      {
        "level": 3,
        "text": "Automatic Compaction",
        "id": "automatic-compaction"
      },
      {
        "level": 2,
        "text": "Configuration Reference",
        "id": "configuration-reference"
      },
      {
        "level": 2,
        "text": "Best Practices",
        "id": "best-practices"
      },
      {
        "level": 3,
        "text": "Development",
        "id": "development"
      },
      {
        "level": 3,
        "text": "Production",
        "id": "production"
      },
      {
        "level": 3,
        "text": "High-Throughput Ingestion",
        "id": "high-throughput-ingestion"
      },
      {
        "level": 3,
        "text": "Memory-Constrained",
        "id": "memory-constrained"
      },
      {
        "level": 2,
        "text": "Monitoring",
        "id": "monitoring"
      },
      {
        "level": 3,
        "text": "Check Storage Status",
        "id": "check-storage-status"
      },
      {
        "level": 3,
        "text": "Check Shard Info",
        "id": "check-shard-info"
      },
      {
        "level": 2,
        "text": "Recovery Scenarios",
        "id": "recovery-scenarios"
      },
      {
        "level": 3,
        "text": "Normal Startup",
        "id": "normal-startup"
      },
      {
        "level": 3,
        "text": "Crash Recovery",
        "id": "crash-recovery"
      },
      {
        "level": 3,
        "text": "Corrupted Parquet",
        "id": "corrupted-parquet"
      },
      {
        "level": 3,
        "text": "Corrupted WAL",
        "id": "corrupted-wal"
      },
      {
        "level": 2,
        "text": "Differential Updates",
        "id": "differential-updates"
      },
      {
        "level": 3,
        "text": "Consolidation",
        "id": "consolidation"
      },
      {
        "level": 2,
        "text": "Troubleshooting",
        "id": "troubleshooting"
      },
      {
        "level": 3,
        "text": "High Write Latency",
        "id": "high-write-latency"
      },
      {
        "level": 3,
        "text": "High Memory Usage",
        "id": "high-memory-usage"
      },
      {
        "level": 3,
        "text": "Slow Startup",
        "id": "slow-startup"
      },
      {
        "level": 3,
        "text": "Missing Data After Crash",
        "id": "missing-data-after-crash"
      },
      {
        "level": 2,
        "text": "Next Steps",
        "id": "next-steps"
      }
    ]
  },
  "guides/python-sdk": {
    "title": "Python SDK",
    "content": "# Python SDK\n\nInputLayer ships a Python Object-Logic Mapper (OLM) that lets you define schemas, insert data, run queries, and manage rules using pure Python - no Datalog syntax required. The OLM compiles Python expressions into Datalog over WebSocket.\n\n## Installation\n\n```bash\npip install inputlayer\n\n# With pandas support\npip install inputlayer[pandas]\n```\n\n**Requirements**: Python 3.10+, a running InputLayer server.\n\n## Connecting\n\n```python\n\nfrom inputlayer import InputLayer\n\nasync def main():\n    async with InputLayer(\"ws://localhost:8080/ws\", username=\"admin\", password=\"admin\") as il:\n        print(f\"Connected, server version: {il.server_version}\")\n\nasyncio.run(main())\n```\n\nAuthentication supports username/password or API keys:\n\n```python\n# API key auth\nasync with InputLayer(\"ws://localhost:8080/ws\", api_key=\"your-key-here\") as il:\n    ...\n```\n\n## Defining Schemas\n\nDefine typed relations as Python classes:\n\n```python\nfrom inputlayer import Relation, Vector, Timestamp\n\nclass Employee(Relation):\n    id: int\n    name: str\n    department: str\n    salary: float\n    active: bool\n\nclass Document(Relation):\n    id: int\n    title: str\n    embedding: Vector[384]\n    created_at: Timestamp\n```\n\nDeploy to the server:\n\n```python\nkg = il.knowledge_graph(\"myapp\")\nawait kg.define(Employee, Document)\n```\n\n`define()` is idempotent - calling it again on an existing relation is safe.\n\n### Supported Types\n\n| Python Type | Datalog Type | Description |\n|-------------|-------------|-------------|\n| `int` | `int` | 64-bit integer |\n| `float` | `float` | 64-bit floating point |\n| `str` | `string` | UTF-8 string |\n| `bool` | `bool` | Boolean |\n| `Vector[N]` | `vector(N)` | N-dimensional float vector |\n| `VectorInt8[N]` | `vector_int8(N)` | N-dimensional int8 vector |\n| `Timestamp` | `timestamp` | Unix epoch milliseconds |\n\n## Inserting Data\n\n```python\n# Single fact\nawait kg.insert(Employee(id=1, name=\"Alice\", department=\"eng\", salary=120000.0, active=True))\n\n# Batch insert\nawait kg.insert([\n    Employee(id=2, name=\"Bob\", department=\"hr\", salary=90000.0, active=True),\n    Employee(id=3, name=\"Charlie\", department=\"eng\", salary=110000.0, active=False),\n])\n\n# From a pandas DataFrame\n\ndf = pd.DataFrame({\"id\": [4, 5], \"name\": [\"Dave\", \"Eve\"], ...})\nawait kg.insert(Employee, data=df)\n```\n\n## Querying\n\n### Basic Queries\n\n```python\n# All employees\nresult = await kg.query(Employee)\nfor emp in result:\n    print(f\"{emp.name}: ${emp.salary}\")\n\n# With filter\nengineers = await kg.query(\n    Employee,\n    where=lambda e: (e.department == \"eng\") & (e.active == True),\n)\n```\n\n### Column Selection\n\n```python\nresult = await kg.query(\n    Employee.name, Employee.salary,\n    join=[Employee],\n    where=lambda e: e.department == \"eng\",\n)\n```\n\n### Joins\n\n```python\nclass Department(Relation):\n    name: str\n    budget: float\n\nresult = await kg.query(\n    Employee.name, Department.budget,\n    join=[Employee, Department],\n    on=lambda e, d: e.department == d.name,\n)\n```\n\n### Aggregations\n\n```python\nfrom inputlayer import count, sum_, avg, min_, max_\n\nresult = await kg.query(\n    Employee.department,\n    count(Employee.id),\n    avg(Employee.salary),\n    join=[Employee],\n)\n```\n\n### Ordering and Pagination\n\n```python\nresult = await kg.query(\n    Employee,\n    order_by=Employee.salary.desc(),\n    limit=10,\n    offset=20,\n)\n```\n\n## Derived Relations (Rules)\n\nDefine computed views with recursive logic:\n\n```python\nfrom typing import ClassVar\nfrom inputlayer import Derived, From\n\nclass Edge(Relation):\n    src: int\n    dst: int\n\nclass Reachable(Derived):\n    src: int\n    dst: int\n    rules: ClassVar[list] = []\n\n# Base case + recursive case\nReachable.rules = [\n    From(Edge).select(src=Edge.src, dst=Edge.dst),\n    From(Reachable, Edge)\n        .where(lambda r, e: r.dst == e.src)\n        .select(src=Reachable.src, dst=Edge.dst),\n]\n\n# Deploy and query\nawait kg.define_rules(Reachable)\nresult = await kg.query(Reachable, where=lambda r: r.src == 1)\n```\n\n## Vector Search\n\n```python\nfrom inputlayer import HnswIndex\n\n# Create index\nawait kg.create_index(HnswIndex(\n    name=\"doc_emb_idx\",\n    relation=Document,\n    column=\"embedding\",\n    metric=\"cosine\",\n    m=32,\n    ef_construction=200,\n))\n\n# Search\nresult = await kg.vector_search(\n    Document,\n    query_vec=[0.1, 0.2, ...],\n    k=10,\n    metric=\"cosine\",\n)\n```\n\n## Session Rules\n\nEphemeral rules scoped to the current WebSocket connection:\n\n```python\nawait kg.session.define_rules(MyTempView)\nresult = await kg.query(MyTempView, join=[MyTempView])\nawait kg.session.clear()  # or just disconnect\n```\n\n## Notifications\n\nSubscribe to real-time data change events:\n\n```python\n@il.on(\"persistent_update\", relation=\"sensor_reading\")\ndef on_update(event):\n    print(f\"[{event.relation}] {event.count} rows changed\")\n```\n\n## Migrations\n\nThe migration system provides Django-style schema versioning. See [Migrations](migrations) for full documentation.\n\n## Error Handling\n\n```python\nfrom inputlayer import (\n    InputLayerError,\n    AuthenticationError,\n    KnowledgeGraphNotFoundError,\n    SchemaConflictError,\n)\n\ntry:\n    await kg.define(Employee)\nexcept AuthenticationError:\n    print(\"Bad credentials\")\nexcept SchemaConflictError as e:\n    print(f\"Schema mismatch: {e.conflicts}\")\nexcept InputLayerError as e:\n    print(f\"Server error: {e}\")\n```\n\n## Sync Client\n\nFor scripts and non-async contexts:\n\n```python\nfrom inputlayer import InputLayerSync\n\nwith InputLayerSync(\"ws://localhost:8080/ws\", username=\"admin\", password=\"admin\") as il:\n    kg = il.knowledge_graph(\"demo\")\n    kg.define(Employee)\n    kg.insert(Employee(id=1, name=\"Alice\", department=\"eng\", salary=120000.0, active=True))\n    result = kg.query(Employee)\n```",
    "toc": [
      {
        "level": 2,
        "text": "Installation",
        "id": "installation"
      },
      {
        "level": 2,
        "text": "Connecting",
        "id": "connecting"
      },
      {
        "level": 2,
        "text": "Defining Schemas",
        "id": "defining-schemas"
      },
      {
        "level": 3,
        "text": "Supported Types",
        "id": "supported-types"
      },
      {
        "level": 2,
        "text": "Inserting Data",
        "id": "inserting-data"
      },
      {
        "level": 2,
        "text": "Querying",
        "id": "querying"
      },
      {
        "level": 3,
        "text": "Basic Queries",
        "id": "basic-queries"
      },
      {
        "level": 3,
        "text": "Column Selection",
        "id": "column-selection"
      },
      {
        "level": 3,
        "text": "Joins",
        "id": "joins"
      },
      {
        "level": 3,
        "text": "Aggregations",
        "id": "aggregations"
      },
      {
        "level": 3,
        "text": "Ordering and Pagination",
        "id": "ordering-and-pagination"
      },
      {
        "level": 2,
        "text": "Derived Relations (Rules)",
        "id": "derived-relations-rules"
      },
      {
        "level": 2,
        "text": "Vector Search",
        "id": "vector-search"
      },
      {
        "level": 2,
        "text": "Session Rules",
        "id": "session-rules"
      },
      {
        "level": 2,
        "text": "Notifications",
        "id": "notifications"
      },
      {
        "level": 2,
        "text": "Migrations",
        "id": "migrations"
      },
      {
        "level": 2,
        "text": "Error Handling",
        "id": "error-handling"
      },
      {
        "level": 2,
        "text": "Sync Client",
        "id": "sync-client"
      }
    ]
  },
  "guides/quickstart": {
    "title": "Quick Start",
    "content": "# Quick Start\n\nGet InputLayer running and execute your first query in 5 minutes.\n\n## 1. Install\n\n```bash\n# Clone the repository\ngit clone https://github.com/inputlayer/inputlayer.git\ncd inputlayer\n\n# Build (requires Rust 1.88+)\ncargo build --release\n```\n\n## 2. Start the Server and Client\n\n```bash\n# Start the server\n./target/release/inputlayer-server\n\n# In another terminal, start the client\n./target/release/inputlayer-client\n```\n\nYou should see:\n```\nConnecting to server at http://127.0.0.1:8080...\nConnected!\n\nServer status: healthy\nAuthenticated as: admin\nCurrent knowledge graph: default\n```\n\n## 3. Add Some Data\n\n```datalog\n+person[(\"alice\", 30), (\"bob\", 25), (\"charlie\", 35)]\n```\n\nOutput:\n```\nInserted 3 fact(s) into 'person'.\n```\n\n## 4. Query the Data\n\n```datalog\n?person(Name, Age)\n```\n\nOutput:\n\n| Name | Age |\n|---|---|\n| \"alice\" | 30 |\n| \"bob\" | 25 |\n| \"charlie\" | 35 |\n\n*3 rows*\n\n## 5. Add a Filter\n\n```datalog\n?person(Name, Age), Age > 28\n```\n\nOutput:\n\n| Name | Age |\n|---|---|\n| \"alice\" | 30 |\n| \"charlie\" | 35 |\n\n*2 rows*\n\n## 6. Create a Rule\n\n```datalog\n+senior(Name) <- person(Name, Age), Age >= 30\n```\n\nOutput:\n```\nRule 'senior' registered.\n```\n\n```datalog\n?senior(X)\n```\n\nOutput:\n\n| X |\n|---|\n| \"alice\" |\n| \"charlie\" |\n\n*2 rows*\n\n## 7. Use Aggregation\n\n```datalog\n+average_age(avg<Age>) <- person(_, Age)\n```\n\nOutput:\n```\nRule 'average_age' registered.\n```\n\n```datalog\n?average_age(X)\n```\n\nOutput:\n\n| X |\n|---|\n| 30.0 |\n\n*1 rows*\n\n## What's Next?\n\n- [First Program](first-program) - Learn the basics in depth\n- [Core Concepts](core-concepts) - Understand data modeling\n- [REPL Guide](repl) - Master the interactive environment\n\n## Common Commands\n\n| Command | Description |\n|---------|-------------|\n| `.help` | Show all commands |\n| `.rel` | List all relations |\n| `.rule` | List all rules |\n| `.quit` | Exit the REPL |",
    "toc": [
      {
        "level": 2,
        "text": "1. Install",
        "id": "1-install"
      },
      {
        "level": 2,
        "text": "2. Start the Server and Client",
        "id": "2-start-the-server-and-client"
      },
      {
        "level": 2,
        "text": "3. Add Some Data",
        "id": "3-add-some-data"
      },
      {
        "level": 2,
        "text": "4. Query the Data",
        "id": "4-query-the-data"
      },
      {
        "level": 2,
        "text": "5. Add a Filter",
        "id": "5-add-a-filter"
      },
      {
        "level": 2,
        "text": "6. Create a Rule",
        "id": "6-create-a-rule"
      },
      {
        "level": 2,
        "text": "7. Use Aggregation",
        "id": "7-use-aggregation"
      },
      {
        "level": 2,
        "text": "What's Next?",
        "id": "whats-next"
      },
      {
        "level": 2,
        "text": "Common Commands",
        "id": "common-commands"
      }
    ]
  },
  "guides/recursion": {
    "title": "Tutorial: Recursion",
    "content": "# Tutorial: Recursion\n\nRecursive rules let you express transitive closure, reachability, and graph algorithms that would require loops or CTEs in SQL.\n\n## Prerequisites\n\n- Completed [Your First Program](first-program)\n- Understanding of [Core Concepts](core-concepts)\n\n## What is Recursion?\n\nA recursive rule references itself in its body. This allows computing things that require an unknown number of steps, like:\n\n- All nodes reachable from a starting point\n- Transitive relationships (ancestor/descendant)\n- Fixed-point computations\n\n## Setup\n\n```datalog\n.kg create recursion_tutorial\n.kg use recursion_tutorial\n\n// A simple directed graph\n//     1 -> 2 -> 3 -> 4\n//     |   |\n//     5 -> 6 -> 7\n\n+edge[(1, 2), (2, 3), (3, 4),\n      (1, 5), (2, 6), (5, 6), (6, 7)]\n```\n\n## Basic Recursion: Transitive Closure\n\n### The Problem\n\nGiven edges, find all pairs (X, Y) where you can get from X to Y through any path.\n\n### Non-Recursive Attempt (Limited)\n\nYou could try explicit path lengths:\n\n```datalog\n// Direct edges (length 1)\n+path1(X, Y) <- edge(X, Y)\n\n// Length 2\n+path2(X, Y) <- edge(X, Z), edge(Z, Y)\n\n// Length 3\n+path3(X, Y) <- edge(X, Z), path2(Z, Y)\n```\n\nBut this only works for paths up to length 3. What about longer paths?\n\n### Recursive Solution\n\n```datalog\n// Base case: direct edges are paths\n+reachable(X, Y) <- edge(X, Y)\n\n// Recursive case: if X can reach Z, and Z has an edge to Y, then X can reach Y\n+reachable(X, Z) <- reachable(X, Y), edge(Y, Z)\n```\n\nQuery:\n```datalog\n?reachable(1, X)\n```\n\nResult:\n\n| 1 | X |\n|---|---|\n| 1 | 2 |\n| 1 | 3 |\n| 1 | 4 |\n| 1 | 5 |\n| 1 | 6 |\n| 1 | 7 |\n\n*6 rows*\n\nNode 1 can reach all other nodes!\n\n### How Recursion Works\n\nInputLayer evaluates recursive rules using *fixpoint iteration*:\n\n1. **Iteration 0**: Compute base case (direct edges)\n   - reachable = {(1,2), (2,3), (3,4), (1,5), (2,6), (5,6), (6,7)}\n\n2. **Iteration 1**: Apply recursive rule\n   - From (1,2) and edge(2,3): add (1,3)\n   - From (1,2) and edge(2,6): add (1,6)\n   - From (2,3) and edge(3,4): add (2,4)\n   - ... and so on\n\n3. **Iteration 2**: Continue until no new facts\n   - From (1,3) and edge(3,4): add (1,4)\n   - ...\n\n4. **Fixpoint**: When no new facts are derived, we're done\n\n## Left vs Right Recursion\n\n### Left Recursion (Preferred)\n\n```datalog\n+reach_left(X, Y) <- edge(X, Y)\n+reach_left(X, Z) <- reach_left(X, Y), edge(Y, Z)\n```\n\nThe recursive call is on the **left** side of the join.\n\n### Right Recursion\n\n```datalog\n+reach_right(X, Y) <- edge(X, Y)\n+reach_right(X, Z) <- edge(X, Y), reach_right(Y, Z)\n```\n\nThe recursive call is on the **right** side.\n\n**Both work** in InputLayer. Choose whichever reads more naturally for your use case.\n\n## Mutual Recursion\n\nRules can reference each other:\n\n```datalog\n// Odd-length paths from node 1\n+odd_path(X) <- edge(1, X)\n+odd_path(X) <- even_path(Y), edge(Y, X)\n\n// Even-length paths from node 1\n+even_path(X) <- odd_path(Y), edge(Y, X)\n```\n\nThese rules depend on each other and are computed together.\n\n## Practical Examples\n\n### Example 1: Ancestor/Descendant\n\n```datalog\n// Family tree\n+parent[(1, 2), (1, 3), (2, 4), (2, 5), (3, 6)]\n// 1 is parent of 2, 3; 2 is parent of 4, 5; 3 is parent of 6\n\n// Ancestor relationship\n+ancestor(X, Y) <- parent(X, Y)\n+ancestor(X, Z) <- parent(X, Y), ancestor(Y, Z)\n```\n\nQuery: Who are person 1's descendants?\n```datalog\n?ancestor(1, Desc)\n```\n\n### Example 2: Same Generation\n\nFind all pairs at the same generation level:\n\n```datalog\n// Root nodes (no parents)\n+root(X) <- node(X), !parent(_, X)\n\n// Generation level\n+generation(X, 0) <- root(X)\n+generation(X, N) <- parent(P, X), generation(P, M), N = M + 1\n\n// Same generation\n+same_gen(X, Y) <- generation(X, N), generation(Y, N), X != Y\n```\n\n### Example 3: Connected Components\n\nFind which nodes are in the same connected component:\n\n```datalog\n// Symmetric edges for undirected graph\n+sym_edge(X, Y) <- edge(X, Y)\n+sym_edge(X, Y) <- edge(Y, X)\n\n// Same component (connected)\n+same_component(X, X) <- node(X)  // Reflexive\n+same_component(X, Y) <- sym_edge(X, Y)\n+same_component(X, Z) <- same_component(X, Y), sym_edge(Y, Z)\n```\n\n### Example 4: Bill of Materials\n\nClassic manufacturing example - compute all parts needed:\n\n```datalog\n// Part containment: component(assembly, part, quantity)\n+component[(1, 2, 1), (1, 3, 2), (2, 4, 3), (3, 4, 1), (3, 5, 2)]\n\n// All parts required for an assembly (including nested)\n+requires(Assembly, Part) <- component(Assembly, Part, _)\n+requires(Assembly, Part) <-\n  component(Assembly, SubAsm, _),\n  requires(SubAsm, Part)\n```\n\n## Recursion with Aggregation\n\n### Recursive Sum\n\nTotal quantity of each part needed (with multipliers):\n\n```datalog\n// Direct quantity needed\n+qty(Asm, Part, Qty) <- component(Asm, Part, Qty)\n\n// Transitive quantity (multiplied through levels)\n+qty(Asm, Part, TotalQty) <-\n  component(Asm, Sub, SubQty),\n  qty(Sub, Part, PartQty),\n  TotalQty = SubQty * PartQty\n\n// Sum all quantities per part\n+total_qty(Asm, Part, sum<Qty>) <- qty(Asm, Part, Qty)\n```\n\n## Common Patterns\n\n### Pattern 1: Transitive Closure\n\n```datalog\n+closure(X, Y) <- base_relation(X, Y)\n+closure(X, Z) <- closure(X, Y), base_relation(Y, Z)\n```\n\n### Pattern 2: Reflexive-Transitive Closure\n\n```datalog\n+rt_closure(X, X) <- domain(X)  // Reflexive: X reaches itself\n+rt_closure(X, Y) <- base_relation(X, Y)\n+rt_closure(X, Z) <- rt_closure(X, Y), base_relation(Y, Z)\n```\n\n### Pattern 3: Inductive Definition\n\n```datalog\n// Base case\n+inductive(0, \"base\")\n\n// Inductive step\n+inductive(N, \"derived\") <-\n  inductive(M, _),\n  N = M + 1,\n  N < 10\n```\n\n### Pattern 4: Graph Algorithms\n\n```datalog\n// Node with maximum reachability\n+reach_count(X, count<Y>) <- reachable(X, Y)\n+max_reach(max<Count>) <- reach_count(_, Count)\n+most_connected(X) <- reach_count(X, C), max_reach(C)\n```\n\n## Debugging Recursion\n\n### Check Intermediate Results\n\n```datalog\n// Query the recursive relation directly\n?reachable(X, Y)\n\n// See how many facts are derived\ncount_reachable(count<X>) <- reachable(X, _)\n?count_reachable(N)\n```\n\n### Limit Depth for Testing\n\nFor debugging, you can create bounded versions:\n\n```datalog\n+reach1(X, Y) <- edge(X, Y)\n+reach2(X, Y) <- reach1(X, Y)\n+reach2(X, Z) <- reach1(X, Y), edge(Y, Z)\n// etc.\n```\n\n## Performance Considerations\n\n1. **Minimize joins** in recursive rules when possible\n2. **Use constraints early** to prune the search space\n3. **Magic Sets** optimization automatically prunes recursive queries when constants are provided (e.g., `?reachable(1, X)`)\n\n## Exercises\n\n1. Write a rule to find all cycles in the graph (nodes that can reach themselves)\n2. Compute the length of the longest path from node 1\n3. Find nodes that have exactly 2 ancestors\n4. Write rules to detect if a graph is a DAG (directed acyclic graph)\n\n## Next Steps\n\n- **[Vectors](vectors)** - Vector search and similarity queries\n- **[Temporal](temporal)** - Time-based reasoning and decay functions",
    "toc": [
      {
        "level": 2,
        "text": "Prerequisites",
        "id": "prerequisites"
      },
      {
        "level": 2,
        "text": "What is Recursion?",
        "id": "what-is-recursion"
      },
      {
        "level": 2,
        "text": "Setup",
        "id": "setup"
      },
      {
        "level": 2,
        "text": "Basic Recursion: Transitive Closure",
        "id": "basic-recursion-transitive-closure"
      },
      {
        "level": 3,
        "text": "The Problem",
        "id": "the-problem"
      },
      {
        "level": 3,
        "text": "Non-Recursive Attempt (Limited)",
        "id": "non-recursive-attempt-limited"
      },
      {
        "level": 3,
        "text": "Recursive Solution",
        "id": "recursive-solution"
      },
      {
        "level": 3,
        "text": "How Recursion Works",
        "id": "how-recursion-works"
      },
      {
        "level": 2,
        "text": "Left vs Right Recursion",
        "id": "left-vs-right-recursion"
      },
      {
        "level": 3,
        "text": "Left Recursion (Preferred)",
        "id": "left-recursion-preferred"
      },
      {
        "level": 3,
        "text": "Right Recursion",
        "id": "right-recursion"
      },
      {
        "level": 2,
        "text": "Mutual Recursion",
        "id": "mutual-recursion"
      },
      {
        "level": 2,
        "text": "Practical Examples",
        "id": "practical-examples"
      },
      {
        "level": 3,
        "text": "Example 1: Ancestor/Descendant",
        "id": "example-1-ancestordescendant"
      },
      {
        "level": 3,
        "text": "Example 2: Same Generation",
        "id": "example-2-same-generation"
      },
      {
        "level": 3,
        "text": "Example 3: Connected Components",
        "id": "example-3-connected-components"
      },
      {
        "level": 3,
        "text": "Example 4: Bill of Materials",
        "id": "example-4-bill-of-materials"
      },
      {
        "level": 2,
        "text": "Recursion with Aggregation",
        "id": "recursion-with-aggregation"
      },
      {
        "level": 3,
        "text": "Recursive Sum",
        "id": "recursive-sum"
      },
      {
        "level": 2,
        "text": "Common Patterns",
        "id": "common-patterns"
      },
      {
        "level": 3,
        "text": "Pattern 1: Transitive Closure",
        "id": "pattern-1-transitive-closure"
      },
      {
        "level": 3,
        "text": "Pattern 2: Reflexive-Transitive Closure",
        "id": "pattern-2-reflexive-transitive-closure"
      },
      {
        "level": 3,
        "text": "Pattern 3: Inductive Definition",
        "id": "pattern-3-inductive-definition"
      },
      {
        "level": 3,
        "text": "Pattern 4: Graph Algorithms",
        "id": "pattern-4-graph-algorithms"
      },
      {
        "level": 2,
        "text": "Debugging Recursion",
        "id": "debugging-recursion"
      },
      {
        "level": 3,
        "text": "Check Intermediate Results",
        "id": "check-intermediate-results"
      },
      {
        "level": 3,
        "text": "Limit Depth for Testing",
        "id": "limit-depth-for-testing"
      },
      {
        "level": 2,
        "text": "Performance Considerations",
        "id": "performance-considerations"
      },
      {
        "level": 2,
        "text": "Exercises",
        "id": "exercises"
      },
      {
        "level": 2,
        "text": "Next Steps",
        "id": "next-steps"
      }
    ]
  },
  "guides/repl": {
    "title": "REPL Guide",
    "content": "# REPL Guide\n\nThe REPL is where you define facts, rules, and queries interactively.\n\n## Starting the Client\n\n```bash\ninputlayer-client\n```\n\nYou'll see:\n```\nConnecting to server at http://127.0.0.1:8080...\nConnected!\n\nServer status: healthy\nAuthenticated as: admin\nCurrent knowledge graph: default\n```\n\n## Command Categories\n\n### Knowledge Graph Commands (`.kg`)\n\nManage your knowledge graphs:\n\n| Command | Description |\n|---------|-------------|\n| `.kg` | Show current knowledge graph |\n| `.kg list` | List all knowledge graphs |\n| `.kg create <name>` | Create a new knowledge graph |\n| `.kg use <name>` | Switch to a knowledge graph |\n| `.kg drop <name>` | Delete a knowledge graph (cannot drop current) |\n\n**Examples:**\n```datalog\n.kg create myproject\n.kg use myproject\n.kg list\n.kg\n```\n\n### Relation Commands (`.rel`)\n\nInspect base facts:\n\n| Command | Description |\n|---------|-------------|\n| `.rel` | List all relations with data |\n| `.rel <name>` | Show schema and sample data for a relation |\n| `.rel drop <name>` | Drop a relation and its data |\n\n**Examples:**\n```datalog\n.rel\n.rel edge\n.rel employee\n```\n\n### Rule Commands (`.rule`)\n\nManage persistent rules:\n\n| Command | Description |\n|---------|-------------|\n| `.rule` | List all defined rules |\n| `.rule <name>` | Query a rule (show computed results) |\n| `.rule def <name>` | Show the rule definition (clauses) |\n| `.rule drop <name>` | Delete a rule |\n| `.rule remove <name> <n>` | Remove clause #n (1-based) |\n| `.rule clear <name>` | Clear clauses for re-registration |\n| `.rule edit <name> <n> <clause>` | Replace clause #n |\n\n**Examples:**\n```datalog\n.rule                           // List all rules\n.rule path                      // Query the 'path' rule\n.rule def path                  // Show path's definition\n.rule drop path                 // Delete the path rule\n.rule clear path                // Clear for re-definition\n.rule edit path 1 +path(X,Y) <- edge(X,Y)  // Edit clause 1\n```\n\n### Session Commands (`.session`)\n\nManage transient session rules:\n\n| Command | Description |\n|---------|-------------|\n| `.session` | List session rules |\n| `.session clear` | Clear all session rules |\n| `.session drop <n>` | Remove session rule #n |\n\n**Examples:**\n```datalog\ntemp(X) <- edge(1, X)    // Add session rule\n.session                   // List session rules\n.session drop 1            // Remove first rule\n.session clear             // Clear all\n```\n\n### File Commands (`.load`)\n\nLoad and execute Datalog files:\n\n| Command | Description |\n|---------|-------------|\n| `.load <file>` | Execute a .idl file |\n\n**Examples:**\n```datalog\n.load schema.idl\n.load rules.idl\n```\n\n**Note:** The `.load` command reads the file and executes each statement in order. It continues through non-fatal errors so cleanup commands always run.\n\n### Index Commands (`.index` / `.idx`)\n\nManage HNSW vector indexes:\n\n| Command | Description |\n|---------|-------------|\n| `.index` or `.index list` | List all indexes |\n| `.index create <name> on <rel>(<col>) [options]` | Create an HNSW index |\n| `.index drop <name>` | Drop an index |\n| `.index stats <name>` | Show index statistics |\n| `.index rebuild <name>` | Force rebuild an index |\n\n**Examples:**\n```datalog\n.index create vec_idx on docs(embedding) type hnsw metric cosine m 16 ef_construction 200\n.index stats vec_idx\n.index drop vec_idx\n```\n\n### User Commands (`.user`)\n\nManage users (requires admin):\n\n| Command | Description |\n|---------|-------------|\n| `.user list` | List all users |\n| `.user create <name> <password> <role>` | Create a user (`admin`, `editor`, `viewer`) |\n| `.user drop <name>` | Delete a user |\n| `.user password <name> <password>` | Change a user's password |\n| `.user role <name> <role>` | Change a user's role |\n\n### API Key Commands (`.apikey`)\n\nManage API keys for machine-to-machine auth:\n\n| Command | Description |\n|---------|-------------|\n| `.apikey create <label>` | Create a new API key |\n| `.apikey list` | List all API keys |\n| `.apikey revoke <label>` | Revoke an API key |\n\n### Access Control Commands (`.kg acl`)\n\nManage per-knowledge-graph access:\n\n| Command | Description |\n|---------|-------------|\n| `.kg acl list [<kg>]` | List ACLs for a knowledge graph |\n| `.kg acl grant <kg> <user> <role>` | Grant access (`owner`, `editor`, `viewer`) |\n| `.kg acl revoke <kg> <user>` | Revoke access |\n\n### System Commands\n\n| Command | Description |\n|---------|-------------|\n| `.status` | Show system status |\n| `.compact` | Compact WAL and consolidate storage |\n| `.explain <query>` | Show query plan without executing |\n| `.clear prefix <p>` | Clear all facts from relations matching prefix |\n| `.help` | Show help message |\n| `.quit` or `.exit` | Exit the REPL |\n\n## Statement Types\n\n### Insert Facts (`+`)\n\n```datalog\n// Single fact\n+edge(1, 2)\n\n// Bulk insert\n+edge[(1, 2), (2, 3), (3, 4)]\n\n// With different types\n+person(\"alice\", 30, \"engineering\")\n```\n\n### Delete Facts (`-`)\n\n```datalog\n// Single fact\n-edge(1, 2)\n\n// Conditional delete (must reference relation in body)\n-edge(X, Y) <- edge(X, Y), X > 10\n\n// Delete all from a relation\n-edge(X, Y) <- edge(X, Y)\n```\n\n### Updates (Delete then Insert)\n\n```datalog\n// First delete old value\n-counter(1, 0)\n// Then insert new value\n+counter(1, 5)\n```\n\n### Persistent Rules (`+head <- body`)\n\n```datalog\n// Simple rule\n+adult(Name, Age) <- person(Name, Age), Age >= 18\n\n// Recursive rule\n+path(X, Y) <- edge(X, Y)\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n\n// With aggregation\n+dept_count(Dept, count<Id>) <- employee(Id, Dept)\n```\n\n### Session Rules (`head <- body`)\n\n```datalog\n// Transient rule (no + prefix)\ntemp_result(X, Y) <- edge(X, Y), X < Y\n```\n\n### Queries (`?`)\n\n```datalog\n// Simple query\n?edge(1, X)\n\n// With constraints\n?person(Name, Age), Age > 25\n\n// Query derived data\n?path(1, X)\n```\n\n### Schema Declarations\n\n```datalog\n// Typed schema\n+employee(id: int, name: string, dept: string)\n+user(id: int, name: string, email: string)\n```\n\n## Tips and Tricks\n\n### Multi-line Statements\n\nIn script files (`.idl`), statements can span multiple lines. In the interactive REPL, each line is sent as a separate statement:\n\n```datalog\n// In a .idl script file:\n+complex_rule(X, Y, Z) <-\n  first_condition(X, A),\n  second_condition(A, Y),\n  third_condition(Y, Z),\n  X < Y,\n  Y < Z\n\n// In the interactive REPL, write it as a single line:\n+complex_rule(X, Y, Z) <- first_condition(X, A), second_condition(A, Y), third_condition(Y, Z), X < Y, Y < Z\n```\n\n### Comments\n\n```datalog\n// Single line comment\n+edge(1, 2)  // Inline comment\n\n/*\n   Multi-line\n   block comment\n*/\n```\n\n### Viewing Results\n\nQuery results are displayed as formatted tables:\n\n```\n> ?edge(X, Y)\n```\n\n| X | Y |\n|---|---|\n| 1 | 2 |\n| 2 | 3 |\n| 3 | 4 |\n| 4 | 5 |\n| 5 | 6 |\n\n*5 rows*\n\n### Using Wildcards\n\nUse `_` to ignore columns:\n\n```datalog\n// Get all source nodes (ignore target)\n?edge(X, _)\n\n// Count unique sources\ntemp(count<X>) <- edge(X, _)\n```\n\n## Common Workflows\n\n### 1. Exploratory Analysis\n\n```datalog\n.kg create exploration\n.kg use exploration\n.load data.idl\n.rel                          // See what data exists\n?some_relation(X, Y)         // Explore\ntemp(X) <- complex_query...  // Session rule for analysis\n.session clear                // Clean up when done\n```\n\n### 2. Building a Schema\n\n```datalog\n.kg create production\n.kg use production\n\n// Define schemas first\n+user(id: int, name: string, email: string)\n+order(id: int, user_id: int, amount: float)\n\n// Load data\n.load users.idl\n.load orders.idl\n\n// Verify\n.rel user\n.rel order\n```\n\n### 3. Defining Business Rules\n\n```datalog\n// Define persistent rules\n+high_value_customer(UserId) <-\n  order(_, UserId, Amount),\n  Amount > 1000\n\n// Aggregate total spend per customer\n+customer_spend(UserId, sum<Amount>) <-\n  order(_, UserId, Amount)\n\n// VIPs have high total spend\n+vip(UserId, Total) <-\n  high_value_customer(UserId),\n  customer_spend(UserId, Total),\n  Total > 5000\n\n// Query\n?vip(User, Spend)\n```\n\n### 4. Iterating on Rules\n\n```datalog\n// First attempt\n+path(X, Y) <- edge(X, Y)\n\n// Check results\n.rule path\n\n// Not right? Clear and redefine\n.rule clear path\n+path(X, Y) <- edge(X, Y)\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n\n// Verify\n.rule def path\n.rule path\n```\n\n## Keyboard Shortcuts\n\n| Shortcut | Action |\n|----------|--------|\n| `Ctrl+C` | Cancel current input |\n| `Ctrl+D` | Exit REPL (same as `.quit`) |\n| `Up` / `Down` | Navigate command history |\n| `Ctrl+R` | Search command history |\n\n## Error Handling\n\nWhen something goes wrong, InputLayer provides error messages:\n\n```\n> +edge(1, \"two\")\nType mismatch in 'edge': column 1 expected int, got string\n```\n\n```\n> ?undefined_relation(X)\nRelation 'undefined_relation' not found.\n```\n\n## Next Steps\n\n- **[Recursion](recursion)** - Recursive query patterns\n- **[Troubleshooting](troubleshooting)** - Common issues and solutions",
    "toc": [
      {
        "level": 2,
        "text": "Starting the Client",
        "id": "starting-the-client"
      },
      {
        "level": 2,
        "text": "Command Categories",
        "id": "command-categories"
      },
      {
        "level": 3,
        "text": "Knowledge Graph Commands (.kg)",
        "id": "knowledge-graph-commands-kg"
      },
      {
        "level": 3,
        "text": "Relation Commands (.rel)",
        "id": "relation-commands-rel"
      },
      {
        "level": 3,
        "text": "Rule Commands (.rule)",
        "id": "rule-commands-rule"
      },
      {
        "level": 3,
        "text": "Session Commands (.session)",
        "id": "session-commands-session"
      },
      {
        "level": 3,
        "text": "File Commands (.load)",
        "id": "file-commands-load"
      },
      {
        "level": 3,
        "text": "Index Commands (.index / .idx)",
        "id": "index-commands-index-idx"
      },
      {
        "level": 3,
        "text": "User Commands (.user)",
        "id": "user-commands-user"
      },
      {
        "level": 3,
        "text": "API Key Commands (.apikey)",
        "id": "api-key-commands-apikey"
      },
      {
        "level": 3,
        "text": "Access Control Commands (.kg acl)",
        "id": "access-control-commands-kg-acl"
      },
      {
        "level": 3,
        "text": "System Commands",
        "id": "system-commands"
      },
      {
        "level": 2,
        "text": "Statement Types",
        "id": "statement-types"
      },
      {
        "level": 3,
        "text": "Insert Facts (+)",
        "id": "insert-facts-"
      },
      {
        "level": 3,
        "text": "Delete Facts (-)",
        "id": "delete-facts-"
      },
      {
        "level": 3,
        "text": "Updates (Delete then Insert)",
        "id": "updates-delete-then-insert"
      },
      {
        "level": 3,
        "text": "Persistent Rules (+head <- body)",
        "id": "persistent-rules-head-body"
      },
      {
        "level": 3,
        "text": "Session Rules (head <- body)",
        "id": "session-rules-head-body"
      },
      {
        "level": 3,
        "text": "Queries (?)",
        "id": "queries-"
      },
      {
        "level": 3,
        "text": "Schema Declarations",
        "id": "schema-declarations"
      },
      {
        "level": 2,
        "text": "Tips and Tricks",
        "id": "tips-and-tricks"
      },
      {
        "level": 3,
        "text": "Multi-line Statements",
        "id": "multi-line-statements"
      },
      {
        "level": 3,
        "text": "Comments",
        "id": "comments"
      },
      {
        "level": 3,
        "text": "Viewing Results",
        "id": "viewing-results"
      },
      {
        "level": 3,
        "text": "Using Wildcards",
        "id": "using-wildcards"
      },
      {
        "level": 2,
        "text": "Common Workflows",
        "id": "common-workflows"
      },
      {
        "level": 3,
        "text": "1. Exploratory Analysis",
        "id": "1-exploratory-analysis"
      },
      {
        "level": 3,
        "text": "2. Building a Schema",
        "id": "2-building-a-schema"
      },
      {
        "level": 3,
        "text": "3. Defining Business Rules",
        "id": "3-defining-business-rules"
      },
      {
        "level": 3,
        "text": "4. Iterating on Rules",
        "id": "4-iterating-on-rules"
      },
      {
        "level": 2,
        "text": "Keyboard Shortcuts",
        "id": "keyboard-shortcuts"
      },
      {
        "level": 2,
        "text": "Error Handling",
        "id": "error-handling"
      },
      {
        "level": 2,
        "text": "Next Steps",
        "id": "next-steps"
      }
    ]
  },
  "guides/rest-api": {
    "title": "HTTP API Guide",
    "content": "# HTTP API Guide\n\nInputLayer is a **WebSocket-first** system. All data operations (queries, inserts, rule definitions, meta commands) go through the WebSocket API. The HTTP REST endpoints provide health checks, metrics, and documentation.\n\nFor query execution, see the [WebSocket API](websocket-api) guide.\n\n## Starting the Server\n\n```bash\n# Start the HTTP server\n./target/release/inputlayer-server\n\n# With custom port\n./target/release/inputlayer-server --port 9090\n\n# With config file\n./target/release/inputlayer-server --config config.toml\n```\n\nOr enable in configuration:\n\n```toml\n[http]\nenabled = true\nhost = \"127.0.0.1\"\nport = 8080\n```\n\n## Base URL\n\nAll endpoints are available at both root and versioned paths:\n\n```\nhttp://localhost:8080/health\nhttp://localhost:8080/v1/health\n```\n\n---\n\n## Health & Monitoring Endpoints\n\n### Health Check\n\n```http\nGET /health\n```\n\nReturns server health status. Returns HTTP 200 when healthy, 503 when storage is degraded.\n\n**Response (200):**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"status\": \"healthy\",\n    \"version\": \"0.1.0\",\n    \"uptime_secs\": 3600\n  }\n}\n```\n\n**Response (503) - Storage lock contended:**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"status\": \"degraded\",\n    \"version\": \"0.1.0\",\n    \"uptime_secs\": 3600\n  }\n}\n```\n\n### Liveness Probe\n\n```http\nGET /live\n```\n\nAlways returns HTTP 200 if the process is alive. Does not check storage. Use for Kubernetes liveness probes.\n\n### Readiness Probe\n\n```http\nGET /ready\n```\n\nReturns HTTP 200 if the server can handle requests (storage accessible). Returns 503 if storage lock is contended. Use for Kubernetes readiness probes.\n\n### Server Statistics\n\n```http\nGET /metrics\n```\n\n**Response:**\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"knowledge_graphs\": 3,\n    \"relations\": 10,\n    \"views\": 5,\n    \"memory_usage_bytes\": 262144,\n    \"query_count\": 15000,\n    \"uptime_secs\": 3600,\n    \"sessions\": {\n      \"total\": 5,\n      \"clean\": 3,\n      \"dirty\": 2,\n      \"total_ephemeral_facts\": 100,\n      \"total_ephemeral_rules\": 10\n    }\n  }\n}\n```\n\n### Prometheus Metrics\n\n```http\nGET /metrics/prometheus\n```\n\nReturns metrics in Prometheus text exposition format:\n\n```\n# HELP inputlayer_uptime_seconds Server uptime in seconds.\n# TYPE inputlayer_uptime_seconds gauge\ninputlayer_uptime_seconds 3600\n# HELP inputlayer_queries_total Total queries executed.\n# TYPE inputlayer_queries_total counter\ninputlayer_queries_total 15000\n# HELP inputlayer_knowledge_graphs Number of knowledge graphs.\n# TYPE inputlayer_knowledge_graphs gauge\ninputlayer_knowledge_graphs 3\n# HELP inputlayer_relations_total Total base relations.\n# TYPE inputlayer_relations_total gauge\ninputlayer_relations_total 10\n# HELP inputlayer_views_total Total derived views (rules).\n# TYPE inputlayer_views_total gauge\ninputlayer_views_total 5\n# HELP inputlayer_sessions_total Active sessions.\n# TYPE inputlayer_sessions_total gauge\ninputlayer_sessions_total 5\n# HELP inputlayer_sessions_clean Clean (idle) sessions.\n# TYPE inputlayer_sessions_clean gauge\ninputlayer_sessions_clean 3\n# HELP inputlayer_sessions_dirty Dirty (active) sessions.\n# TYPE inputlayer_sessions_dirty gauge\ninputlayer_sessions_dirty 2\n# HELP inputlayer_tuples_total Total stored tuples.\n# TYPE inputlayer_tuples_total gauge\ninputlayer_tuples_total 50000\n# HELP inputlayer_memory_usage_bytes Estimated memory usage.\n# TYPE inputlayer_memory_usage_bytes gauge\ninputlayer_memory_usage_bytes 104857600\n# HELP inputlayer_ephemeral_facts Ephemeral facts across sessions.\n# TYPE inputlayer_ephemeral_facts gauge\ninputlayer_ephemeral_facts 10\n# HELP inputlayer_ephemeral_rules Ephemeral rules across sessions.\n# TYPE inputlayer_ephemeral_rules gauge\ninputlayer_ephemeral_rules 2\n```\n\n---\n\n## WebSocket Endpoints\n\n### Global WebSocket\n\n```http\nGET /ws\n```\n\nUpgrades to a WebSocket connection. Requires authentication as the first message. See [WebSocket API](websocket-api) for the full protocol.\n\n### Session-Scoped WebSocket\n\n```http\nGET /sessions/:id/ws\n```\n\nConnects to an existing session by ID.\n\n---\n\n## Documentation Endpoints\n\n### AsyncAPI Specification\n\n```http\nGET /api/asyncapi.yaml\n```\n\nReturns the AsyncAPI specification describing the WebSocket message protocol.\n\n### OpenAPI Specification\n\n```http\nGET /api/openapi.yaml\n```\n\nReturns the OpenAPI specification describing the REST endpoints.\n\n### WebSocket Documentation\n\n```http\nGET /api/ws-docs\n```\n\nReturns an interactive HTML page for exploring the WebSocket API.\n\n---\n\n## Authentication\n\nREST endpoints (except health/live/ready) require an API key in the Authorization header:\n\n```http\nAuthorization: Bearer <api-key>\n```\n\nAPI keys are managed via meta commands in the WebSocket session:\n\n```datalog\n.apikey create my-service-key\n.apikey list\n.apikey revoke my-service-key\n```\n\nSee [Authentication](authentication) for details.\n\n---\n\n## Error Responses\n\nAll errors use a consistent format:\n\n```json\n{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"NOT_FOUND\",\n    \"message\": \"Resource not found\"\n  }\n}\n```\n\n**Error Codes:**\n\n| Code | HTTP Status | Description |\n|------|-------------|-------------|\n| `NOT_FOUND` | 404 | Resource not found |\n| `BAD_REQUEST` | 400 | Invalid request |\n| `INTERNAL_ERROR` | 500 | Server error |\n| `SERVICE_UNAVAILABLE` | 503 | Server not ready |\n\n---\n\n## Client Examples\n\n### cURL - Health Check\n\n```bash\n# Check server health\ncurl http://localhost:8080/health\n\n# Get server stats\ncurl http://localhost:8080/metrics\n\n# Get Prometheus metrics\ncurl http://localhost:8080/metrics/prometheus\n```\n\n### cURL - With Authentication\n\n```bash\n# Authenticated request\ncurl -H \"Authorization: Bearer your-api-key\" http://localhost:8080/metrics\n```\n\n### Kubernetes Probes\n\n```yaml\nlivenessProbe:\n  httpGet:\n    path: /live\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 5\n```\n\n---\n\n## Next Steps\n\n- [WebSocket API](websocket-api) - Execute queries and manage data\n- [Configuration](configuration) - Server configuration options\n- [Authentication](authentication) - User and API key management",
    "toc": [
      {
        "level": 2,
        "text": "Starting the Server",
        "id": "starting-the-server"
      },
      {
        "level": 2,
        "text": "Base URL",
        "id": "base-url"
      },
      {
        "level": 2,
        "text": "Health & Monitoring Endpoints",
        "id": "health-monitoring-endpoints"
      },
      {
        "level": 3,
        "text": "Health Check",
        "id": "health-check"
      },
      {
        "level": 3,
        "text": "Liveness Probe",
        "id": "liveness-probe"
      },
      {
        "level": 3,
        "text": "Readiness Probe",
        "id": "readiness-probe"
      },
      {
        "level": 3,
        "text": "Server Statistics",
        "id": "server-statistics"
      },
      {
        "level": 3,
        "text": "Prometheus Metrics",
        "id": "prometheus-metrics"
      },
      {
        "level": 2,
        "text": "WebSocket Endpoints",
        "id": "websocket-endpoints"
      },
      {
        "level": 3,
        "text": "Global WebSocket",
        "id": "global-websocket"
      },
      {
        "level": 3,
        "text": "Session-Scoped WebSocket",
        "id": "session-scoped-websocket"
      },
      {
        "level": 2,
        "text": "Documentation Endpoints",
        "id": "documentation-endpoints"
      },
      {
        "level": 3,
        "text": "AsyncAPI Specification",
        "id": "asyncapi-specification"
      },
      {
        "level": 3,
        "text": "OpenAPI Specification",
        "id": "openapi-specification"
      },
      {
        "level": 3,
        "text": "WebSocket Documentation",
        "id": "websocket-documentation"
      },
      {
        "level": 2,
        "text": "Authentication",
        "id": "authentication"
      },
      {
        "level": 2,
        "text": "Error Responses",
        "id": "error-responses"
      },
      {
        "level": 2,
        "text": "Client Examples",
        "id": "client-examples"
      },
      {
        "level": 3,
        "text": "cURL - Health Check",
        "id": "curl-health-check"
      },
      {
        "level": 3,
        "text": "cURL - With Authentication",
        "id": "curl-with-authentication"
      },
      {
        "level": 3,
        "text": "Kubernetes Probes",
        "id": "kubernetes-probes"
      },
      {
        "level": 2,
        "text": "Next Steps",
        "id": "next-steps"
      }
    ]
  },
  "guides/temporal": {
    "title": "Temporal Reasoning Tutorial",
    "content": "# Temporal Reasoning Tutorial\n\n14 built-in temporal functions: event processing, time-decay scoring, and interval analysis.\n\n## Timestamp Basics\n\nTimestamps in InputLayer are Unix milliseconds (64-bit integers):\n\n```datalog\n// Store events with timestamps\n+events(1, \"login\", 1704067200000)      // 2024-01-01 00:00:00 UTC\n+events(2, \"purchase\", 1704153600000)   // 2024-01-02 00:00:00 UTC\n+events(3, \"logout\", 1704240000000)     // 2024-01-03 00:00:00 UTC\n```\n\n### Common Time Constants\n\n| Duration | Milliseconds |\n|----------|-------------|\n| 1 second | 1000 |\n| 1 minute | 60000 |\n| 1 hour | 3600000 |\n| 1 day | 86400000 |\n| 1 week | 604800000 |\n\n---\n\n## Core Time Functions\n\n### `time_now()`\n\nGet the current timestamp:\n\n```datalog\n? Now = time_now()\n```\n\n**Returns:** Current Unix timestamp in milliseconds.\n\n### `time_diff(t1, t2)`\n\nCalculate the difference between two timestamps:\n\n```datalog\n? events(Id1, _, T1), events(Id2, _, T2),\n   Id1 < Id2,\n   Diff = time_diff(T1, T2)\n```\n\n**Returns:** `t1 - t2` in milliseconds (can be negative).\n\n### `time_add(ts, duration_ms)`\n\nAdd duration to a timestamp:\n\n```datalog\n// Calculate when session expires (24 hours after login)\n+session_expires(UserId, time_add(LoginTime, 86400000)) <-\n    login_events(UserId, LoginTime)\n\n? session_expires(UserId, ExpiresAt)\n```\n\n### `time_sub(ts, duration_ms)`\n\nSubtract duration from a timestamp:\n\n```datalog\n// Find the timestamp 1 hour ago\n? Now = time_now(),\n   OneHourAgo = time_sub(Now, 3600000)\n```\n\n---\n\n## Time Comparisons\n\n### `time_before(t1, t2)`\n\nCheck if t1 is before t2:\n\n```datalog\n// Find events that happened before a cutoff\n+old_events(Id, Type) <-\n    events(Id, Type, Ts),\n    cutoff(Cutoff),\n    time_before(Ts, Cutoff)\n```\n\n**Returns:** Boolean `true` if t1 < t2.\n\n### `time_after(t1, t2)`\n\nCheck if t1 is after t2:\n\n```datalog\n// Find events after a specific date\n+recent_events(Id, Type) <-\n    events(Id, Type, Ts),\n    start_date(Start),\n    time_after(Ts, Start)\n```\n\n**Returns:** Boolean `true` if t1 > t2.\n\n### `time_between(ts, start, end)`\n\nCheck if a timestamp falls within a range:\n\n```datalog\n// Find events within a time window\n+in_window(Id, Type) <-\n    events(Id, Type, Ts),\n    window_start(Start),\n    window_end(End),\n    time_between(Ts, Start, End)\n```\n\n**Returns:** Boolean `true` if start <= ts <= end.\n\n### `within_last(ts, now, duration_ms)`\n\nCheck if a timestamp is within a duration of a reference time:\n\n```datalog\n// Find events from the last 24 hours\n+recent(Id, Type) <-\n    events(Id, Type, Ts),\n    Now = time_now(),\n    within_last(Ts, Now, 86400000)\n\n? recent(Id, Type)\n```\n\n**Returns:** Boolean `true` if `now - duration_ms <= ts <= now`.\n\n---\n\n## Time Decay Functions\n\nTime decay functions weight recent events more heavily than older ones.\n\n### `time_decay(ts, now, half_life_ms)`\n\nExponential decay based on half-life:\n\n```datalog\n// Score events with exponential decay (half-life = 7 days)\n+event_score(Id, Type, time_decay(Ts, Now, 604800000)) <-\n    events(Id, Type, Ts),\n    Now = time_now()\n\n? event_score(Id, Type, Score)\n```\n\n**Returns:** Float in range [0, 1]. Score = 0.5 when age equals half-life.\n\n**Formula:** `score = 2^(-age / half_life)`\n\n| Age | Score |\n|-----|-------|\n| 0 | 1.0 |\n| half_life | 0.5 |\n| 2 * half_life | 0.25 |\n| 3 * half_life | 0.125 |\n\n### `time_decay_linear(ts, now, max_age_ms)`\n\nLinear decay that reaches zero at max_age:\n\n```datalog\n// Score events with linear decay (max age = 30 days)\n+event_score(Id, Type, time_decay_linear(Ts, Now, 2592000000)) <-\n    events(Id, Type, Ts),\n    Now = time_now()\n\n? event_score(Id, Type, Score)\n```\n\n**Returns:** Float in range [0, 1]. Score = 0 when age >= max_age.\n\n**Formula:** `score = max(0, 1 - age / max_age)`\n\n| Age | Score |\n|-----|-------|\n| 0 | 1.0 |\n| max_age / 2 | 0.5 |\n| max_age | 0.0 |\n| > max_age | 0.0 |\n\n---\n\n## Interval Functions\n\nInterval functions help with scheduling, calendar analysis, and time-range queries.\n\n### `interval_duration(start, end)`\n\nCalculate the duration of an interval:\n\n```datalog\n+meeting(1, \"Standup\", 1704114000000, 1704115800000)  // 30 min meeting\n\n? meeting(Id, Name, Start, End),\n   Duration = interval_duration(Start, End)\n```\n\n**Returns:** `end - start` in milliseconds.\n\n### `point_in_interval(ts, start, end)`\n\nCheck if a timestamp falls within an interval:\n\n```datalog\n// Check if current time is during business hours\n+during_business_hours(true) <-\n    Now = time_now(),\n    business_start(Start),\n    business_end(End),\n    point_in_interval(Now, Start, End)\n```\n\n**Returns:** Boolean `true` if start <= ts <= end.\n\n### `intervals_overlap(s1, e1, s2, e2)`\n\nCheck if two intervals overlap:\n\n```datalog\n// Find conflicting meetings\n+conflict(M1, M2) <-\n    meeting(M1, _, S1, E1),\n    meeting(M2, _, S2, E2),\n    M1 < M2,\n    intervals_overlap(S1, E1, S2, E2)\n\n? conflict(M1, M2)\n```\n\n**Returns:** Boolean `true` if the intervals share any time.\n\n### `interval_contains(s1, e1, s2, e2)`\n\nCheck if interval 1 fully contains interval 2:\n\n```datalog\n// Find meetings that fit entirely within a time block\n+fits_in_block(MeetingId) <-\n    meeting(MeetingId, _, MStart, MEnd),\n    time_block(BlockStart, BlockEnd),\n    interval_contains(BlockStart, BlockEnd, MStart, MEnd)\n```\n\n**Returns:** Boolean `true` if [s2, e2] is entirely within [s1, e1].\n\n---\n\n## Practical Examples\n\n### Event Stream Analysis\n\n```datalog\n// Store user activity events\n+activity[(1, \"alice\", \"view\", 1704067200000),\n          (2, \"bob\", \"click\", 1704070800000),\n          (3, \"alice\", \"purchase\", 1704074400000),\n          (4, \"alice\", \"view\", 1704078000000)]\n\n// Count events per user in last hour\n+hourly_activity(User, count<Id>) <-\n    activity(Id, User, _, Ts),\n    Now = time_now(),\n    within_last(Ts, Now, 3600000)\n\n? hourly_activity(User, Count)\n```\n\n### Time-Decayed Recommendations\n\n```datalog\n// User interactions with items\n+interactions[(101, 1, \"view\", 1704067200000),\n              (101, 2, \"purchase\", 1704153600000),\n              (101, 1, \"view\", 1704240000000)]\n\n// Weight purchases higher than views\n+purchase_score(UserId, ItemId, sum<Score>) <-\n    interactions(UserId, ItemId, \"purchase\", Ts),\n    Now = time_now(),\n    Decay = time_decay(Ts, Now, 604800000),  // 7-day half-life\n    Score = 10.0 * Decay\n\n+view_score(UserId, ItemId, sum<Score>) <-\n    interactions(UserId, ItemId, \"view\", Ts),\n    Now = time_now(),\n    Decay = time_decay(Ts, Now, 604800000),\n    Score = 1.0 * Decay\n\n+item_score(UserId, ItemId, sum<S>) <-\n    purchase_score(UserId, ItemId, S)\n+item_score(UserId, ItemId, sum<S>) <-\n    view_score(UserId, ItemId, S)\n\n// Get top items for user\n+recommendations(UserId, ItemId, top_k<5, Score, desc>) <-\n    item_score(UserId, ItemId, Score)\n\n? recommendations(101, ItemId, Score)\n```\n\n### Session Detection\n\n```datalog\n// Detect sessions (gaps > 30 minutes = new session)\n+page_views[(1, \"alice\", \"/home\", 1704067200000),\n            (2, \"alice\", \"/products\", 1704067500000),\n            (3, \"alice\", \"/cart\", 1704069000000),\n            (4, \"alice\", \"/checkout\", 1704072600000)]  // 1 hour later\n\n+session_boundary(UserId, ViewId) <-\n    page_views(ViewId, UserId, _, Ts),\n    page_views(PrevId, UserId, _, PrevTs),\n    PrevId < ViewId,\n    Diff = time_diff(PrevTs, Ts),\n    Diff > 1800000  // 30 minute gap\n\n? session_boundary(UserId, ViewId)\n```\n\n### Meeting Scheduler\n\n```datalog\n// Existing meetings\n+meetings[(1, \"Team Sync\", 1704114000000, 1704117600000),\n          (2, \"1:1\", 1704121200000, 1704123000000),\n          (3, \"Planning\", 1704128400000, 1704135600000)]\n\n// Available time slots (potential meetings)\n+slots[(100, 1704106800000, 1704114000000),   // 9am-11am\n       (101, 1704117600000, 1704121200000),   // 12pm-1pm\n       (102, 1704123000000, 1704128400000)]   // 1:30pm-3pm\n\n// Find free slots (no overlap with existing meetings)\n+free_slots(SlotId, Start, End) <-\n    slots(SlotId, Start, End),\n    !has_conflict(SlotId)\n\n+has_conflict(SlotId) <-\n    slots(SlotId, S1, E1),\n    meetings(_, _, S2, E2),\n    intervals_overlap(S1, E1, S2, E2)\n\n? free_slots(SlotId, Start, End)\n```\n\n---\n\n## Function Reference\n\n| Function | Signature | Returns |\n|----------|-----------|---------|\n| `time_now` | `time_now()` | Current timestamp (ms) |\n| `time_diff` | `time_diff(t1, t2)` | t1 - t2 (ms) |\n| `time_add` | `time_add(ts, duration)` | ts + duration |\n| `time_sub` | `time_sub(ts, duration)` | ts - duration |\n| `time_before` | `time_before(t1, t2)` | t1 < t2 |\n| `time_after` | `time_after(t1, t2)` | t1 > t2 |\n| `time_between` | `time_between(ts, start, end)` | start <= ts <= end |\n| `within_last` | `within_last(ts, now, duration)` | ts within duration of now |\n| `time_decay` | `time_decay(ts, now, half_life)` | Exponential decay [0,1] |\n| `time_decay_linear` | `time_decay_linear(ts, now, max_age)` | Linear decay [0,1] |\n| `interval_duration` | `interval_duration(start, end)` | end - start (ms) |\n| `point_in_interval` | `point_in_interval(ts, start, end)` | ts in [start, end] |\n| `intervals_overlap` | `intervals_overlap(s1, e1, s2, e2)` | Intervals share time |\n| `interval_contains` | `interval_contains(s1, e1, s2, e2)` | [s1,e1] contains [s2,e2] |\n\n---\n\n## Next Steps\n\n- [Vector Search](vectors) - Combine temporal decay with similarity\n- [Configuration](configuration) - Persistence settings for event data",
    "toc": [
      {
        "level": 2,
        "text": "Timestamp Basics",
        "id": "timestamp-basics"
      },
      {
        "level": 3,
        "text": "Common Time Constants",
        "id": "common-time-constants"
      },
      {
        "level": 2,
        "text": "Core Time Functions",
        "id": "core-time-functions"
      },
      {
        "level": 3,
        "text": "timenow()",
        "id": "timenow"
      },
      {
        "level": 3,
        "text": "timediff(t1, t2)",
        "id": "timedifft1-t2"
      },
      {
        "level": 3,
        "text": "timeadd(ts, durationms)",
        "id": "timeaddts-durationms"
      },
      {
        "level": 3,
        "text": "timesub(ts, durationms)",
        "id": "timesubts-durationms"
      },
      {
        "level": 2,
        "text": "Time Comparisons",
        "id": "time-comparisons"
      },
      {
        "level": 3,
        "text": "timebefore(t1, t2)",
        "id": "timebeforet1-t2"
      },
      {
        "level": 3,
        "text": "timeafter(t1, t2)",
        "id": "timeaftert1-t2"
      },
      {
        "level": 3,
        "text": "timebetween(ts, start, end)",
        "id": "timebetweents-start-end"
      },
      {
        "level": 3,
        "text": "withinlast(ts, now, durationms)",
        "id": "withinlastts-now-durationms"
      },
      {
        "level": 2,
        "text": "Time Decay Functions",
        "id": "time-decay-functions"
      },
      {
        "level": 3,
        "text": "timedecay(ts, now, halflifems)",
        "id": "timedecayts-now-halflifems"
      },
      {
        "level": 3,
        "text": "timedecaylinear(ts, now, maxagems)",
        "id": "timedecaylinearts-now-maxagems"
      },
      {
        "level": 2,
        "text": "Interval Functions",
        "id": "interval-functions"
      },
      {
        "level": 3,
        "text": "intervalduration(start, end)",
        "id": "intervaldurationstart-end"
      },
      {
        "level": 3,
        "text": "pointininterval(ts, start, end)",
        "id": "pointinintervalts-start-end"
      },
      {
        "level": 3,
        "text": "intervalsoverlap(s1, e1, s2, e2)",
        "id": "intervalsoverlaps1-e1-s2-e2"
      },
      {
        "level": 3,
        "text": "intervalcontains(s1, e1, s2, e2)",
        "id": "intervalcontainss1-e1-s2-e2"
      },
      {
        "level": 2,
        "text": "Practical Examples",
        "id": "practical-examples"
      },
      {
        "level": 3,
        "text": "Event Stream Analysis",
        "id": "event-stream-analysis"
      },
      {
        "level": 3,
        "text": "Time-Decayed Recommendations",
        "id": "time-decayed-recommendations"
      },
      {
        "level": 3,
        "text": "Session Detection",
        "id": "session-detection"
      },
      {
        "level": 3,
        "text": "Meeting Scheduler",
        "id": "meeting-scheduler"
      },
      {
        "level": 2,
        "text": "Function Reference",
        "id": "function-reference"
      },
      {
        "level": 2,
        "text": "Next Steps",
        "id": "next-steps"
      }
    ]
  },
  "guides/troubleshooting": {
    "title": "Troubleshooting: Common Errors",
    "content": "# Troubleshooting: Common Errors\n\nThis guide covers common errors and how to resolve them.\n\n## Parse Errors\n\n### \"Unexpected token\"\n\n**Cause**: Syntax error in statement.\n\n```datalog\n// Wrong\n+edge(1, 2\n\n// Correct\n+edge(1, 2)\n```\n\n### \"Invalid relation name\"\n\n**Cause**: Relation names must start with lowercase.\n\n```datalog\n// Wrong\n+Edge(1, 2)\n\n// Correct\n+edge(1, 2)\n```\n\n### \"Expected '<-'\"\n\n**Cause**: Rule syntax error.\n\n```datalog\n// Wrong - missing <-\n+path(X, Y) edge(X, Y)\n\n// Correct\n+path(X, Y) <- edge(X, Y)\n```\n\n### \"Unclosed parenthesis\"\n\n**Cause**: Mismatched parentheses.\n\n```datalog\n// Wrong\n+edge((1, 2)\n\n// Correct\n+edge(1, 2)\n```\n\n## Type Errors\n\n### \"Type mismatch\"\n\n**Cause**: Value doesn't match schema.\n\n```datalog\n// If schema is: +person(id: int, name: string)\n\n// Wrong\n+person(\"alice\", 1)\n\n// Correct\n+person(1, \"alice\")\n```\n\n### \"Expected int, got float\"\n\n**Cause**: Using 1.0 where 1 is expected.\n\n```datalog\n// Wrong (if relation expects int)\n+score(1.0)\n\n// Correct\n+score(1)\n```\n\n## Query Errors\n\n### \"Unknown relation\"\n\n**Cause**: Querying a relation that doesn't exist.\n\n```datalog\n?nonexistent(X)\n// Error: Unknown relation 'nonexistent'\n```\n\n**Solutions**:\n1. Check spelling\n2. Use `.rel` to list existing relations\n3. Create the relation first with `+relation(...)`\n\n### \"Unbound variable in head\"\n\n**Cause**: Variable in rule head not used in body.\n\n```datalog\n// Wrong - Z is not in the body\n+path(X, Z) <- edge(X, Y)\n\n// Correct\n+path(X, Y) <- edge(X, Y)\n```\n\n### \"Unsafe variable\"\n\n**Cause**: Variable only appears in negation or constraint.\n\n```datalog\n// Wrong - X only appears in negation\n+orphan(X) <- !parent(_, X)\n\n// Correct - X must appear positively\n+orphan(X) <- person(X), !parent(_, X)\n```\n\n## Recursion Errors\n\n### \"Non-stratifiable program\"\n\n**Cause**: Negation through recursion.\n\n```datalog\n// Wrong - circular negation\n+a(X) <- b(X)\n+b(X) <- !a(X)  // Error: a depends on not-a\n```\n\n**Solution**: Restructure to avoid negation in recursive cycles.\n\n### \"Recursion timeout\"\n\n**Cause**: Infinite recursion or very deep recursion.\n\n```datalog\n// Potential issue - unbounded generation\n+nums(0)\n+nums(N) <- nums(M), N = M + 1  // Never terminates!\n```\n\n**Solution**: Add termination conditions.\n\n```datalog\n+nums(0)\n+nums(N) <- nums(M), N = M + 1, N < 100  // Bounded\n```\n\n## Knowledge Graph Errors\n\n### \"Knowledge graph not found\"\n\n**Cause**: Trying to use a non-existent knowledge graph.\n\n```datalog\n.kg use nonexistent\n// Error: Knowledge graph 'nonexistent' not found\n```\n\n**Solution**: Create it first.\n\n```datalog\n.kg create mykg\n.kg use mykg\n```\n\n### \"Cannot drop current knowledge graph\"\n\n**Cause**: Trying to drop the knowledge graph you're in.\n\n**Solution**: Switch to another knowledge graph first.\n\n```datalog\n.kg use default\n.kg drop mykg\n```\n\n### \"No current knowledge graph\"\n\n**Cause**: Operating without selecting a knowledge graph.\n\n**Solution**: Use `.kg use <name>` first.\n\n## Rule Errors\n\n### \"Rule not found\"\n\n**Cause**: Trying to query/drop a rule that doesn't exist.\n\n```datalog\n.rule drop nonexistent\n// Error: Rule 'nonexistent' not found\n```\n\n**Solution**: Check `.rule` to list existing rules.\n\n### \"Duplicate rule clause\"\n\n**Cause**: Adding the exact same rule twice.\n\n**Note**: This is usually a warning, not an error. The duplicate is ignored.\n\n## Aggregation Errors\n\n### \"Aggregation variable must be grouped\"\n\n**Cause**: Using a variable in the head without aggregating or grouping.\n\n```datalog\n// Wrong - Name appears but isn't grouped\n+total(Name, sum<Amount>) <- purchase(_, Amount)\n\n// Correct - Name is in the body\n+total(Name, sum<Amount>) <- purchase(Name, Amount)\n```\n\n### \"Cannot aggregate over empty set\"\n\n**Cause**: Aggregating with no matching facts.\n\n**Note**: This returns empty results, not an error. Check your filters.\n\n## Storage Errors\n\n### \"Permission denied\"\n\n**Cause**: Cannot write to data directory.\n\n**Solution**:\n```bash\nmkdir -p ./data\nchmod 755 ./data\n```\n\n### \"Disk full\"\n\n**Cause**: No space for data files.\n\n**Solutions**:\n1. Free up disk space\n2. Run `.compact` to consolidate storage\n3. Move data directory to larger disk\n\n### \"WAL corruption\"\n\n**Cause**: Crash during write operation.\n\n**Solution**: InputLayer should recover automatically. If not:\n1. Check for `.wal` files in data directory\n2. Remove corrupt WAL files (will lose uncommitted data)\n3. Restart\n\n## Performance Issues\n\n### \"Query taking too long\"\n\n**Possible causes**:\n1. Very large datasets\n2. Cartesian product (joining without shared variables)\n3. Deep recursion\n\n**Solutions**:\n\n1. **Add constraints early**:\n   ```datalog\n   // Slow - filters after join\n   ?huge_table1(X, Y), huge_table2(Y, Z), X < 10\n\n   // Fast - filter first\n   ?huge_table1(X, Y), X < 10, huge_table2(Y, Z)\n   ```\n\n2. **Check for Cartesian products**:\n   ```datalog\n   // Bad - no shared variables = cross product\n   ?table1(X), table2(Y)\n\n   // Good - joined on Y\n   ?table1(X, Y), table2(Y, Z)\n   ```\n\n3. **Limit results**:\n   ```datalog\n   // For exploration, check small sample first\n   ?huge_relation(X, Y), X < 10\n   ```\n\n### \"High memory usage\"\n\n**Solutions**:\n1. Run `.compact` to consolidate storage\n2. Reduce intermediate result sizes with filters\n3. Break large operations into smaller chunks\n\n## Getting More Help\n\n### Debug Mode\n\nSet environment variable for verbose output:\n```bash\nIL_TRACE_LEVEL=debug inputlayer-server\n```\n\n### Check System Status\n\n```datalog\n.status\n```\n\nShows knowledge graph info and other diagnostics.\n\n### File Locations\n\nFind data files:\n```bash\nls -la ./data/\n```\n\nCheck logs (if file logging configured via `IL_TRACE_FILE`):\n```bash\ntail -f /path/to/configured/log/file\n```\n\n## Error Message Quick Reference\n\n| Error | Likely Cause | Solution |\n|-------|--------------|----------|\n| \"Unexpected token\" | Syntax error | Check statement syntax |\n| \"Unknown relation\" | Typo or missing data | Check spelling, `.rel` |\n| \"Type mismatch\" | Wrong value type | Check schema |\n| \"Unbound variable\" | Variable not in body | Add to body predicate |\n| \"Non-stratifiable\" | Negation cycle | Restructure rules |\n| \"Knowledge graph not found\" | Wrong name | `.kg list` to check |\n| \"Permission denied\" | File permissions | Check data dir |\n\n## Still Stuck?\n\n1. Check the examples for working patterns\n2. Report issues at: https://github.com/inputlayer/inputlayer/issues",
    "toc": [
      {
        "level": 2,
        "text": "Parse Errors",
        "id": "parse-errors"
      },
      {
        "level": 3,
        "text": "\"Unexpected token\"",
        "id": "unexpected-token"
      },
      {
        "level": 3,
        "text": "\"Invalid relation name\"",
        "id": "invalid-relation-name"
      },
      {
        "level": 3,
        "text": "\"Expected '<-'\"",
        "id": "expected-"
      },
      {
        "level": 3,
        "text": "\"Unclosed parenthesis\"",
        "id": "unclosed-parenthesis"
      },
      {
        "level": 2,
        "text": "Type Errors",
        "id": "type-errors"
      },
      {
        "level": 3,
        "text": "\"Type mismatch\"",
        "id": "type-mismatch"
      },
      {
        "level": 3,
        "text": "\"Expected int, got float\"",
        "id": "expected-int-got-float"
      },
      {
        "level": 2,
        "text": "Query Errors",
        "id": "query-errors"
      },
      {
        "level": 3,
        "text": "\"Unknown relation\"",
        "id": "unknown-relation"
      },
      {
        "level": 3,
        "text": "\"Unbound variable in head\"",
        "id": "unbound-variable-in-head"
      },
      {
        "level": 3,
        "text": "\"Unsafe variable\"",
        "id": "unsafe-variable"
      },
      {
        "level": 2,
        "text": "Recursion Errors",
        "id": "recursion-errors"
      },
      {
        "level": 3,
        "text": "\"Non-stratifiable program\"",
        "id": "non-stratifiable-program"
      },
      {
        "level": 3,
        "text": "\"Recursion timeout\"",
        "id": "recursion-timeout"
      },
      {
        "level": 2,
        "text": "Knowledge Graph Errors",
        "id": "knowledge-graph-errors"
      },
      {
        "level": 3,
        "text": "\"Knowledge graph not found\"",
        "id": "knowledge-graph-not-found"
      },
      {
        "level": 3,
        "text": "\"Cannot drop current knowledge graph\"",
        "id": "cannot-drop-current-knowledge-graph"
      },
      {
        "level": 3,
        "text": "\"No current knowledge graph\"",
        "id": "no-current-knowledge-graph"
      },
      {
        "level": 2,
        "text": "Rule Errors",
        "id": "rule-errors"
      },
      {
        "level": 3,
        "text": "\"Rule not found\"",
        "id": "rule-not-found"
      },
      {
        "level": 3,
        "text": "\"Duplicate rule clause\"",
        "id": "duplicate-rule-clause"
      },
      {
        "level": 2,
        "text": "Aggregation Errors",
        "id": "aggregation-errors"
      },
      {
        "level": 3,
        "text": "\"Aggregation variable must be grouped\"",
        "id": "aggregation-variable-must-be-grouped"
      },
      {
        "level": 3,
        "text": "\"Cannot aggregate over empty set\"",
        "id": "cannot-aggregate-over-empty-set"
      },
      {
        "level": 2,
        "text": "Storage Errors",
        "id": "storage-errors"
      },
      {
        "level": 3,
        "text": "\"Permission denied\"",
        "id": "permission-denied"
      },
      {
        "level": 3,
        "text": "\"Disk full\"",
        "id": "disk-full"
      },
      {
        "level": 3,
        "text": "\"WAL corruption\"",
        "id": "wal-corruption"
      },
      {
        "level": 2,
        "text": "Performance Issues",
        "id": "performance-issues"
      },
      {
        "level": 3,
        "text": "\"Query taking too long\"",
        "id": "query-taking-too-long"
      },
      {
        "level": 3,
        "text": "\"High memory usage\"",
        "id": "high-memory-usage"
      },
      {
        "level": 2,
        "text": "Getting More Help",
        "id": "getting-more-help"
      },
      {
        "level": 3,
        "text": "Debug Mode",
        "id": "debug-mode"
      },
      {
        "level": 3,
        "text": "Check System Status",
        "id": "check-system-status"
      },
      {
        "level": 3,
        "text": "File Locations",
        "id": "file-locations"
      },
      {
        "level": 2,
        "text": "Error Message Quick Reference",
        "id": "error-message-quick-reference"
      },
      {
        "level": 2,
        "text": "Still Stuck?",
        "id": "still-stuck"
      }
    ]
  },
  "guides/vectors": {
    "title": "Vector Search Tutorial",
    "content": "# Vector Search Tutorial\n\nStore embeddings alongside structured data. Query by cosine, euclidean, or dot-product similarity - no external vector DB needed.\n\n## Vector Basics\n\n### Storing Vectors\n\nVectors are arrays of floating-point numbers, typically embeddings from ML models:\n\n```datalog\n// Store document embeddings\n+document(1, \"Introduction to Datalog\", [0.1, 0.2, 0.3, 0.4])\n+document(2, \"Vector Similarity\", [0.15, 0.25, 0.28, 0.42])\n+document(3, \"Graph Databases\", [0.8, 0.1, 0.05, 0.05])\n```\n\n### Schema with Vectors\n\nDeclare vector columns in your schema:\n\n```datalog\n+document(id: int, title: string, embedding: vector)\n```\n\n---\n\n## Distance Functions\n\nInputLayer supports 4 distance metrics:\n\n### Euclidean Distance\n\nL2 distance - straight-line distance in vector space:\n\n```datalog\n? document(Id1, _, V1), document(Id2, _, V2),\n   Id1 < Id2,\n   Dist = euclidean(V1, V2)\n```\n\n### Cosine Distance\n\nAngular distance (1 - cosine similarity) - ignores magnitude:\n\n```datalog\n? document(Id1, _, V1), document(Id2, _, V2),\n   Id1 < Id2,\n   Dist = cosine(V1, V2)\n```\n\n**Use cosine for:** Text embeddings, normalized vectors\n\n### Dot Product\n\nInner product - higher is more similar:\n\n```datalog\n? document(Id1, _, V1), document(Id2, _, V2),\n   Id1 < Id2,\n   Score = dot(V1, V2)\n```\n\n**Use dot product for:** When vectors have meaningful magnitude\n\n### Manhattan Distance\n\nL1 distance - sum of absolute differences:\n\n```datalog\n? document(Id1, _, V1), document(Id2, _, V2),\n   Id1 < Id2,\n   Dist = manhattan(V1, V2)\n```\n\n---\n\n## Semantic Search Example\n\nBuild a document search system:\n\n```datalog\n// Store documents with embeddings\n+docs[(1, \"Introduction to Datalog\", [0.1, 0.2, 0.3]),\n      (2, \"Vector Databases\", [0.12, 0.22, 0.31]),\n      (3, \"Graph Theory\", [0.8, 0.1, 0.05]),\n      (4, \"Machine Learning\", [0.15, 0.18, 0.28])]\n\n// Query vector (from your embedding model)\nquery_vec([0.11, 0.21, 0.29])\n\n// Find similar documents\n+similar(Id, Title, top_k<3, Dist>) <-\n    query_vec(QV),\n    docs(Id, Title, V),\n    Dist = cosine(QV, V)\n\n? similar(Id, Title, Dist)\n```\n\n**Output:**\n\n| Id | Title | Dist |\n|---|---|---|\n| 1 | \"Introduction to Datalog\" | 0.002 |\n| 2 | \"Vector Databases\" | 0.003 |\n| 4 | \"Machine Learning\" | 0.015 |\n\n*3 rows*\n\n---\n\n## Vector Operations\n\n### Normalize\n\nConvert to unit vector (length 1):\n\n```datalog\n? document(Id, _, V),\n   NormV = normalize(V)\n```\n\n### Get Dimension\n\nGet the number of elements:\n\n```datalog\n? document(Id, _, V),\n   Dim = vec_dim(V)\n```\n\n### Add Vectors\n\nElement-wise addition:\n\n```datalog\n? v1([1.0, 2.0, 3.0]), v2([0.5, 0.5, 0.5]),\n   Sum = vec_add(V1, V2)  // [1.5, 2.5, 3.5]\n```\n\n### Scale Vector\n\nMultiply by scalar:\n\n```datalog\n? v([1.0, 2.0, 3.0]),\n   Scaled = vec_scale(V, 2.0)  // [2.0, 4.0, 6.0]\n```\n\n---\n\n## LSH (Locality Sensitive Hashing)\n\nFor approximate nearest neighbor search on large datasets:\n\n### Basic LSH Bucket\n\nHash vectors into buckets:\n\n```datalog\n? document(Id, _, V),\n   Bucket = lsh_bucket(V, 0, 8)  // table 0, 8 hyperplanes\n```\n\n### LSH Probes\n\nGet multiple candidate buckets to check:\n\n```datalog\n? query_vec(QV),\n   Buckets = lsh_probes(QV, 0, 8, 3)  // 3 probe levels\n```\n\n### LSH Multi-Probe Search\n\nFull multi-probe search:\n\n```datalog\n+candidates(Id, Dist) <-\n    query_vec(QV),\n    Probes = lsh_multi_probe(QV, 0, 8, 3),\n    member(Bucket, Probes),\n    document(Id, _, V),\n    lsh_bucket(V, 0, 8) = Bucket,\n    Dist = cosine(QV, V)\n```\n\n---\n\n## Int8 Quantization\n\nReduce memory by 75% using 8-bit quantization:\n\n### Quantize Vectors\n\n```datalog\n// Linear quantization (uniform distribution)\n+quantized(Id, quantize_linear(V)) <- document(Id, _, V)\n\n// Symmetric quantization (centered at 0)\n+quantized(Id, quantize_symmetric(V)) <- document(Id, _, V)\n```\n\n### Dequantize\n\nConvert back to float:\n\n```datalog\n? quantized(Id, QV),\n   V = dequantize(QV)\n```\n\n### Int8 Distance Functions\n\nDirect computation on quantized vectors:\n\n```datalog\n? quantized(Id1, QV1), quantized(Id2, QV2),\n   Dist = euclidean_int8(QV1, QV2)\n\n// Also available: cosine_int8, dot_int8, manhattan_int8\n```\n\n---\n\n## Building a Recommendation System\n\nComplete example for item recommendations:\n\n```datalog\n// Item embeddings (from your ML model)\n+items[(1, \"Blue T-Shirt\", [0.2, 0.8, 0.1, 0.3]),\n       (2, \"Red Dress\", [0.25, 0.75, 0.15, 0.35]),\n       (3, \"Running Shoes\", [0.9, 0.1, 0.8, 0.2]),\n       (4, \"Hiking Boots\", [0.85, 0.15, 0.75, 0.25]),\n       (5, \"Formal Shoes\", [0.4, 0.6, 0.3, 0.5])]\n\n// User purchase history (for creating user profile)\n+purchases[(101, 1), (101, 2),    // User 101 bought shirts & dresses\n           (102, 3), (102, 4)]    // User 102 bought athletic footwear\n\n// Compute user profile as average of purchased item embeddings\n+user_profile(UserId, avg<V>) <-\n    purchases(UserId, ItemId),\n    items(ItemId, _, V)\n\n// Recommend items similar to user profile, excluding already purchased\n+recommendations(UserId, ItemId, Name, top_k<3, Dist>) <-\n    user_profile(UserId, Profile),\n    items(ItemId, Name, V),\n    !purchases(UserId, ItemId),  // Exclude already purchased\n    Dist = cosine(Profile, V)\n\n// Get recommendations for user 101\n? recommendations(101, ItemId, Name, Dist)\n```\n\n---\n\n## Performance Tips\n\n### 1. Use Appropriate Distance Metric\n\n| Embedding Type | Recommended Metric |\n|---------------|-------------------|\n| Text (BERT, etc.) | `cosine` |\n| Images | `euclidean` or `cosine` |\n| Normalized | `dot` (fastest) |\n\n### 2. Create HNSW Indexes\n\nFor large datasets (>10K vectors):\n\n```\n.index create doc_idx on documents(embedding) metric cosine m 16 ef_construction 200 ef_search 50\n```\n\n### 2b. Query HNSW Indexes\n\nUse `hnsw_nearest()` in query bodies to perform fast approximate nearest-neighbor search:\n\n```datalog\n// Find 5 nearest neighbors\n? hnsw_nearest(\"doc_idx\", [0.1, 0.2, 0.3], 5, Id, Dist)\n\n// Use with a bound query vector\n? query_vec(QV), hnsw_nearest(\"doc_idx\", QV, 10, Id, Dist)\n\n// Override ef_search for higher recall\n? hnsw_nearest(\"doc_idx\", [0.1, 0.2, 0.3], 5, Id, Dist, 200)\n```\n\n**Syntax:** `hnsw_nearest(\"index_name\", QueryVec, K, IdVar, DistVar [, EfSearch])`\n- `index_name` - String literal naming the HNSW index\n- `QueryVec` - Variable bound to a vector, or a vector literal\n- `K` - Integer: number of neighbors to return\n- `IdVar` - Variable to bind result IDs\n- `DistVar` - Variable to bind distances\n- `EfSearch` - Optional integer: override ef_search for this query\n\n### 3. Use Quantization for Memory\n\nFor millions of vectors, quantize to Int8:\n\n```datalog\n+docs_quantized(Id, Title, quantize_symmetric(V)) <- docs(Id, Title, V)\n```\n\n### 4. Filter Before Distance Computation\n\n```datalog\n// Filter first, then compute distances\n+similar(Id, Title, Dist) <-\n    query_vec(QV),\n    docs(Id, Title, V, Category),\n    Category = \"technology\",     // Filter first\n    Dist = cosine(QV, V)         // Then compute distance\n```\n\n---\n\n## Next Steps\n\n- [Indexing Guide](indexing) - Create HNSW indexes for fast search\n- [Temporal Functions](temporal) - Add time-decay to recommendations",
    "toc": [
      {
        "level": 2,
        "text": "Vector Basics",
        "id": "vector-basics"
      },
      {
        "level": 3,
        "text": "Storing Vectors",
        "id": "storing-vectors"
      },
      {
        "level": 3,
        "text": "Schema with Vectors",
        "id": "schema-with-vectors"
      },
      {
        "level": 2,
        "text": "Distance Functions",
        "id": "distance-functions"
      },
      {
        "level": 3,
        "text": "Euclidean Distance",
        "id": "euclidean-distance"
      },
      {
        "level": 3,
        "text": "Cosine Distance",
        "id": "cosine-distance"
      },
      {
        "level": 3,
        "text": "Dot Product",
        "id": "dot-product"
      },
      {
        "level": 3,
        "text": "Manhattan Distance",
        "id": "manhattan-distance"
      },
      {
        "level": 2,
        "text": "Semantic Search Example",
        "id": "semantic-search-example"
      },
      {
        "level": 2,
        "text": "Vector Operations",
        "id": "vector-operations"
      },
      {
        "level": 3,
        "text": "Normalize",
        "id": "normalize"
      },
      {
        "level": 3,
        "text": "Get Dimension",
        "id": "get-dimension"
      },
      {
        "level": 3,
        "text": "Add Vectors",
        "id": "add-vectors"
      },
      {
        "level": 3,
        "text": "Scale Vector",
        "id": "scale-vector"
      },
      {
        "level": 2,
        "text": "LSH (Locality Sensitive Hashing)",
        "id": "lsh-locality-sensitive-hashing"
      },
      {
        "level": 3,
        "text": "Basic LSH Bucket",
        "id": "basic-lsh-bucket"
      },
      {
        "level": 3,
        "text": "LSH Probes",
        "id": "lsh-probes"
      },
      {
        "level": 3,
        "text": "LSH Multi-Probe Search",
        "id": "lsh-multi-probe-search"
      },
      {
        "level": 2,
        "text": "Int8 Quantization",
        "id": "int8-quantization"
      },
      {
        "level": 3,
        "text": "Quantize Vectors",
        "id": "quantize-vectors"
      },
      {
        "level": 3,
        "text": "Dequantize",
        "id": "dequantize"
      },
      {
        "level": 3,
        "text": "Int8 Distance Functions",
        "id": "int8-distance-functions"
      },
      {
        "level": 2,
        "text": "Building a Recommendation System",
        "id": "building-a-recommendation-system"
      },
      {
        "level": 2,
        "text": "Performance Tips",
        "id": "performance-tips"
      },
      {
        "level": 3,
        "text": "1. Use Appropriate Distance Metric",
        "id": "1-use-appropriate-distance-metric"
      },
      {
        "level": 3,
        "text": "2. Create HNSW Indexes",
        "id": "2-create-hnsw-indexes"
      },
      {
        "level": 3,
        "text": "2b. Query HNSW Indexes",
        "id": "2b-query-hnsw-indexes"
      },
      {
        "level": 3,
        "text": "3. Use Quantization for Memory",
        "id": "3-use-quantization-for-memory"
      },
      {
        "level": 3,
        "text": "4. Filter Before Distance Computation",
        "id": "4-filter-before-distance-computation"
      },
      {
        "level": 2,
        "text": "Next Steps",
        "id": "next-steps"
      }
    ]
  },
  "guides/websocket-api": {
    "title": "WebSocket API",
    "content": "# WebSocket API\n\nThe WebSocket API provides real-time, bidirectional communication with InputLayer. It supports authentication, query execution, streaming results, session rules, and push notifications.\n\n## Connection\n\nConnect to the global WebSocket endpoint:\n\n```\nws://host:port/ws\n```\n\nWith TLS (via reverse proxy):\n\n```\nwss://host:port/ws\n```\n\nOptionally specify a knowledge graph via query parameter:\n\n```\nws://host:port/ws?kg=mydb\n```\n\n## Authentication\n\nThe first message after connection must authenticate. There are two methods:\n\n### Username/Password\n\n```json\n{\n  \"type\": \"login\",\n  \"username\": \"admin\",\n  \"password\": \"admin\"\n}\n```\n\n### API Key\n\n```json\n{\n  \"type\": \"authenticate\",\n  \"api_key\": \"your-api-key\"\n}\n```\n\n### Auth Response\n\nOn success:\n\n```json\n{\n  \"type\": \"authenticated\",\n  \"session_id\": \"a1b2c3d4\",\n  \"knowledge_graph\": \"default\",\n  \"version\": \"0.1.0\",\n  \"role\": \"admin\"\n}\n```\n\nOn failure:\n\n```json\n{\n  \"type\": \"auth_error\",\n  \"message\": \"Invalid credentials\"\n}\n```\n\nThe server allows 30 seconds for authentication before closing the connection.\n\n## Executing Statements\n\nSend any Datalog statement or meta command:\n\n```json\n{\n  \"type\": \"execute\",\n  \"program\": \"?edge(X, Y)\"\n}\n```\n\nThe `program` field accepts any valid InputLayer statement: queries, inserts, rule definitions, meta commands, etc.\n\n### Result Response\n\n```json\n{\n  \"type\": \"result\",\n  \"columns\": [\"X\", \"Y\"],\n  \"rows\": [[1, 2], [2, 3], [3, 4]],\n  \"row_count\": 3,\n  \"total_count\": 3,\n  \"truncated\": false,\n  \"execution_time_ms\": 2\n}\n```\n\n### Error Response\n\n```json\n{\n  \"type\": \"error\",\n  \"message\": \"Unknown relation 'edge'\"\n}\n```\n\n## Knowledge Graph Operations\n\nUse meta commands via execute:\n\n```json\n{\"type\": \"execute\", \"program\": \".kg create mydb\"}\n{\"type\": \"execute\", \"program\": \".kg use mydb\"}\n{\"type\": \"execute\", \"program\": \".kg list\"}\n{\"type\": \"execute\", \"program\": \".kg drop mydb\"}\n```\n\nWhen `.kg use` switches knowledge graphs, the result includes a `switched_kg` field.\n\n## Streaming Results\n\nFor large result sets (> 1MB serialized), the server automatically streams results in chunks:\n\n### Stream Start\n\n```json\n{\n  \"type\": \"result_start\",\n  \"columns\": [\"X\", \"Y\"],\n  \"total_count\": 50000,\n  \"truncated\": false,\n  \"execution_time_ms\": 245\n}\n```\n\n### Stream Chunk\n\n```json\n{\n  \"type\": \"result_chunk\",\n  \"rows\": [[1, 2], [2, 3]],\n  \"chunk_index\": 0\n}\n```\n\nEach chunk contains up to 500 rows. Chunks are sent sequentially.\n\n### Stream End\n\n```json\n{\n  \"type\": \"result_end\",\n  \"row_count\": 50000,\n  \"chunk_count\": 100\n}\n```\n\nSmall results (< 1MB) use the single `result` message, maintaining backward compatibility.\n\n## Session Rules\n\nSession rules are ephemeral rules scoped to the current WebSocket connection. They are automatically cleared when the connection closes.\n\n### Define Session Rule\n\nSession rules omit the `+` prefix:\n\n```json\n{\n  \"type\": \"execute\",\n  \"program\": \"temp(X, Y) <- edge(X, Y), X < Y\"\n}\n```\n\n### Session Facts\n\nInsert transient facts scoped to the session:\n\n```json\n{\n  \"type\": \"execute\",\n  \"program\": \"context(\\\"user\\\", \\\"alice\\\")\"\n}\n```\n\n### Query with Session Rules\n\nSession rules are automatically included when querying:\n\n```json\n{\n  \"type\": \"execute\",\n  \"program\": \"?temp(X, Y)\"\n}\n```\n\n### Clear Session\n\n```json\n{\n  \"type\": \"execute\",\n  \"program\": \".session clear\"\n}\n```\n\n## Notifications\n\nThe server pushes notifications when persistent data changes in the session's knowledge graph.\n\n### Persistent Update\n\nSent when base facts in a relation change:\n\n```json\n{\n  \"type\": \"persistent_update\",\n  \"knowledge_graph\": \"default\",\n  \"relation\": \"edge\",\n  \"operation\": \"insert\",\n  \"count\": 5,\n  \"timestamp_ms\": 1700000000000,\n  \"seq\": 42\n}\n```\n\n### Rule Change\n\nSent when a rule is registered, removed, or dropped:\n\n```json\n{\n  \"type\": \"rule_change\",\n  \"knowledge_graph\": \"default\",\n  \"rule_name\": \"reachable\",\n  \"operation\": \"registered\",\n  \"timestamp_ms\": 1700000000000,\n  \"seq\": 43\n}\n```\n\n### KG Change\n\nSent when a knowledge graph is created or dropped:\n\n```json\n{\n  \"type\": \"kg_change\",\n  \"knowledge_graph\": \"mydb\",\n  \"operation\": \"created\",\n  \"timestamp_ms\": 1700000000000,\n  \"seq\": 44\n}\n```\n\n### Schema Change\n\nSent when a relation is dropped:\n\n```json\n{\n  \"type\": \"schema_change\",\n  \"knowledge_graph\": \"default\",\n  \"entity\": \"edge\",\n  \"operation\": \"dropped\",\n  \"timestamp_ms\": 1700000000000,\n  \"seq\": 45\n}\n```\n\nThe `seq` field is a monotonic sequence number for deduplication on reconnect.\n\n## Keep-Alive\n\nSend a ping to keep the connection alive:\n\n```json\n{\"type\": \"ping\"}\n```\n\nResponse:\n\n```json\n{\"type\": \"pong\"}\n```\n\nThe server also sends WebSocket-level pings every 30 seconds to detect dead connections.\n\n## Connection Lifecycle\n\n1. **Connect** to `ws://host:port/ws`\n2. **Authenticate** with `login` or `authenticate`\n3. **Execute** statements and queries\n4. **Receive** notifications for data changes\n5. **Close** gracefully by sending a WebSocket Close frame\n\n### Graceful Close\n\nAlways send a Close frame before disconnecting to avoid server warnings:\n\n```python\n# Python\nawait websocket.close()\n```\n\n```javascript\n// JavaScript\nws.close(1000, \"Normal closure\");\n```\n\n## Reconnection\n\nIf the connection drops:\n\n1. Wait with exponential backoff (1s, 2s, 4s, 8s, max 30s)\n2. Reconnect and re-authenticate\n3. Session rules and facts are lost — re-define them after reconnecting\n4. Persistent data (facts, rules, indexes) is unaffected\n5. Use the `seq` field from notifications to detect missed events\n\n## Rate Limiting\n\nThe server applies per-connection rate limiting (default: 1000 messages/sec, configurable). Exceeding the limit returns:\n\n```json\n{\n  \"type\": \"error\",\n  \"message\": \"Rate limit exceeded (1000 msgs/sec)\"\n}\n```\n\n## Example: Python Client\n\n```python\n\n\n\n\nasync def main():\n    async with websockets.connect(\"ws://localhost:8080/ws\") as ws:\n        # Authenticate\n        await ws.send(json.dumps({\n            \"type\": \"login\",\n            \"username\": \"admin\",\n            \"password\": \"admin\"\n        }))\n        auth_resp = json.loads(await ws.recv())\n        assert auth_resp[\"type\"] == \"authenticated\"\n        print(f\"Session: {auth_resp['session_id']}\")\n\n        # Insert data\n        await ws.send(json.dumps({\n            \"type\": \"execute\",\n            \"program\": \"+edge[(1, 2), (2, 3), (3, 4)]\"\n        }))\n        result = json.loads(await ws.recv())\n\n        # Query\n        await ws.send(json.dumps({\n            \"type\": \"execute\",\n            \"program\": \"?edge(X, Y)\"\n        }))\n        result = json.loads(await ws.recv())\n        print(f\"Got {result['row_count']} rows\")\n\n        # Close gracefully\n        await ws.close()\n\nasyncio.run(main())\n```\n\n## Example: JavaScript Client\n\n```javascript\nconst ws = new WebSocket(\"ws://localhost:8080/ws\");\n\nws.onopen = () => {\n  ws.send(JSON.stringify({\n    type: \"login\",\n    username: \"admin\",\n    password: \"admin\"\n  }));\n};\n\nws.onmessage = (event) => {\n  const msg = JSON.parse(event.data);\n\n  if (msg.type === \"authenticated\") {\n    // Authenticated — start querying\n    ws.send(JSON.stringify({\n      type: \"execute\",\n      program: \"?edge(X, Y)\"\n    }));\n  }\n\n  if (msg.type === \"result\") {\n    console.log(`Query returned ${msg.row_count} rows`);\n  }\n\n  // Handle streaming results\n  if (msg.type === \"result_start\") {\n    console.log(`Streaming ${msg.total_count} rows...`);\n  }\n  if (msg.type === \"result_chunk\") {\n    console.log(`Received chunk ${msg.chunk_index}`);\n  }\n  if (msg.type === \"result_end\") {\n    console.log(`Stream complete: ${msg.row_count} rows`);\n  }\n\n  // Handle notifications\n  if (msg.type === \"persistent_update\") {\n    console.log(`${msg.relation}: ${msg.operation} ${msg.count} rows`);\n  }\n};\n```",
    "toc": [
      {
        "level": 2,
        "text": "Connection",
        "id": "connection"
      },
      {
        "level": 2,
        "text": "Authentication",
        "id": "authentication"
      },
      {
        "level": 3,
        "text": "Username/Password",
        "id": "usernamepassword"
      },
      {
        "level": 3,
        "text": "API Key",
        "id": "api-key"
      },
      {
        "level": 3,
        "text": "Auth Response",
        "id": "auth-response"
      },
      {
        "level": 2,
        "text": "Executing Statements",
        "id": "executing-statements"
      },
      {
        "level": 3,
        "text": "Result Response",
        "id": "result-response"
      },
      {
        "level": 3,
        "text": "Error Response",
        "id": "error-response"
      },
      {
        "level": 2,
        "text": "Knowledge Graph Operations",
        "id": "knowledge-graph-operations"
      },
      {
        "level": 2,
        "text": "Streaming Results",
        "id": "streaming-results"
      },
      {
        "level": 3,
        "text": "Stream Start",
        "id": "stream-start"
      },
      {
        "level": 3,
        "text": "Stream Chunk",
        "id": "stream-chunk"
      },
      {
        "level": 3,
        "text": "Stream End",
        "id": "stream-end"
      },
      {
        "level": 2,
        "text": "Session Rules",
        "id": "session-rules"
      },
      {
        "level": 3,
        "text": "Define Session Rule",
        "id": "define-session-rule"
      },
      {
        "level": 3,
        "text": "Session Facts",
        "id": "session-facts"
      },
      {
        "level": 3,
        "text": "Query with Session Rules",
        "id": "query-with-session-rules"
      },
      {
        "level": 3,
        "text": "Clear Session",
        "id": "clear-session"
      },
      {
        "level": 2,
        "text": "Notifications",
        "id": "notifications"
      },
      {
        "level": 3,
        "text": "Persistent Update",
        "id": "persistent-update"
      },
      {
        "level": 3,
        "text": "Rule Change",
        "id": "rule-change"
      },
      {
        "level": 3,
        "text": "KG Change",
        "id": "kg-change"
      },
      {
        "level": 3,
        "text": "Schema Change",
        "id": "schema-change"
      },
      {
        "level": 2,
        "text": "Keep-Alive",
        "id": "keep-alive"
      },
      {
        "level": 2,
        "text": "Connection Lifecycle",
        "id": "connection-lifecycle"
      },
      {
        "level": 3,
        "text": "Graceful Close",
        "id": "graceful-close"
      },
      {
        "level": 2,
        "text": "Reconnection",
        "id": "reconnection"
      },
      {
        "level": 2,
        "text": "Rate Limiting",
        "id": "rate-limiting"
      },
      {
        "level": 2,
        "text": "Example: Python Client",
        "id": "example-python-client"
      },
      {
        "level": 2,
        "text": "Example: JavaScript Client",
        "id": "example-javascript-client"
      }
    ]
  },
  "index": {
    "title": "InputLayer Documentation",
    "content": "\n# InputLayer\n\n**A streaming deductive knowledge graph database built on Differential Dataflow.**\n\nInputLayer combines the power of Datalog with incremental computation to provide real-time, declarative knowledge graph management for AI agents and applications.\n\n## Get Started\n\n- [Quick Start](/docs/guides/quickstart) — Get running in 5 minutes\n- [Installation](/docs/guides/installation) — Install InputLayer on your platform\n- [Your First Program](/docs/guides/first-program) — Write your first Datalog program\n\n## Core Concepts\n\n- [Data Modeling](/docs/guides/core-concepts) — Relations, facts, and rules\n- [Recursion](/docs/guides/recursion) — Transitive closure and recursive queries\n- [Vector Search](/docs/guides/vectors) — Similarity search with HNSW indexes\n\n## Reference\n\n- [Commands](/docs/reference/commands) — Complete command reference\n- [Functions](/docs/reference/functions) — All built-in functions\n- [Syntax](/docs/reference/syntax) — Datalog syntax reference",
    "toc": [
      {
        "level": 2,
        "text": "Get Started",
        "id": "get-started"
      },
      {
        "level": 2,
        "text": "Core Concepts",
        "id": "core-concepts"
      },
      {
        "level": 2,
        "text": "Reference",
        "id": "reference"
      }
    ]
  },
  "internals/architecture": {
    "title": "InputLayer Architecture",
    "content": "# InputLayer Architecture\n\n**Version**: 3.0 (Production-Ready)\n**Date**: 2026-02-05\n**Status**: All architectural issues resolved, ready for HNSW indexing\n\n---\n\n## Overview\n\nInputLayer is an incremental Datalog database engine built on Differential Dataflow (DD). The architecture supports:\n\n- **Persistent incremental computation** via DDComputation with shared arrangements\n- **Lock-free concurrent reads** via ArcSwap snapshot system\n- **Rule materialization** with automatic cascade invalidation\n- **Session isolation** for ephemeral facts and rules\n- **Multi-worker parallel execution** for batch queries\n\n**Test Coverage**: ~3107 unit tests + ~1121 snapshot tests = ~4228 total, all passing.\n\n---\n\n## 1. Architecture Overview\n\n```\n+-----------------------------------------------------------------------------+\n|                              StorageEngine                                    |\n|                    DashMap<String, Arc<RwLock<KnowledgeGraph>>>              |\n|                                                                               |\n|  +-------------------------------------------------------------------------+ |\n|  |                        KnowledgeGraph                                    | |\n|  |                                                                          | |\n|  |  +--------------------+  +--------------------+  +-------------------+  | |\n|  |  |   DatalogEngine    |  |    RuleCatalog     |  |  DDComputation    |  | |\n|  |  |                    |  |                    |  |   (Optional)      |  | |\n|  |  | input_tuples:      |  | rules: HashMap     |  |                   |  | |\n|  |  |   HashMap<String,  |  | catalog.json       |  | InputSessions     |  | |\n|  |  |   Vec<Tuple>>      |  |                    |  | TraceAgents       |  | |\n|  |  |                    |  | Stratification     |  | Arrangements      |  | |\n|  |  | num_workers: usize |  | Validation         |  | ProbeHandle<u64>  |  | |\n|  |  +--------------------+  +--------------------+  |                   |  | |\n|  |                                                   | DerivedRelations |  | |\n|  |  +--------------------------------------------+  |   Manager        |  | |\n|  |  |        ArcSwap<KnowledgeGraphSnapshot>      |  +-------------------+  | |\n|  |  |                                             |                          | |\n|  |  |  input_tuples: Arc<HashMap>                |                          | |\n|  |  |  rules: Arc<Vec<Rule>>                     |                          | |\n|  |  |  materialized_relations: Arc<HashSet>      |                          | |\n|  |  |  num_workers: usize                        |                          | |\n|  |  +--------------------------------------------+                          | |\n|  +-------------------------------------------------------------------------+ |\n|                                                                               |\n|  +-------------------------------------------------------------------------+ |\n|  |                          FilePersist                                     | |\n|  |              WAL (Write-Ahead Log) + Parquet Batch Files                | |\n|  +-------------------------------------------------------------------------+ |\n+-----------------------------------------------------------------------------+\n```\n\n---\n\n## 2. Core Components\n\n### 2.1 StorageEngine\n\n**Purpose**: Multi-knowledge-graph storage with concurrent access.\n\n```rust\npub struct StorageEngine {\n    knowledge_graphs: DashMap<String, Arc<RwLock<KnowledgeGraph>>>,\n    logical_time: AtomicU64,\n    persist: Arc<FilePersist>,\n    config: StorageConfig,\n}\n```\n\n**Key Features**:\n- **DashMap** enables concurrent access to different KGs without global locking\n- **Logical timestamps** provide monotonically increasing write ordering\n- **Per-KG locking** allows parallel queries to different knowledge graphs\n\n### 2.2 KnowledgeGraph\n\n**Purpose**: Single knowledge graph with data, rules, and optional incremental computation.\n\n```rust\npub struct KnowledgeGraph {\n    name: String,\n    engine: DatalogEngine,\n    rule_catalog: RuleCatalog,\n    schema_catalog: SchemaCatalog,  // Per-KG schema isolation\n    snapshot: ArcSwap<KnowledgeGraphSnapshot>,\n    dd_computation: Option<DDComputation>,\n    num_workers: usize,\n}\n```\n\n**Key Features**:\n- **DatalogEngine**: Holds base relation data (`input_tuples`, private with accessors)\n- **RuleCatalog**: Persistent rule storage with stratification validation\n- **SchemaCatalog**: Per-KG schema validation (isolated from other KGs)\n- **Snapshot**: Lock-free point-in-time consistent views via ArcSwap\n- **DDComputation**: Optional persistent incremental computation (returns `Result`)\n\n### 2.3 KnowledgeGraphSnapshot\n\n**Purpose**: Immutable point-in-time view for lock-free reads.\n\n```rust\npub struct KnowledgeGraphSnapshot {\n    version: u64,\n    timestamp: u64,\n    input_tuples: Arc<HashMap<String, Vec<Tuple>>>,\n    rules: Arc<Vec<Rule>>,\n    materialized_relations: Arc<HashSet<String>>,\n    num_workers: usize,\n}\n```\n\n**Key Features**:\n- **Arc-wrapped data** enables O(1) clone operations\n- **ArcSwap** provides atomic snapshot publication\n- **Materialization awareness** skips rules for already-materialized relations\n- **Session isolation** via `execute_with_session_facts()` method\n\n### 2.4 DDComputation\n\n**Purpose**: Persistent incremental computation with shared arrangements.\n\n```rust\npub struct DDComputation {\n    command_tx: Sender<DDCommand>,\n    current_time: Arc<AtomicU64>,\n    max_write_time: Arc<AtomicU64>,\n    derived_relations: Arc<Mutex<DerivedRelationsManager>>,\n    known_relations: Mutex<HashSet<String>>,\n    worker_handle: Option<JoinHandle<()>>,\n}\n```\n\n**Architecture**:\n```\nMain Thread --command_tx--> DD Worker Thread\n                            +- Owns timely Worker<u64>\n                            +- Owns InputSessions per relation\n                            +- Owns TraceAgents (arrangements)\n                            +- Steps worker in event loop\n                            +- Processes commands between steps\n```\n\n**Command Protocol**:\n```rust\nenum DDCommand {\n    InsertDelta { relation, updates: Vec<(Tuple, u64, isize)> },\n    AdvanceTime(u64),\n    WaitUntilCaughtUp(u64, oneshot::Sender<()>),\n    AddRelation(String),\n    RegisterRule { name, dependencies },\n    RemoveRule(String),\n    SetMaterialized { relation, tuples },\n    NotifyBaseUpdate(String),\n    ReadRelation(String, oneshot::Sender<Vec<Tuple>>),\n    Shutdown,\n}\n```\n\n### 2.5 DerivedRelationsManager\n\n**Purpose**: Manage materialized derived relations with validity tracking.\n\n```rust\npub struct DerivedRelationsManager {\n    compiled_rules: HashMap<String, CompiledRule>,\n    materialized: HashMap<String, MaterializedRelation>,\n    base_to_derived: HashMap<String, HashSet<String>>,\n    derived_to_derived: HashMap<String, HashSet<String>>,\n    base_versions: HashMap<String, u64>,\n}\n\npub struct MaterializedRelation {\n    tuples: Vec<Tuple>,\n    version: u64,\n    base_versions: HashMap<String, u64>,\n    valid: bool,\n    materialized_at: u64,\n}\n```\n\n**Key Features**:\n- **Dependency tracking** maps base relations to derived relations\n- **Cascade invalidation** automatically invalidates dependent materializations\n- **Validity tracking** ensures queries see consistent data\n- **Version management** for optimistic concurrency\n\n---\n\n## 3. Data Flow Paths\n\n### 3.1 Write Path\n\n```\nInsert Request\n    |\n    v\nFilePersist.append() --------------------------> Durability (WAL)\n    |\n    v\nKnowledgeGraph.insert_in_memory()\n    |\n    +-> engine.input_tuples.insert() -----------> In-memory state\n    |\n    +-> DDComputation.insert() (if enabled)\n        |\n        +-> InputSession.update(tuple, time, +1)\n        |\n        +-> notify_base_update() ---------------> Cascade invalidation\n            |\n            +-> Invalidate dependent materializations\n    |\n    v\nauto_rematerialize_invalid_rules() -------------> Recompute materializations\n    |\n    v\npublish_snapshot() -----------------------------> Atomic snapshot publication\n    |\n    +-> Merge valid materializations into input_tuples\n    |\n    +-> ArcSwap.store(new_snapshot)\n```\n\n### 3.2 Read Path (Query)\n\n```\nQuery Request\n    |\n    v\nHandler.query_program()\n    |\n    +-> Validate session rules -----------------> validate_rule()\n    |\n    +-> Parse statements\n    |\n    +-> Collect session facts and rules\n    |\n    v\nStorageEngine.execute_query_with_session_facts_on()\n    |\n    v\nKnowledgeGraphSnapshot.execute_with_session_facts()\n    |\n    +-> Clone input_tuples (isolated copy)\n    |\n    +-> Add session facts to clone\n    |\n    +-> Build combined program:\n    |   +-> Persistent rules (skip if materialized)\n    |   +-> Session rules\n    |   +-> Query\n    |\n    +-> DatalogEngine.execute_tuples()\n        |\n        +-> CodeGenerator with num_workers config\n    |\n    v\nResults (concurrent queries unaffected)\n```\n\n### 3.3 Materialization Flow\n\n```\nRule Registration\n    |\n    v\nRuleCatalog.register_rule()\n    |\n    +-> Stratification validation\n    |\n    +-> DDComputation.register_rule()\n        |\n        +-> DerivedRelationsManager.register_rule()\n    |\n    v\nauto_materialize_rule()\n    |\n    +-> Execute rule against current data\n    |\n    +-> DDComputation.set_materialized()\n    |\n    v\npublish_snapshot()\n    |\n    +-> Merge materialized tuples into snapshot\n    |\n    +-> Record materialized_relations set\n```\n\n---\n\n## 4. Consistency Guarantees\n\n### 4.1 Snapshot Consistency\n\n**Guarantee**: Point-in-time consistent views via immutable Arc-wrapped data.\n\n- Readers get consistent snapshots without holding locks\n- Snapshot cloning is O(1) due to Arc sharing\n- Session facts isolation via cloned HashMap\n- No TOCTOU vulnerabilities (lock held through publication)\n\n### 4.2 Write Consistency\n\n**Guarantee**: All writes are durable before acknowledgment.\n\n- WAL append before in-memory update\n- DD shadow writes propagate with proper timestamps\n- Cascade invalidation is atomic (compute-then-apply pattern)\n\n### 4.3 Read-After-Write Consistency\n\n**Guarantee**: Via `read_relation_consistent()` in DDComputation.\n\n```rust\npub fn read_relation_consistent(&self, relation: &str) -> Result<Vec<Tuple>> {\n    // 1. Advance time to max_write_time + 1\n    // 2. Wait for DD frontier to pass\n    // 3. Read from arrangement cursor\n}\n```\n\n### 4.4 Session Isolation\n\n**Guarantee**: Session facts and rules don't affect concurrent queries.\n\n```rust\npub fn execute_with_session_facts(\n    &self,\n    program: &str,\n    session_facts: Vec<(String, Tuple)>,\n) -> Result<Vec<Tuple>> {\n    // Clone input_tuples (isolated copy)\n    let mut isolated = (*self.input_tuples).clone();\n\n    // Add session facts to the clone\n    for (relation, tuple) in session_facts {\n        isolated.entry(relation).or_default().push(tuple);\n    }\n\n    // Execute against isolated state\n    // ...\n}\n```\n\n---\n\n## 5. Rule System\n\n### 5.1 Persistent Rules (`+` prefix)\n\n```datalog\n+path(X, Y) <- edge(X, Y)\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n```\n\n**Characteristics**:\n- Stored in RuleCatalog (JSON persistence)\n- Registered with DDComputation for dependency tracking\n- Auto-materialized on registration and base data changes\n- Cascade invalidation when dependencies change\n\n### 5.2 Session Rules (no prefix)\n\n```datalog\nreachable_from(Y) <- path(1, Y)\n?reachable_from(X)\n```\n\n**Characteristics**:\n- Ephemeral (per-request lifecycle)\n- Validated for stratification before execution\n- Can reference materialized persistent rules\n- Computed fresh each time (not cached)\n\n### 5.3 Rule Validation\n\n```rust\n/// Validate single rule (self-negation, head safety, range restriction)\npub fn validate_rule(rule: &Rule, name: &str) -> Result<(), String>;\n\n/// Validate rule set for stratification (negation cycles)\npub fn validate_rules_stratification(rules: &[Rule]) -> Result<(), String>;\n```\n\n**Checks**:\n- **Self-negation**: Can't negate own head\n- **Head variable safety**: All head variables must be bound by positive atoms\n- **Range restriction**: Variables in negated atoms must be bound by positive atoms\n- **Stratification**: No mutual negation cycles\n\n---\n\n## 6. Data Types\n\n### 6.1 Unified Tuple Format\n\n```rust\npub struct Tuple(pub Vec<Value>);\n\npub enum Value {\n    Null,\n    Bool(bool),\n    Int(i64),\n    Float(f64),\n    String(Arc<String>),\n    Vector(Arc<Vec<f32>>),\n    VectorInt8(Arc<Vec<i8>>),\n}\n```\n\n**Key Features**:\n- **Single format** throughout the system (Tuple2 removed)\n- **Arbitrary arity** tuples\n- **Vector support** for similarity search\n- **Abomonation** implemented for multi-worker DD\n\n### 6.2 Abomonation Implementation\n\n```rust\n// Proper entomb/exhume for multi-worker DD communication\nunsafe impl Abomonation for Value {\n    unsafe fn entomb<W: Write>(&self, write: &mut W) -> IoResult<()> {\n        // Write tag + data in custom format\n    }\n\n    unsafe fn exhume<'b>(&mut self, bytes: &'b mut [u8]) -> Option<&'b mut [u8]> {\n        // Reconstruct Value from bytes with proper Arc allocations\n    }\n}\n```\n\n### 6.3 AST Display Implementations\n\nAll AST types implement `Display` for consistent Datalog text formatting:\n\n```rust\nimpl Display for Term { ... }      // Variables, constants, aggregates, etc.\nimpl Display for Atom { ... }      // relation(arg1, arg2, ...)\nimpl Display for BodyPredicate { ... }  // Positive, negated, comparison\nimpl Display for Rule { ... }      // head <- body\nimpl Display for ArithExpr { ... } // Arithmetic expressions\nimpl Display for AggregateFunc { ... }  // count, sum, top_k<...>, etc.\nimpl Display for ComparisonOp { ... }   // =, !=, <, <=, >, >=\n```\n\n**Usage**: All components use `term.to_string()` instead of duplicate formatting functions.\n\n---\n\n## 7. Execution Model\n\n### 7.1 Batch Execution (CodeGenerator)\n\n```rust\npub struct CodeGenerator {\n    input_data: HashMap<String, Vec<Tuple>>,\n}\n\npub struct ExecutionConfig {\n    num_workers: usize,\n}\n```\n\n**Execution Paths**:\n1. **Non-recursive**: Single `execute_directly` call\n2. **Transitive closure**: DD iterative scope with `SemigroupVariable`\n3. **General recursive**: DD `.iterative()` scope with live collections\n\n**Parallelization**:\n- Rayon-based parallel execution for scan/filter/map\n- Join queries use single-worker DD for correctness\n- Configurable via `num_workers`\n\n### 7.2 Incremental Execution (DDComputation)\n\n```rust\nimpl DDComputation {\n    // Returns Result - no more panicking on initialization errors\n    pub fn new(relations: Vec<String>) -> Result<Self, String>;\n\n    pub fn insert(&self, relation: &str, tuples: Vec<Tuple>, time: u64) -> Result<()>;\n    pub fn delete(&self, relation: &str, tuples: Vec<Tuple>, time: u64) -> Result<()>;\n    pub fn read_relation_consistent(&self, relation: &str) -> Result<Vec<Tuple>>;\n}\n```\n\n**Features**:\n- **Fire-and-forget inserts**: Shadow writes don't block query execution\n- **Lazy time advancement**: Writes buffer, time only advances on consistent reads\n- **Dynamic relations**: InputSessions created at runtime\n\n---\n\n## 8. Persistence Layer\n\n### 8.1 WAL (Write-Ahead Log)\n\n```rust\npub struct Update {\n    pub data: Tuple,\n    pub time: u64,\n    pub diff: i64,  // +1 for insert, -1 for delete\n}\n\npub struct WalEntry {\n    pub shard: String,\n    pub update: Update,\n}\n```\n\n### 8.2 Batch Files (Parquet)\n\n```rust\npub struct ShardMeta {\n    pub shard: String,\n    pub batches: Vec<BatchMeta>,\n    pub since: u64,   // Compaction frontier\n    pub upper: u64,   // Write frontier\n}\n```\n\n### 8.3 Recovery Sequence\n\n```\n1. Load shard metadata from disk\n2. Create DDComputation for each KG\n3. Replay batch files through InputSessions\n4. Replay WAL entries since last batch\n5. Step workers until frontier advances past WAL upper\n6. DDComputation is live and ready\n```\n\n---\n\n## 9. Protocol Layer\n\n### 9.1 Handler\n\n```rust\npub struct Handler {\n    storage: Arc<RwLock<StorageEngine>>,\n    start_time: Instant,\n    query_count: AtomicU64,\n    insert_count: AtomicU64,\n}\n```\n\n**Key Features**:\n- **Read lock** for all query operations (concurrent queries)\n- **Explicit KG naming** via `_on()`, `_into()`, `_from()` variants\n- **Session fact isolation** via `execute_query_with_session_facts_on()`\n- **Per-KG schema validation** delegated to StorageEngine (schemas isolated per KG)\n\n### 9.2 Statement Types\n\n```rust\npub enum Statement {\n    SchemaDecl(SchemaDecl),\n    PersistentRule(Rule),      // +name(...) <- body\n    SessionRule(Rule),          // name(...) <- body\n    PersistentFact(Fact),      // +relation(values)\n    SessionFact(Fact),          // relation(values)\n    Query(Query),               // ?body\n    ConditionalDelete(Rule),   // -relation(X) <- condition\n    DotCommand(DotCommand),    // .kg, .rule, .rel, etc.\n}\n```\n\n---\n\n## 10. HNSW Indexing Readiness\n\n### 10.1 Prerequisites (All Complete)\n\n| Prerequisite | Status | Description |\n|-------------|--------|-------------|\n| Abomonation fix | Done | Proper exhume() for multi-worker DD |\n| Generic timestamps | Done | CodeGenerator uses `G::Timestamp: Lattice + Ord + Default` |\n| DDComputation | Done | Persistent incremental computation |\n| Insert path wiring | Done | Shadow writes to DDComputation |\n| Consistent reads | Done | Lazy time advancement + frontier tracking |\n| Rule materialization | Done | DerivedRelationsManager with validity tracking |\n| Auto-materialization | Done | Persistent rules auto-materialize |\n| Session isolation | Done | Cloned snapshots for session facts |\n\n### 10.2 HNSW Integration Points\n\n```\nInsert -> Persist WAL -> In-Memory State -> DDComputation\n                                              |\n                                              v\n                                        Arrangement\n                                              |\n                                              v\n                                    [HNSW INDEX SINK]\n                                              |\n                                    On insert: hnsw.insert(embedding, id)\n                                    On delete: tombstone.add(id)\n```\n\n### 10.3 Required Extensions for HNSW\n\n1. **IndexManager** in KnowledgeGraph\n2. **IRNode::IndexScan** in IR\n3. **`.index create/drop/rebuild`** commands\n4. **Query planning** for distance predicates\n\n---\n\n## 11. Configuration\n\n### 11.1 Performance Configuration\n\n```toml\n[storage.performance]\nnum_threads = 4       # Workers for parallel execution\nbatch_size = 1000     # Batch size for operations\n```\n\n### 11.2 Storage Configuration\n\n```toml\n[storage]\ndata_dir = \"./data\"\nauto_create_kg = true\ndefault_kg = \"default\"\n```\n\n### 11.3 Environment Variables\n\nAll environment variables use the `IL_` prefix:\n\n| Variable | Purpose |\n|----------|---------|\n| `IL_DEBUG` | Enable debug output for IR building, execution, and optimization |\n| `IL_DEBUG_SESSION` | Enable debug output for session fact handling |\n\n---\n\n## 12. Query Optimization\n\nInputLayer includes query optimization infrastructure for improving join performance.\n\n### 12.1 Optimization Modules\n\n| Module | File | Purpose |\n|--------|------|---------|\n| **Bloom Filter** | `src/bloom_filter.rs` | Probabilistic set membership testing |\n| **Hash Index** | `src/hash_index.rs` | O(1) join key lookups with Bloom acceleration |\n| **Statistics** | `src/statistics.rs` | Cardinality and selectivity estimation |\n| **Join Planning** | `src/join_planning/mod.rs` | Join ordering via MST algorithm |\n| **SIP Rewriting** | `src/sip_rewriting/mod.rs` | Sideways Information Passing |\n| **Subplan Sharing** | `src/subplan_sharing/mod.rs` | Common subexpression elimination |\n\n### 12.2 Bloom Filter\n\nSpace-efficient probabilistic data structure for fast set membership testing.\n\n```rust\nuse inputlayer::bloom_filter::{BloomFilter, BloomFilterBuilder};\n\n// Create filter for 10K elements with 1% false positive rate\nlet mut filter = BloomFilter::new(10_000, 0.01);\nfilter.insert(&\"key1\");\nfilter.insert(&\"key2\");\n\n// Check membership (no false negatives, possible false positives)\nif filter.might_contain(&\"key1\") {\n    // Definitely present or false positive\n}\n```\n\n**Properties:**\n- No false negatives: if `might_contain()` returns `false`, the element is definitely not present\n- Possible false positives: if `might_contain()` returns `true`, element might or might not be present\n- Space efficient: ~10 bits per element for 1% false positive rate\n\n### 12.3 Hash Index\n\nAccelerates join key lookups with Bloom filter pre-filtering.\n\n```rust\nuse inputlayer::hash_index::{HashIndex, HashIndexManager, JoinKeySpec};\n\n// Create index for edge(src, dst) on column 0\nlet spec = JoinKeySpec::new(\"edge\", vec![0]);\nlet mut index = HashIndex::new(spec);\n\n// Build from tuples\nindex.build_from_tuples(&edge_tuples);\n\n// Lookup with Bloom filter check first\nif let Some(matching) = index.lookup(&key) {\n    for tuple in matching {\n        // Process matching tuples\n    }\n}\n```\n\n### 12.4 Statistics Manager\n\nCollects and maintains relation statistics for query optimization.\n\n```rust\nuse inputlayer::statistics::{StatisticsManager, RelationStats};\n\nlet mut stats_manager = StatisticsManager::new();\n\n// Update statistics for a relation\nstats_manager.update_stats(\"edge\", &edge_tuples);\n\n// Get cardinality estimate\nif let Some(stats) = stats_manager.get_stats(\"edge\") {\n    println!(\"Cardinality: {}\", stats.cardinality);\n    println!(\"Distinct values in col 0: {}\", stats.column_stats[0].distinct_count);\n}\n\n// Estimate join selectivity\nlet selectivity = stats_manager.estimate_join_selectivity(\"edge\", 0, \"node\", 0);\n```\n\n### 12.5 BloomSemijoin IR Node\n\nThe IR includes a `BloomSemijoin` node for semijoin reduction:\n\n```rust\nIRNode::BloomSemijoin {\n    input: Box<IRNode>,           // Relation to filter\n    filter_source: Box<IRNode>,   // Relation providing filter keys\n    input_key_columns: Vec<usize>,\n    filter_key_columns: Vec<usize>,\n    output_schema: Vec<String>,\n}\n```\n\nThis node filters `input` to keep only tuples whose key columns exist in `filter_source`, implemented via Differential Dataflow's `semijoin()` operator.\n\n---\n\n## 13. File Reference\n\n| File | Purpose |\n|------|---------|\n| `src/lib.rs` | DatalogEngine, public API |\n| `src/ast/mod.rs` | AST types with Display implementations |\n| `src/value/mod.rs` | Value, Tuple, Abomonation |\n| `src/storage_engine/mod.rs` | StorageEngine, KnowledgeGraph |\n| `src/storage_engine/snapshot.rs` | KnowledgeGraphSnapshot |\n| `src/incremental.rs` | IncrementalEngine |\n| `src/derived_relations.rs` | DerivedRelationsManager |\n| `src/rule_catalog.rs` | RuleCatalog, validation |\n| `src/schema/catalog.rs` | SchemaCatalog (per-KG) |\n| `src/code_generator/mod.rs` | CodeGenerator, execution |\n| `src/protocol/handler.rs` | Request handling |\n| `src/storage/persist/mod.rs` | FilePersist, WAL, batches |\n| `src/bloom_filter.rs` | Bloom filter implementation |\n| `src/hash_index.rs` | Hash index with Bloom acceleration |\n| `src/statistics.rs` | Statistics collection and estimation |\n| `src/ir/mod.rs` | Intermediate Representation nodes |\n| `src/join_planning/mod.rs` | Join order optimization |\n| `src/sip_rewriting/mod.rs` | Sideways Information Passing |\n| `src/subplan_sharing/mod.rs` | Common subexpression elimination |\n\n---\n\n## 14. Architectural Patterns\n\n### 14.1 Arc-Wrapped Data for Sharing\n\nAll persistent data wrapped in Arc for O(1) cloning:\n```rust\ninput_tuples: Arc<HashMap<String, Vec<Tuple>>>\nrules: Arc<Vec<Rule>>\nmaterialized_relations: Arc<HashSet<String>>\n```\n\n### 14.2 ArcSwap for Atomic Publishing\n\n```rust\nself.snapshot.store(Arc::new(new_snapshot));  // O(1) atomic swap\n```\n\n### 14.3 Fire-and-Forget Shadow Writes\n\n```rust\ndd.insert(relation, tuples, time)?;  // Returns immediately\n// DD processes asynchronously\n```\n\n### 14.4 Lazy Time Advancement\n\n```rust\nmax_write_time.store(time, Ordering::Relaxed);  // Track writes\n// Only advance on read: max_write_time + 1\n```\n\n### 14.5 Atomic Cascade Invalidation\n\n```rust\n// Compute invalidation set (read-only)\nlet to_invalidate = self.compute_invalidation_set(base_relation);\n\n// Bump version\n*version += 1;\n\n// Apply all invalidations atomically\nfor rel in &to_invalidate {\n    mat.invalidate();\n}\n```\n\n### 14.6 Session Isolation via Cloning\n\n```rust\nlet mut isolated = (*snapshot.input_tuples).clone();\nisolated.insert(session_fact);\n// Execute against isolated copy\n```\n\n---\n\n## 15. Summary\n\nInputLayer's architecture provides:\n\n| Capability | Implementation |\n|-----------|----------------|\n| **Incremental computation** | DDComputation with persistent arrangements |\n| **Lock-free reads** | ArcSwap snapshot system |\n| **Concurrent writes** | DashMap + per-KG locking |\n| **Rule materialization** | DerivedRelationsManager with auto-rematerialization |\n| **Schema isolation** | Per-KG SchemaCatalog |\n| **Session isolation** | Cloned snapshots for ephemeral data |\n| **Parallel execution** | Rayon-based multi-worker batch queries |\n| **Durability** | WAL + Parquet batch files |\n| **Consistency** | Frontier tracking, atomic cascade invalidation |\n\n**Test Coverage**: ~3107 unit tests + ~1121 snapshot tests = ~4228 total, all passing.\n\n**HNSW Readiness**: All prerequisites complete. Indexes can attach as arrangement sinks.",
    "toc": [
      {
        "level": 2,
        "text": "Overview",
        "id": "overview"
      },
      {
        "level": 2,
        "text": "1. Architecture Overview",
        "id": "1-architecture-overview"
      },
      {
        "level": 2,
        "text": "2. Core Components",
        "id": "2-core-components"
      },
      {
        "level": 3,
        "text": "2.1 StorageEngine",
        "id": "21-storageengine"
      },
      {
        "level": 3,
        "text": "2.2 KnowledgeGraph",
        "id": "22-knowledgegraph"
      },
      {
        "level": 3,
        "text": "2.3 KnowledgeGraphSnapshot",
        "id": "23-knowledgegraphsnapshot"
      },
      {
        "level": 3,
        "text": "2.4 DDComputation",
        "id": "24-ddcomputation"
      },
      {
        "level": 3,
        "text": "2.5 DerivedRelationsManager",
        "id": "25-derivedrelationsmanager"
      },
      {
        "level": 2,
        "text": "3. Data Flow Paths",
        "id": "3-data-flow-paths"
      },
      {
        "level": 3,
        "text": "3.1 Write Path",
        "id": "31-write-path"
      },
      {
        "level": 3,
        "text": "3.2 Read Path (Query)",
        "id": "32-read-path-query"
      },
      {
        "level": 3,
        "text": "3.3 Materialization Flow",
        "id": "33-materialization-flow"
      },
      {
        "level": 2,
        "text": "4. Consistency Guarantees",
        "id": "4-consistency-guarantees"
      },
      {
        "level": 3,
        "text": "4.1 Snapshot Consistency",
        "id": "41-snapshot-consistency"
      },
      {
        "level": 3,
        "text": "4.2 Write Consistency",
        "id": "42-write-consistency"
      },
      {
        "level": 3,
        "text": "4.3 Read-After-Write Consistency",
        "id": "43-read-after-write-consistency"
      },
      {
        "level": 3,
        "text": "4.4 Session Isolation",
        "id": "44-session-isolation"
      },
      {
        "level": 2,
        "text": "5. Rule System",
        "id": "5-rule-system"
      },
      {
        "level": 3,
        "text": "5.1 Persistent Rules (+ prefix)",
        "id": "51-persistent-rules-prefix"
      },
      {
        "level": 3,
        "text": "5.2 Session Rules (no prefix)",
        "id": "52-session-rules-no-prefix"
      },
      {
        "level": 3,
        "text": "5.3 Rule Validation",
        "id": "53-rule-validation"
      },
      {
        "level": 2,
        "text": "6. Data Types",
        "id": "6-data-types"
      },
      {
        "level": 3,
        "text": "6.1 Unified Tuple Format",
        "id": "61-unified-tuple-format"
      },
      {
        "level": 3,
        "text": "6.2 Abomonation Implementation",
        "id": "62-abomonation-implementation"
      },
      {
        "level": 3,
        "text": "6.3 AST Display Implementations",
        "id": "63-ast-display-implementations"
      },
      {
        "level": 2,
        "text": "7. Execution Model",
        "id": "7-execution-model"
      },
      {
        "level": 3,
        "text": "7.1 Batch Execution (CodeGenerator)",
        "id": "71-batch-execution-codegenerator"
      },
      {
        "level": 3,
        "text": "7.2 Incremental Execution (DDComputation)",
        "id": "72-incremental-execution-ddcomputation"
      },
      {
        "level": 2,
        "text": "8. Persistence Layer",
        "id": "8-persistence-layer"
      },
      {
        "level": 3,
        "text": "8.1 WAL (Write-Ahead Log)",
        "id": "81-wal-write-ahead-log"
      },
      {
        "level": 3,
        "text": "8.2 Batch Files (Parquet)",
        "id": "82-batch-files-parquet"
      },
      {
        "level": 3,
        "text": "8.3 Recovery Sequence",
        "id": "83-recovery-sequence"
      },
      {
        "level": 2,
        "text": "9. Protocol Layer",
        "id": "9-protocol-layer"
      },
      {
        "level": 3,
        "text": "9.1 Handler",
        "id": "91-handler"
      },
      {
        "level": 3,
        "text": "9.2 Statement Types",
        "id": "92-statement-types"
      },
      {
        "level": 2,
        "text": "10. HNSW Indexing Readiness",
        "id": "10-hnsw-indexing-readiness"
      },
      {
        "level": 3,
        "text": "10.1 Prerequisites (All Complete)",
        "id": "101-prerequisites-all-complete"
      },
      {
        "level": 3,
        "text": "10.2 HNSW Integration Points",
        "id": "102-hnsw-integration-points"
      },
      {
        "level": 3,
        "text": "10.3 Required Extensions for HNSW",
        "id": "103-required-extensions-for-hnsw"
      },
      {
        "level": 2,
        "text": "11. Configuration",
        "id": "11-configuration"
      },
      {
        "level": 3,
        "text": "11.1 Performance Configuration",
        "id": "111-performance-configuration"
      },
      {
        "level": 3,
        "text": "11.2 Storage Configuration",
        "id": "112-storage-configuration"
      },
      {
        "level": 3,
        "text": "11.3 Environment Variables",
        "id": "113-environment-variables"
      },
      {
        "level": 2,
        "text": "12. Query Optimization",
        "id": "12-query-optimization"
      },
      {
        "level": 3,
        "text": "12.1 Optimization Modules",
        "id": "121-optimization-modules"
      },
      {
        "level": 3,
        "text": "12.2 Bloom Filter",
        "id": "122-bloom-filter"
      },
      {
        "level": 3,
        "text": "12.3 Hash Index",
        "id": "123-hash-index"
      },
      {
        "level": 3,
        "text": "12.4 Statistics Manager",
        "id": "124-statistics-manager"
      },
      {
        "level": 3,
        "text": "12.5 BloomSemijoin IR Node",
        "id": "125-bloomsemijoin-ir-node"
      },
      {
        "level": 2,
        "text": "13. File Reference",
        "id": "13-file-reference"
      },
      {
        "level": 2,
        "text": "14. Architectural Patterns",
        "id": "14-architectural-patterns"
      },
      {
        "level": 3,
        "text": "14.1 Arc-Wrapped Data for Sharing",
        "id": "141-arc-wrapped-data-for-sharing"
      },
      {
        "level": 3,
        "text": "14.2 ArcSwap for Atomic Publishing",
        "id": "142-arcswap-for-atomic-publishing"
      },
      {
        "level": 3,
        "text": "14.3 Fire-and-Forget Shadow Writes",
        "id": "143-fire-and-forget-shadow-writes"
      },
      {
        "level": 3,
        "text": "14.4 Lazy Time Advancement",
        "id": "144-lazy-time-advancement"
      },
      {
        "level": 3,
        "text": "14.5 Atomic Cascade Invalidation",
        "id": "145-atomic-cascade-invalidation"
      },
      {
        "level": 3,
        "text": "14.6 Session Isolation via Cloning",
        "id": "146-session-isolation-via-cloning"
      },
      {
        "level": 2,
        "text": "15. Summary",
        "id": "15-summary"
      }
    ]
  },
  "internals/coding-standards": {
    "title": "InputLayer Coding Standards & Principles",
    "content": "# InputLayer Coding Standards & Principles\n\n> **Project Status**: Production-Ready\n>\n> The architecture is complete and tested (~3,107 unit tests + ~1,121 snapshot tests = ~4,228 total). This means:\n> - APIs are stable but may evolve with clear migration paths\n> - Breaking changes require justification and documentation\n> - Deprecated code should be removed promptly after migration period\n\n---\n\n## Core Principles\n\n### 1. Single Source of Truth\n\n**Every concept should have exactly one canonical definition.**\n\n```rust\n// BAD: Same concept defined multiple times\n// ast/mod.rs\npub enum AggregateFunc { Count, Sum, ... }\n\n// ir/mod.rs\npub enum AggregateFunction { Count, Sum, ... }  // Different name!\n\n// statement.rs\npub enum SerializableAggregateFunc { Count, Sum, ... }  // Third copy!\n```\n\n```rust\n// GOOD: Single definition, re-exported where needed\n// types/aggregates.rs\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum Aggregate { Count, Sum, ... }\n\n// ast/mod.rs\npub use crate::types::Aggregate;\n\n// ir/mod.rs\npub use crate::types::Aggregate;\n```\n\n**Why**: Prevents drift between definitions, eliminates conversion boilerplate, makes adding new variants trivial.\n\n---\n\n### 2. No Legacy Compatibility Code\n\n**Delete deprecated code immediately. We have no users to break.**\n\n```rust\n// BAD: Keeping old code \"just in case\" (Tuple2 was REMOVED - this is the old pattern)\npub type OldFormat = (i32, i32);   // Legacy - DON'T DO THIS\npub struct Tuple(Vec<Value>);      // New\n\nfn execute_old(...) -> Vec<OldFormat> { ... }  // Keep for compatibility?\nfn execute_new(...) -> Vec<Tuple> { ... }      // The real one\n```\n\n```rust\n// GOOD: One way to do things (Tuple is now the ONLY data format)\npub struct Tuple(Vec<Value>);\n\nfn execute(...) -> Vec<Tuple> { ... }\n```\n\n**Why**: Duplicate code paths double maintenance burden, confuse new developers, and create subtle bugs when they diverge.\n\n---\n\n### 3. Types Flow Through the Pipeline\n\n**Type information should be available at every stage, not just parsing.**\n\n```rust\n// BAD: Losing type info\nstruct IRNode {\n    Scan { relation: String, schema: Vec<String> }  // Only names, no types!\n}\n\n// Later in code generator:\nfn generate_filter(...) {\n    // Can't validate types here because we don't have them\n}\n```\n\n```rust\n// GOOD: Types preserved\nstruct IRNode {\n    Scan { relation: String, schema: TupleSchema }  // Full type info!\n}\n\nfn generate_filter(schema: &TupleSchema, predicate: &Predicate) {\n    let col_type = schema.field_type(predicate.column());\n    assert!(col_type.is_comparable(), \"Invalid comparison on {:?}\", col_type);\n}\n```\n\n**Why**: Enables better error messages, catches bugs earlier, allows type-aware optimizations.\n\n---\n\n### 4. Small, Focused Modules\n\n**Each file should have a single, clear purpose. If you can't describe it in one sentence, split it.**\n\n```rust\n// BAD: Monolithic file\n// statement.rs (2,860 lines)\n// - Meta command parsing\n// - Schema parsing\n// - Data operation parsing\n// - Serialization types\n// - Conversion functions\n// - Query parsing\n```\n\n```rust\n// GOOD: Focused modules\n// statement/\n//   mod.rs          - Re-exports and Statement enum\n//   meta.rs         - Meta command parsing (.kg, .rule, .load)\n//   schema.rs       - Schema declaration parsing\n//   data.rs         - Insert/Delete/Update operations\n//   serialize.rs    - JSON serialization types\n//   parser.rs       - Core parsing utilities\n```\n\n**Guidelines**:\n- Maximum 500 lines per file (prefer 200-300)\n- One `pub struct` or `pub enum` as the primary export\n- Helper functions should be private unless needed elsewhere\n\n---\n\n### 5. Explicit Over Implicit\n\n**Be explicit about behavior. Avoid magic and hidden side effects.**\n\n```rust\n// BAD: Hidden behavior\nfn add_fact(&mut self, relation: &str, data: Vec<(i32, i32)>) {\n    // Secretly infers schema if not exists\n    // Secretly converts to internal format\n    // Secretly persists to disk\n}\n```\n\n```rust\n// GOOD: Explicit operations\nfn ensure_schema(&mut self, relation: &str, schema: TupleSchema) -> Result<()>\nfn insert_tuples(&mut self, relation: &str, tuples: Vec<Tuple>) -> Result<()>\nfn persist(&mut self) -> Result<()>\n\n// Usage is clear\nengine.ensure_schema(\"edge\", schema![(\"from\", Int64), (\"to\", Int64)])?;\nengine.insert_tuples(\"edge\", tuples)?;\nengine.persist()?;\n```\n\n---\n\n### 6. Error Messages Are User Interface\n\n**Errors should help users fix problems, not just report them.**\n\n```rust\n// BAD: Unhelpful error\nErr(\"parse error\".to_string())\n```\n\n```rust\n// GOOD: Actionable error\nErr(format!(\n    \"Unknown relation '{}' at line {}. Did you mean '{}'? \\\n     Available relations: {}\",\n    name, line, suggestion, available.join(\", \")\n))\n```\n\n**Error message checklist**:\n- [ ] What went wrong?\n- [ ] Where did it happen? (file, line, column if applicable)\n- [ ] Why is it wrong?\n- [ ] How can the user fix it?\n\n---\n\n### 7. Composition Over Inheritance\n\n**Build complex behavior by combining simple pieces.**\n\n```rust\n// BAD: Deep trait hierarchies\ntrait Queryable { ... }\ntrait Filterable: Queryable { ... }\ntrait Joinable: Filterable { ... }\ntrait Aggregatable: Joinable { ... }\n```\n\n```rust\n// GOOD: Composable operations\nstruct QueryPipeline {\n    stages: Vec<Stage>,\n}\n\nenum Stage {\n    Scan(ScanOp),\n    Filter(FilterOp),\n    Join(JoinOp),\n    Aggregate(AggregateOp),\n}\n\nimpl QueryPipeline {\n    fn add_filter(mut self, predicate: Predicate) -> Self {\n        self.stages.push(Stage::Filter(FilterOp { predicate }));\n        self\n    }\n}\n```\n\n---\n\n## Code Style Guidelines\n\n### Naming Conventions\n\n| Item | Convention | Example |\n|------|------------|---------|\n| Types | PascalCase | `TupleSchema`, `IRNode` |\n| Functions | snake_case | `parse_statement`, `build_ir` |\n| Constants | SCREAMING_SNAKE | `MAX_RECURSION_DEPTH` |\n| Modules | snake_case | `code_generator`, `ir_builder` |\n| Type parameters | Single uppercase | `T`, `E`, `F` |\n\n**Naming principles**:\n- Be descriptive: `parse_meta_command` not `pmc`\n- Be consistent: if one function is `parse_X`, all similar functions are `parse_Y`\n- Avoid abbreviations unless universal: `id` ok, `idx` ok, `rel` not ok (use `relation`)\n\n### Function Size\n\n**Functions should fit on one screen (~50 lines max).**\n\nIf a function is longer:\n1. Extract helper functions\n2. Use early returns to reduce nesting\n3. Consider if it's doing too many things\n\n```rust\n// BAD: Too long, deeply nested\nfn execute(&mut self, program: &str) -> Result<Vec<Tuple>> {\n    // 200 lines of code with 5 levels of nesting\n}\n\n// GOOD: Composed of helpers\nfn execute(&mut self, program: &str) -> Result<Vec<Tuple>> {\n    let compiled = self.compile(program)?;\n    let context = self.prepare_context()?;\n    self.execute_rules(&compiled, &context)\n}\n```\n\n### Comments\n\n**Code should be self-documenting. Use comments for \"why\", not \"what\".**\n\n```rust\n// BAD: Restating the code\n// Increment counter by 1\ncounter += 1;\n\n// BAD: Explaining obvious code\n// Check if the relation exists in the schema\nif schema.contains(relation) { ... }\n```\n\n```rust\n// GOOD: Explaining non-obvious decisions\n// We use a BTreeMap instead of HashMap here because we need deterministic\n// iteration order for reproducible query results across runs.\nlet relations: BTreeMap<String, Schema> = ...;\n\n// GOOD: Explaining edge cases\n// Empty body means this is a fact, not a rule. Facts are handled specially\n// because they don't need fixpoint iteration.\nif rule.body.is_empty() {\n    return self.handle_fact(rule);\n}\n```\n\n### Documentation\n\n**All public items must have doc comments.**\n\n```rust\n/// Parse a Datalog statement from source text.\n///\n/// # Arguments\n///\n/// * `input` - The source text to parse\n///\n/// # Returns\n///\n/// The parsed statement, or an error if parsing fails.\n///\n/// # Examples\n///\n/// ```\n/// let stmt = parse_statement(\"+edge(1, 2).\")?;\n/// assert!(matches!(stmt, Statement::Insert(_)));\n/// ```\npub fn parse_statement(input: &str) -> Result<Statement, ParseError> {\n    // ...\n}\n```\n\n### Error Handling\n\n**Use `Result` and `?` operator. Avoid `.unwrap()` except in tests.**\n\n```rust\n// BAD: Panics on error\nlet schema = catalog.get(relation).unwrap();\n\n// BAD: Ignores error\nlet _ = persist_to_disk();\n\n// GOOD: Propagates error with context\nlet schema = catalog.get(relation)\n    .ok_or_else(|| SchemaError::UnknownRelation(relation.to_string()))?;\n\n// GOOD: Handles error explicitly\nmatch persist_to_disk() {\n    Ok(()) => log::info!(\"Persisted successfully\"),\n    Err(e) => log::warn!(\"Failed to persist: {}\", e),\n}\n```\n\n---\n\n## Module Organization\n\n### Directory Structure\n\n```\nsrc/\n+-- lib.rs                    # DatalogEngine, public API\n+-- main.rs                   # Server binary\n+-- config.rs                 # TOML configuration\n+-- catalog.rs                # System catalog\n+-- ast/                      # Abstract Syntax Tree\n|   +-- mod.rs                # AST types, BuiltinFunc, AggregateFunc, Display\n+-- ir/                       # Intermediate Representation\n|   +-- mod.rs                # IRNode, IRExpression, BuiltinFunction\n+-- ir_builder/               # AST to IR conversion\n|   +-- mod.rs\n+-- parser/                   # Datalog parser\n|   +-- mod.rs                # Lexer + recursive descent parser\n+-- statement/                # Statement parsing\n|   +-- mod.rs                # Statement enum\n|   +-- parser.rs             # Statement-level parsing\n|   +-- meta.rs               # Meta command parsing (.kg, .rule, .index)\n|   +-- schema.rs             # Schema declaration parsing\n|   +-- data.rs               # Insert/Delete operations\n|   +-- serialize.rs          # JSON serialization types\n|   +-- types.rs              # Statement-related types\n+-- optimizer/                # Query optimization passes\n|   +-- mod.rs\n+-- join_planning/            # Join order optimization\n|   +-- mod.rs\n+-- sip_rewriting/            # Sideways Information Passing\n|   +-- mod.rs\n+-- subplan_sharing/          # Common subexpression detection\n|   +-- mod.rs\n+-- boolean_specialization/   # Set/bag semantics analysis\n|   +-- mod.rs\n+-- code_generator/           # IR to DD execution\n|   +-- mod.rs\n+-- storage_engine/           # Multi-KG storage\n|   +-- mod.rs                # StorageEngine, KnowledgeGraph\n|   +-- snapshot.rs           # KnowledgeGraphSnapshot\n+-- storage/                  # Persistence layer\n|   +-- ...                   # FilePersist, WAL, Parquet batches\n+-- schema/                   # Schema management\n|   +-- mod.rs\n|   +-- catalog.rs            # SchemaCatalog (per-KG)\n|   +-- validator.rs\n+-- value/                    # Value types and tuples\n|   +-- mod.rs                # Value, Tuple, Abomonation\n|   +-- arrow_convert.rs      # Arrow/Parquet conversion\n+-- protocol/                 # HTTP/API handler\n|   +-- handler.rs\n+-- dd_computation.rs         # DDComputation, DDCommand\n+-- derived_relations.rs      # DerivedRelationsManager\n+-- rule_catalog.rs           # RuleCatalog, validation\n+-- recursion.rs              # Recursion detection/handling\n+-- vector_ops.rs             # Vector, quantization, LSH functions\n+-- temporal_ops.rs           # Temporal functions\n+-- hnsw_index.rs             # HNSW index implementation\n+-- index_manager.rs          # Per-KG index management\n+-- pipeline_trace.rs         # Pipeline tracing/debugging\n```\n\n### Module Dependencies\n\n**Dependencies should flow downward. Lower modules should not depend on higher ones.**\n\n```\n          lib.rs (public API)\n              |\n    +---------+---------+\n    |         |         |\n    v         v         v\nstatement  optimizer  code_generator\n    |         |         |\n    +----> ir/ast <-----+\n              |\n           types/\n              |\n           value/\n```\n\n**Forbidden**:\n- `types/` depending on `ast/`\n- `value/` depending on `ir/`\n- Circular dependencies of any kind\n\n---\n\n## Testing Standards\n\n### Test Organization\n\n```\ntests/\n+-- integration/        # Full pipeline tests\n+-- snapshots/          # Golden file tests\n+-- unit/               # Module-specific tests (or inline in src/)\n```\n\n### Test Naming\n\n```rust\n#[test]\nfn test_<module>_<scenario>_<expected_outcome>() {\n    // ...\n}\n\n// Examples:\nfn test_parser_empty_input_returns_error() { ... }\nfn test_join_two_relations_produces_cartesian_product() { ... }\nfn test_aggregation_empty_group_returns_zero() { ... }\n```\n\n### Test Coverage Requirements\n\n- All public functions must have at least one test\n- All error paths should be tested\n- Edge cases: empty input, single element, max values\n\n### Snapshot Tests\n\n**Use for complex output that's hard to assert programmatically.**\n\n```\nexamples/datalog/\n+-- 01_basics/\n|   +-- 01_simple_query.idl      # Input\n|   +-- 01_simple_query.idl.out  # Expected output\n```\n\nUpdate snapshots with:\n```bash\n./scripts/run_snapshot_tests.sh --update\n```\n\n---\n\n## Performance Guidelines\n\n### Avoid Unnecessary Allocations\n\n```rust\n// BAD: Creates intermediate String\nlet name = format!(\"{}\", relation);\nif catalog.contains(&name) { ... }\n\n// GOOD: Works with &str directly\nif catalog.contains(relation) { ... }\n```\n\n### Use Iterators Instead of Collecting\n\n```rust\n// BAD: Allocates intermediate Vec\nlet filtered: Vec<_> = items.iter().filter(|x| x.valid).collect();\nlet result: Vec<_> = filtered.iter().map(|x| x.value).collect();\n\n// GOOD: Lazy evaluation\nlet result: Vec<_> = items.iter()\n    .filter(|x| x.valid)\n    .map(|x| x.value)\n    .collect();\n```\n\n### Clone Consciously\n\n```rust\n// BAD: Cloning when not needed\nfn process(data: Vec<Tuple>) {\n    let copy = data.clone();  // Why?\n    // ...\n}\n\n// GOOD: Take ownership or borrow\nfn process(data: Vec<Tuple>) { ... }  // Takes ownership\nfn process(data: &[Tuple]) { ... }    // Borrows\n```\n\n---\n\n## Git Workflow\n\n### Commit Messages\n\n```\n<type>: <short description>\n\n<optional longer description>\n\n<optional footer>\n```\n\n**Types**:\n- `feat`: New feature\n- `fix`: Bug fix\n- `refactor`: Code change that neither fixes a bug nor adds a feature\n- `docs`: Documentation only\n- `test`: Adding or updating tests\n- `chore`: Build, CI, dependencies\n\n**Examples**:\n```\nfeat: add TopK aggregate function\n\nImplements top-k selection with ordering column support\nSyntax: top_k<10, name, score:desc>\n\nrefactor: split statement.rs into focused modules\n\nImproves maintainability by separating concerns:\n- meta.rs: .kg, .rule, .load commands\n- schema.rs: type declarations\n- data.rs: insert/delete/update\n```\n\n### Branch Naming\n\n```\n<type>/<short-description>\n\nfeat/topk-aggregate\nfix/parser-empty-input\nrefactor/split-statement-module\n```\n\n### Pull Request Checklist\n\nBefore merging:\n- [ ] All tests pass (`cargo test`)\n- [ ] Snapshot tests pass (`./scripts/run_snapshot_tests.sh`)\n- [ ] No clippy warnings (`cargo clippy -- -D warnings`)\n- [ ] Code is formatted (`cargo fmt`)\n- [ ] Public APIs are documented\n- [ ] Complex logic has comments explaining \"why\"\n\n---\n\n## Anti-Patterns to Avoid\n\n### 1. The God Module\n\n**Don't**: Put everything in one file because \"it's easier\".\n**Do**: Split into focused modules from the start.\n\n### 2. Stringly Typed APIs\n\n**Don't**:\n```rust\nfn execute(&mut self, operation: &str, target: &str, data: &str) -> String\n```\n\n**Do**:\n```rust\nfn execute(&mut self, op: Operation) -> Result<ExecutionResult>\n```\n\n### 3. Boolean Parameters\n\n**Don't**:\n```rust\nfn parse(input: &str, strict: bool, allow_empty: bool, normalize: bool)\n```\n\n**Do**:\n```rust\nstruct ParseOptions {\n    strict: bool,\n    allow_empty: bool,\n    normalize: bool,\n}\n\nfn parse(input: &str, options: ParseOptions)\n```\n\n### 4. Defensive Cloning\n\n**Don't**: Clone \"just to be safe\" without understanding ownership.\n**Do**: Understand borrowing and clone only when necessary.\n\n### 5. Comment-Driven Development\n\n**Don't**:\n```rust\n// TODO: fix this later\n// HACK: this works but is ugly\n// XXX: not sure why this is needed\n```\n\n**Do**: Fix it now or create a tracked issue.\n\n---\n\n## Checklist for New Code\n\nBefore submitting code, verify:\n\n- [ ] Single responsibility: each function/module does one thing\n- [ ] No duplicate type definitions\n- [ ] Types flow through the pipeline (not lost after parsing)\n- [ ] Errors are descriptive and actionable\n- [ ] Public items are documented\n- [ ] Tests cover happy path and error cases\n- [ ] No `.unwrap()` outside of tests\n- [ ] No unnecessary clones\n- [ ] Code is formatted (`cargo fmt`)\n- [ ] Clippy is happy (`cargo clippy`)\n\n---\n\n## Quick Reference\n\n### Adding a New Aggregate Function\n\n1. Add variant to `AggregateFunc` enum in `src/ast/mod.rs`\n2. Add IR variant to `AggregateFunction` in `src/ir/mod.rs`\n3. Add conversion in `src/ir_builder/mod.rs`\n4. Add code generation in `src/code_generator/mod.rs`\n5. Add tests in appropriate test file\n6. Add snapshot test in `examples/datalog/`\n\n### Adding a New Built-in Function\n\n1. Add variant to `BuiltinFunc` enum in `src/ast/mod.rs`\n2. Implement `parse()`, `arity()`, `as_str()` in `src/ast/mod.rs`\n3. Add IR variant to `BuiltinFunction` in `src/ir/mod.rs`\n4. Add conversion in `src/ir_builder/mod.rs`\n5. Add evaluation in `src/code_generator/mod.rs`\n6. Implement function logic in `src/vector_ops.rs` or `src/temporal_ops.rs`\n7. Add tests and update `docs/reference/functions.md`\n\n### Adding a New Statement Type\n\n1. Add variant to `src/statement/mod.rs::Statement`\n2. Add parsing in appropriate `src/statement/*.rs` file\n3. Add handling in `src/protocol/handler.rs`\n4. Add tests and snapshot tests\n\n---\n\n## Getting Help\n\n- **Architecture questions**: Read `docs/internals/architecture.md`\n- **API questions**: Check doc comments with `cargo doc --open`\n- **Debugging**: Use `IL_TRACE_LEVEL=debug cargo run` for verbose output\n- **Tests failing**: Run `./scripts/run_snapshot_tests.sh -v` for diffs",
    "toc": [
      {
        "level": 2,
        "text": "Core Principles",
        "id": "core-principles"
      },
      {
        "level": 3,
        "text": "1. Single Source of Truth",
        "id": "1-single-source-of-truth"
      },
      {
        "level": 3,
        "text": "2. No Legacy Compatibility Code",
        "id": "2-no-legacy-compatibility-code"
      },
      {
        "level": 3,
        "text": "3. Types Flow Through the Pipeline",
        "id": "3-types-flow-through-the-pipeline"
      },
      {
        "level": 3,
        "text": "4. Small, Focused Modules",
        "id": "4-small-focused-modules"
      },
      {
        "level": 3,
        "text": "5. Explicit Over Implicit",
        "id": "5-explicit-over-implicit"
      },
      {
        "level": 3,
        "text": "6. Error Messages Are User Interface",
        "id": "6-error-messages-are-user-interface"
      },
      {
        "level": 3,
        "text": "7. Composition Over Inheritance",
        "id": "7-composition-over-inheritance"
      },
      {
        "level": 2,
        "text": "Code Style Guidelines",
        "id": "code-style-guidelines"
      },
      {
        "level": 3,
        "text": "Naming Conventions",
        "id": "naming-conventions"
      },
      {
        "level": 3,
        "text": "Function Size",
        "id": "function-size"
      },
      {
        "level": 3,
        "text": "Comments",
        "id": "comments"
      },
      {
        "level": 3,
        "text": "Documentation",
        "id": "documentation"
      },
      {
        "level": 3,
        "text": "Error Handling",
        "id": "error-handling"
      },
      {
        "level": 2,
        "text": "Module Organization",
        "id": "module-organization"
      },
      {
        "level": 3,
        "text": "Directory Structure",
        "id": "directory-structure"
      },
      {
        "level": 3,
        "text": "Module Dependencies",
        "id": "module-dependencies"
      },
      {
        "level": 2,
        "text": "Testing Standards",
        "id": "testing-standards"
      },
      {
        "level": 3,
        "text": "Test Organization",
        "id": "test-organization"
      },
      {
        "level": 3,
        "text": "Test Naming",
        "id": "test-naming"
      },
      {
        "level": 3,
        "text": "Test Coverage Requirements",
        "id": "test-coverage-requirements"
      },
      {
        "level": 3,
        "text": "Snapshot Tests",
        "id": "snapshot-tests"
      },
      {
        "level": 2,
        "text": "Performance Guidelines",
        "id": "performance-guidelines"
      },
      {
        "level": 3,
        "text": "Avoid Unnecessary Allocations",
        "id": "avoid-unnecessary-allocations"
      },
      {
        "level": 3,
        "text": "Use Iterators Instead of Collecting",
        "id": "use-iterators-instead-of-collecting"
      },
      {
        "level": 3,
        "text": "Clone Consciously",
        "id": "clone-consciously"
      },
      {
        "level": 2,
        "text": "Git Workflow",
        "id": "git-workflow"
      },
      {
        "level": 3,
        "text": "Commit Messages",
        "id": "commit-messages"
      },
      {
        "level": 3,
        "text": "Branch Naming",
        "id": "branch-naming"
      },
      {
        "level": 3,
        "text": "Pull Request Checklist",
        "id": "pull-request-checklist"
      },
      {
        "level": 2,
        "text": "Anti-Patterns to Avoid",
        "id": "anti-patterns-to-avoid"
      },
      {
        "level": 3,
        "text": "1. The God Module",
        "id": "1-the-god-module"
      },
      {
        "level": 3,
        "text": "2. Stringly Typed APIs",
        "id": "2-stringly-typed-apis"
      },
      {
        "level": 3,
        "text": "3. Boolean Parameters",
        "id": "3-boolean-parameters"
      },
      {
        "level": 3,
        "text": "4. Defensive Cloning",
        "id": "4-defensive-cloning"
      },
      {
        "level": 3,
        "text": "5. Comment-Driven Development",
        "id": "5-comment-driven-development"
      },
      {
        "level": 2,
        "text": "Checklist for New Code",
        "id": "checklist-for-new-code"
      },
      {
        "level": 2,
        "text": "Quick Reference",
        "id": "quick-reference"
      },
      {
        "level": 3,
        "text": "Adding a New Aggregate Function",
        "id": "adding-a-new-aggregate-function"
      },
      {
        "level": 3,
        "text": "Adding a New Built-in Function",
        "id": "adding-a-new-built-in-function"
      },
      {
        "level": 3,
        "text": "Adding a New Statement Type",
        "id": "adding-a-new-statement-type"
      },
      {
        "level": 2,
        "text": "Getting Help",
        "id": "getting-help"
      }
    ]
  },
  "internals/index": {
    "title": "Internals Overview",
    "content": "\n# InputLayer Internals\n\nArchitecture and development documentation for contributors.\n\n- [Architecture](/docs/internals/architecture) — System design, data flow, and optimization pipeline\n- [Coding Standards](/docs/internals/coding-standards) — Code style and contribution guidelines\n- [Type System](/docs/internals/type-system) — Internal type representation and coercion\n- [Validation](/docs/internals/validation) — Input validation and error handling\n- [Record Syntax](/docs/internals/record-syntax) — Named field access for relations\n- [Roadmap](/docs/internals/roadmap) — Development roadmap and future plans",
    "toc": []
  },
  "internals/record-syntax": {
    "title": "Record Syntax and Desugaring",
    "content": "# Record Syntax and Desugaring\n\nThis document specifies the record-related syntactic sugar in InputLayer and their desugaring rules.\n\n## Overview\n\nInputLayer supports record types as a way to document the structure of data. Record types serve as documentation; the desugaring features described here are planned for future implementation.\n\n## 1. Type Declarations\n\n### 1.1 Simple Type Aliases\n\n```datalog\ntype Email: string\ntype UserId: int\ntype Score: int\n```\n\nThese declare type aliases that can be used in schema declarations for documentation.\n\n### 1.2 Record Types\n\n```datalog\ntype User: {\n    id:      int,\n    name:    string,\n    email:   string\n}\n```\n\nA **record type** `{ f1 : t1, ..., fn : tn }` documents the structure of a relation.\n\n## 2. Schema Declarations\n\nSchemas are declared using the `+` prefix with typed columns:\n\n```datalog\n+user(id: int, name: string, email: string)\n```\n\nThis declares a 3-ary relation with typed columns. The schema enables:\n- Column name documentation\n- Type validation (when implemented)\n- Constraint declarations\n\n### 2.1 Schema with Type References\n\nYou can reference declared types in schemas:\n\n```datalog\ntype Email: string\ntype UserId: int\n\n+user(id: UserId, name: string, email: Email)\n```\n\n## 3. Working with Data\n\n### 3.1 Inserting Facts\n\n```datalog\n// Single fact\n+user(1, \"Alice\", \"alice@example.com\")\n\n// Bulk insert\n+user[(1, \"Alice\", \"alice@example.com\"), (2, \"Bob\", \"bob@example.com\")]\n```\n\n### 3.2 Persistent Rules\n\n```datalog\n+admin_email(Email) <-\n    user(_, _, Email),\n    admin(Email)\n```\n\n### 3.3 Queries\n\n```datalog\n?user(Id, Name, Email)\n?user(1, Name, _)\n```\n\n## 4. Planned Features (Not Yet Implemented)\n\nThe following features are documented for future implementation:\n\n### 4.1 Record-Based Schema Sugar\n\n**Planned syntax** (not yet implemented):\n\n```datalog\ntype User: { id: int, name: string, email: string }\n\n// Would desugar to: +user(id: int, name: string, email: string).\n+user: User\n```\n\n### 4.2 Named-Field Atoms\n\n**Planned syntax** (not yet implemented):\n\n```datalog\n+user(id = 1, name = \"Alice\", email = \"alice@example.com\")\n```\n\nWould desugar to positional form based on schema order.\n\n### 4.3 Record Literal Atoms\n\n**Planned syntax** (not yet implemented):\n\n```datalog\n+user({ id = 1, name = \"Alice\", email = \"alice@example.com\" })\n```\n\n### 4.4 Record Destructuring Patterns\n\n**Planned syntax** (not yet implemented):\n\n```datalog\nadmin_email(e) <-\n    user({ id: _, name: _, email: e }),\n    admin(e)\n```\n\n## 5. Style Guidelines\n\n### 5.1 Naming Conventions\n\n- **Types**: Capitalized (`Email`, `User`, `Purchase`)\n- **Variables**: Lowercase or capitalized (`e`, `Email`, `user_id`)\n- **Relations**: Lowercase (`user`, `purchase`, `admin_email`)\n\n### 5.2 Current Best Practices\n\nUse explicit positional schemas and facts:\n\n```datalog\n// Declare schema\n+user(id: int, name: string, email: string)\n\n// Insert facts positionally\n+user[(1, \"Alice\", \"alice@example.com\")]\n\n// Query with positional variables\n?user(Id, Name, Email)\n```",
    "toc": [
      {
        "level": 2,
        "text": "Overview",
        "id": "overview"
      },
      {
        "level": 2,
        "text": "1. Type Declarations",
        "id": "1-type-declarations"
      },
      {
        "level": 3,
        "text": "1.1 Simple Type Aliases",
        "id": "11-simple-type-aliases"
      },
      {
        "level": 3,
        "text": "1.2 Record Types",
        "id": "12-record-types"
      },
      {
        "level": 2,
        "text": "2. Schema Declarations",
        "id": "2-schema-declarations"
      },
      {
        "level": 3,
        "text": "2.1 Schema with Type References",
        "id": "21-schema-with-type-references"
      },
      {
        "level": 2,
        "text": "3. Working with Data",
        "id": "3-working-with-data"
      },
      {
        "level": 3,
        "text": "3.1 Inserting Facts",
        "id": "31-inserting-facts"
      },
      {
        "level": 3,
        "text": "3.2 Persistent Rules",
        "id": "32-persistent-rules"
      },
      {
        "level": 3,
        "text": "3.3 Queries",
        "id": "33-queries"
      },
      {
        "level": 2,
        "text": "4. Planned Features (Not Yet Implemented)",
        "id": "4-planned-features-not-yet-implemented"
      },
      {
        "level": 3,
        "text": "4.1 Record-Based Schema Sugar",
        "id": "41-record-based-schema-sugar"
      },
      {
        "level": 3,
        "text": "4.2 Named-Field Atoms",
        "id": "42-named-field-atoms"
      },
      {
        "level": 3,
        "text": "4.3 Record Literal Atoms",
        "id": "43-record-literal-atoms"
      },
      {
        "level": 3,
        "text": "4.4 Record Destructuring Patterns",
        "id": "44-record-destructuring-patterns"
      },
      {
        "level": 2,
        "text": "5. Style Guidelines",
        "id": "5-style-guidelines"
      },
      {
        "level": 3,
        "text": "5.1 Naming Conventions",
        "id": "51-naming-conventions"
      },
      {
        "level": 3,
        "text": "5.2 Current Best Practices",
        "id": "52-current-best-practices"
      }
    ]
  },
  "internals/roadmap": {
    "title": "InputLayer Roadmap",
    "content": "# InputLayer Roadmap\n\nThis document tracks planned features and improvements for InputLayer.\n\n## Recently Completed\n\n### String Functions (v0.1.0)\n\n**Status**: Implemented\n\n7 string functions available: `len`, `upper`, `lower`, `trim`, `substr`, `replace`, `concat`.\n\nSee `docs/reference/functions.md` Section 8 for full reference.\n\n---\n\n### Math Functions (v0.1.0)\n\n**Status**: Implemented\n\n13 math functions available: `abs`, `abs_int64`, `abs_float64`, `sqrt`, `pow`, `log`, `exp`, `sin`, `cos`, `tan`, `floor`, `ceil`, `sign`.\n\nSee `docs/reference/functions.md` Section 7 for full reference.\n\n---\n\n### TopK Aggregate (v0.1.0)\n\n**Status**: Implemented\n\nTop-K selection with ordering support:\n\n```datalog\n+top_scores(top_k<3, Name, Score:desc>) <- scores(Name, Score)\n```\n\nVariants: `top_k`, `top_k_threshold`, `within_radius`.\n\n---\n\n### Functions in Rule Heads (v0.1.0)\n\n**Status**: Implemented\n\nAll builtin functions (vector, math, string, temporal) now work in rule heads via computed head variables:\n\n```datalog\n// Compute and store similarities\n+similarity(Id1, Id2, Score) <-\n    embedding(Id1, V1), embedding(Id2, V2),\n    Id1 < Id2,\n    Score = cosine(V1, V2)\n\n// Arithmetic in rule heads\n+doubled(X, Y) <- nums(X), Y = X * 2\n```\n\n---\n\n## Planned Features\n\n### External Data Loading\n\n**Priority**: Medium\n**Status**: Not implemented\n\nLoad data from external file formats:\n\n```datalog\n.load users.json as user\n.load sales.parquet as sale\n.load events.csv as event\n```\n\n**Supported formats**:\n| Format | Description |\n|--------|-------------|\n| `.json` | JSON arrays or objects |\n| `.parquet` | Apache Parquet columnar format |\n| `.csv` | Comma-separated values |\n\n---\n\n### Distributed Execution\n\n**Priority**: Low\n**Status**: Not implemented\n\nMulti-node execution for horizontal scaling. Currently single-node only.\n\n---\n\n### User-Defined Aggregates\n\n**Priority**: Low\n**Status**: Not implemented\n\nAllow users to define custom aggregation functions beyond the built-in set.\n\n---\n\n### Worst-Case Optimal Joins\n\n**Priority**: Low\n**Status**: Not implemented\n\nWCOJ for cyclic queries where traditional binary join plans are suboptimal.\n\n---\n\n## Implementation Notes\n\n### Adding New Built-in Functions\n\nTo add a new built-in function:\n\n1. Add variant to `BuiltinFunc` enum in `src/ast/mod.rs`\n2. Implement parsing in `BuiltinFunc::parse()`\n3. Add IR variant to `BuiltinFunction` in `src/ir/mod.rs`\n4. Add conversion in `src/ir_builder/mod.rs`\n5. Add evaluation logic in `src/code_generator/mod.rs`\n6. Add tests in `examples/datalog/` with snapshot\n7. Update `docs/reference/functions.md`\n\n---\n\n## Version History\n\n### v0.1.0 (Current)\n- 55 builtin functions (vector, temporal, math, string, LSH, quantization)\n- Functions work in rule heads via computed head variables\n- HNSW vector indexes with full Datalog query integration (`hnsw_nearest` builtin)\n- Count distinct aggregate (`count_distinct`)\n- TopK, TopKThreshold, WithinRadius aggregates\n- ~3,107 unit tests + ~1,121 snapshot tests = ~4,228 total\n- WebSocket API with auth, streaming results, and notification replay (AsyncAPI-documented)\n- Production hardening complete (55/55 issues):\n  - Multi-user authentication and ACL system\n  - Rate limiting (per-connection WS, per-IP HTTP)\n  - Auto-compaction for DD-native persist layer\n  - Lock-free DD queries via storage snapshots\n  - Streaming result transport for large results (>1 MB)\n  - Relation drop, rule drop by prefix, clear by prefix\n  - TLS deployment support\n- WAL-based persistence with configurable durability\n- Basic Datalog operations\n- Persistent and session rules\n- Aggregations (count, sum, min, max, avg)\n- Vector distance functions\n- Type declarations and schemas\n- Differential Dataflow backend",
    "toc": [
      {
        "level": 2,
        "text": "Recently Completed",
        "id": "recently-completed"
      },
      {
        "level": 3,
        "text": "String Functions (v0.1.0)",
        "id": "string-functions-v010"
      },
      {
        "level": 3,
        "text": "Math Functions (v0.1.0)",
        "id": "math-functions-v010"
      },
      {
        "level": 3,
        "text": "TopK Aggregate (v0.1.0)",
        "id": "topk-aggregate-v010"
      },
      {
        "level": 3,
        "text": "Functions in Rule Heads (v0.1.0)",
        "id": "functions-in-rule-heads-v010"
      },
      {
        "level": 2,
        "text": "Planned Features",
        "id": "planned-features"
      },
      {
        "level": 3,
        "text": "External Data Loading",
        "id": "external-data-loading"
      },
      {
        "level": 3,
        "text": "Distributed Execution",
        "id": "distributed-execution"
      },
      {
        "level": 3,
        "text": "User-Defined Aggregates",
        "id": "user-defined-aggregates"
      },
      {
        "level": 3,
        "text": "Worst-Case Optimal Joins",
        "id": "worst-case-optimal-joins"
      },
      {
        "level": 2,
        "text": "Implementation Notes",
        "id": "implementation-notes"
      },
      {
        "level": 3,
        "text": "Adding New Built-in Functions",
        "id": "adding-new-built-in-functions"
      },
      {
        "level": 2,
        "text": "Version History",
        "id": "version-history"
      },
      {
        "level": 3,
        "text": "v0.1.0 (Current)",
        "id": "v010-current"
      }
    ]
  },
  "internals/type-system": {
    "title": "Type System Specification",
    "content": "# Type System Specification\n\nThis document specifies InputLayer's type system for developers implementing or extending the language.\n\n## Overview\n\nA program is a sequence of declarations:\n\n```ebnf\nDecl       ::= TypeDecl | SchemaDecl | RuleDecl | FactDecl\n```\n\n- `type` - defines **value types** (aliases, refinements, record types)\n- `+name(col: type, ...)` - defines **relation schemas** with typed columns\n- Facts (`+name[(...)]`) - provide **base tuples** for relations\n- Rules (`+name(...) <- body`) - define **derived tuples** for relations\n\n## 1. Type Declarations (`type`)\n\n### 1.1 Grammar\n\n```ebnf\nTypeDecl   ::= \"type\" TypeName \":\" TypeExpr \".\"\nTypeName   ::= UIdent          // capitalized, e.g. Email, User\n\nTypeExpr   ::= SimpleType\n             | RecordType\n\nSimpleType ::= BaseType [Refinements]\nBaseType   ::= \"int\" | \"string\" | \"bool\"\n             | \"list\" \"[\" TypeExpr \"]\"\n             | TypeName         // previously declared type\n\nRecordType ::= \"{\" FieldList \"}\"\n FieldList  ::= Field (\",\" Field)*\nField      ::= FieldName \":\" TypeExpr\n\n FieldName  ::= LIdent\n\nRefinements ::= \"(\" Refinement (\",\" Refinement)* \")\"\nRefinement  ::= Ident \"(\" ... \")\"   // opaque to the core language\n```\n\n### 1.2 Semantics\n\nA `type` declaration introduces a **value type**. Types describe the **shape and constraints of values** but **do not define relations** by themselves.\n\n### 1.3 Aliases and Refinements\n\n```datalog\ntype Email: string(pattern(\"^[^@]+@[^@]+$\"))\ntype Id:    int(range(1, 1000000))\ntype Tags:  list[string](not_empty)\n```\n\nInterpretation as refinement types:\n\n- `Email` is a subset of `string`\n- `Id` is a subset of `int`\n- `Tags` is a subset of `list[string]`\n\nTyping judgment:\n\n```\nG |- v : Email  =>  G |- v : string  and  v satisfies pattern(\"^[^@]+@[^@]+$\")\n```\n\nThe semantics of refinements (`pattern`, `range`, `not_empty`, ...) are implementation-defined.\n\n### 1.4 Record Types\n\n```datalog\ntype User: {\n    id:      Id,\n    name:    string(not_empty),\n    email:   Email,\n    my_tags: Tags\n}\n```\n\nA **record type** `{ f1 : t1, ..., fn : tn }` is the type of records with named fields `fi` of types `ti`.\n\nImportant:\n- `User` is a **value type**\n- A single `User` value is a record with 4 fields\n- This is *not* yet a relation; it's just a value type\n\n## 2. Schema Declarations\n\nA **relation schema** declares the structure of a relation (like a table).\n\n### 2.1 Grammar\n\n```ebnf\nSchemaDecl ::= \"+\" RelName \"(\" ParamList \")\" \".\"\n\nRelName    ::= LIdent           // e.g. user, high_spender\n\nParamList  ::= Param (\",\" Param)*\nParam      ::= ParamName \":\" TypeExpr [Annotations]\nParamName  ::= LIdent\n\nAnnotations ::= Annotation+\nAnnotation  ::= \"@\" AnnotName\n AnnotName   ::= \"key\" | \"unique\" | \"not_empty\" | ...\n```\n\n### 2.2 Schema Declaration\n\n```datalog\n+user(\n    id:      int,\n    name:    string,\n    email:   string\n)\n```\n\nThis declares a 3-ary relation:\n\n```\nuser : int x string x string -> Bool\n```\n\ni.e. `user` is a subset of `int x string x string`.\n\nEach parameter is a **column** in the relation.\n\n### 2.3 Schema with Type References\n\n```datalog\ntype Id:    int\ntype Email: string\n\n+user(id: Id, name: string, email: Email)\n```\n\n## 3. Terms and Expressions\n\n### 3.1 Grammar\n\n```ebnf\nTerm       ::= Var\n             | Constant\n\nVar        ::= LIdent | UIdent\n```\n\nConstants are implementation-defined (ints, strings, bools, lists, ...).\n\n## 4. Atoms, Facts, and Rules\n\n### 4.1 Atoms\n\n```ebnf\nAtom        ::= RelName \"(\" ArgList? \")\"\nArgList     ::= Arg (\",\" Arg)*\nArg         ::= Term\n```\n\nExample (positional):\n\n```datalog\nuser(1, \"Alice\", \"alice@example.com\")\n```\n\n### 4.2 Facts\n\n```ebnf\nFactDecl    ::= \"+\" Atom \".\"\n              | \"+\" RelName \"[\" TupleList \"]\" \".\"\n```\n\nExample:\n\n```datalog\n+user(1, \"Alice\", \"alice@example.com\")\n+user[(2, \"Bob\", \"bob@example.com\"), (3, \"Charlie\", \"charlie@example.com\")]\n```\n\n### 4.3 Rules\n\n```ebnf\nRuleDecl    ::= [\"+\" ] HeadAtom \"<-\" Body \".\"\nHeadAtom    ::= RelName \"(\" HeadArgList? \")\"\nHeadArgList ::= HeadArg (\",\" HeadArg)*\nHeadArg     ::= Term\n\nBody        ::= BodyAtom (\",\" BodyAtom)*\n BodyAtom    ::= Atom | Condition\n\nCondition   ::= Term RelOp Term\nRelOp       ::= \"=\" | \"!=\" | \"<\" | \">\" | \"<=\" | \">=\"\n```\n\n### 4.4 Persistent vs Session Rules\n\n```datalog\n// Persistent rule (with + prefix) - stored and incrementally maintained\n+admin_email(Email) <-\n    user(_, _, Email),\n    admin(Email)\n\n// Session rule (no + prefix) - computed on-demand, not stored\ntemp_result(X, Y) <- source(X, Y), X > 10\n```\n\n## 5. Base vs Derived Relations\n\n- **Base data (EDB)**: Relations populated via **facts only**\n- **Derived data (IDB/views)**: Relations populated via **rules**\n\nBoth are still just **relations**:\n- Schema gives the structure\n- Facts and rules together define the relation's **extension**\n\nSQL analogy:\n- `+user(...) + facts` is analogous to `CREATE TABLE user (...) + INSERT INTO user .`\n- `+admin_email(...) + rules` is analogous to `CREATE VIEW admin_email AS SELECT .`\n\n## 6. Persistent Rules and the `+` Prefix\n\n### 6.1 Design Principles\n\nClear separation of concerns:\n\n| Syntax | Purpose | DD Materialization | Type Checking |\n|--------|---------|-------------------|---------------|\n| `type` | Value type definitions | No | N/A |\n| `+name(col: type, ...)` | Schema declaration | **No** | **Yes** |\n| `+name(...) <- body` | Persistent rule (DD view) | **Yes** | Only if schema exists |\n| `name(...) <- body` | Session rule (transient) | **No** | Only if schema exists |\n| `+`/`-` (facts) | Base data manipulation | No | Only if schema exists |\n\n### 6.2 Persistent Rule Grammar\n\n```ebnf\nPersistentRule ::= \"+\" RuleName \"(\" ParamList \")\" \"<-\" Body \".\"\n```\n\n### 6.3 Session Rules\n\nRules without `+` prefix are **session rules**:\n\n```datalog\ntemp_result(X, Y) <- source(X, Y), X > 10\n```\n\n- Computed on-demand during evaluation\n- NOT persisted or incrementally maintained\n- Useful for ad-hoc queries\n\n## 7. Implementation Notes\n\n### 7.1 Type Persistence\n\nAll typing information needs to be persisted on a database level. The server implements multiple databases where same-named types could have different semantic meanings.\n\n### 7.2 Naming Conventions\n\nTo avoid confusion between types and variables:\n\n- **Types**: `UIdent` (capitalized) - `Email`, `User`, `Purchase`\n- **Variables**: `LIdent` or `UIdent` - `e`, `Email`, `user_id`\n- **Relations**: `LIdent` (lowercase) - `user`, `purchase`, `admin_email`\n\n### 7.3 Command vs Keyword Distinction\n\nDo NOT confuse:\n- `+name(col: type)` - Schema declaration syntax\n- `.rel` meta command - REPL command to list/describe relations\n\n## 8. Complete Example\n\n```datalog\n// Type definitions\ntype Id: int(range(1, 1000000))\ntype Email: string(pattern(\"^[^@]+@[^@]+$\"))\n\n// Schema declarations\n+user(id: Id, name: string, email: Email)\n+purchase(user_id: Id, amount: int)\n\n// Base data\n+user[(1, \"Alice\", \"alice@example.com\"), (2, \"Bob\", \"bob@example.com\")]\n+purchase[(1, 1500), (1, 200), (2, 300)]\n\n// Persistent rule (explicit DD materialization)\n+high_spender(UserId) <-\n    purchase(UserId, Amount),\n    Amount > 1000\n\n// Session rule (not materialized, just computed on query)\ntemp(Id) <- user(Id, _, _), high_spender(Id)\n\n// Query\n?high_spender(X)\n```\n\n## 9. Type Checking Algorithm\n\n### 9.1 Schema Lookup\n\nGiven a rule:\n\n```datalog\n+admin_email(Email) <- user(_, _, Email), admin(Email)\n```\n\nType checking steps:\n1. Lookup schema: if `+admin_email(email: Email)` exists, check arity matches\n2. Check that variable types are consistent across the rule\n3. Report type errors if mismatches found\n\n### 9.2 Variable Type Inference\n\nVariables get their types from:\n1. Their position in atoms with declared schemas\n2. Comparison with constants\n3. Aggregation context",
    "toc": [
      {
        "level": 2,
        "text": "Overview",
        "id": "overview"
      },
      {
        "level": 2,
        "text": "1. Type Declarations (type)",
        "id": "1-type-declarations-type"
      },
      {
        "level": 3,
        "text": "1.1 Grammar",
        "id": "11-grammar"
      },
      {
        "level": 3,
        "text": "1.2 Semantics",
        "id": "12-semantics"
      },
      {
        "level": 3,
        "text": "1.3 Aliases and Refinements",
        "id": "13-aliases-and-refinements"
      },
      {
        "level": 3,
        "text": "1.4 Record Types",
        "id": "14-record-types"
      },
      {
        "level": 2,
        "text": "2. Schema Declarations",
        "id": "2-schema-declarations"
      },
      {
        "level": 3,
        "text": "2.1 Grammar",
        "id": "21-grammar"
      },
      {
        "level": 3,
        "text": "2.2 Schema Declaration",
        "id": "22-schema-declaration"
      },
      {
        "level": 3,
        "text": "2.3 Schema with Type References",
        "id": "23-schema-with-type-references"
      },
      {
        "level": 2,
        "text": "3. Terms and Expressions",
        "id": "3-terms-and-expressions"
      },
      {
        "level": 3,
        "text": "3.1 Grammar",
        "id": "31-grammar"
      },
      {
        "level": 2,
        "text": "4. Atoms, Facts, and Rules",
        "id": "4-atoms-facts-and-rules"
      },
      {
        "level": 3,
        "text": "4.1 Atoms",
        "id": "41-atoms"
      },
      {
        "level": 3,
        "text": "4.2 Facts",
        "id": "42-facts"
      },
      {
        "level": 3,
        "text": "4.3 Rules",
        "id": "43-rules"
      },
      {
        "level": 3,
        "text": "4.4 Persistent vs Session Rules",
        "id": "44-persistent-vs-session-rules"
      },
      {
        "level": 2,
        "text": "5. Base vs Derived Relations",
        "id": "5-base-vs-derived-relations"
      },
      {
        "level": 2,
        "text": "6. Persistent Rules and the + Prefix",
        "id": "6-persistent-rules-and-the-prefix"
      },
      {
        "level": 3,
        "text": "6.1 Design Principles",
        "id": "61-design-principles"
      },
      {
        "level": 3,
        "text": "6.2 Persistent Rule Grammar",
        "id": "62-persistent-rule-grammar"
      },
      {
        "level": 3,
        "text": "6.3 Session Rules",
        "id": "63-session-rules"
      },
      {
        "level": 2,
        "text": "7. Implementation Notes",
        "id": "7-implementation-notes"
      },
      {
        "level": 3,
        "text": "7.1 Type Persistence",
        "id": "71-type-persistence"
      },
      {
        "level": 3,
        "text": "7.2 Naming Conventions",
        "id": "72-naming-conventions"
      },
      {
        "level": 3,
        "text": "7.3 Command vs Keyword Distinction",
        "id": "73-command-vs-keyword-distinction"
      },
      {
        "level": 2,
        "text": "8. Complete Example",
        "id": "8-complete-example"
      },
      {
        "level": 2,
        "text": "9. Type Checking Algorithm",
        "id": "9-type-checking-algorithm"
      },
      {
        "level": 3,
        "text": "9.1 Schema Lookup",
        "id": "91-schema-lookup"
      },
      {
        "level": 3,
        "text": "9.2 Variable Type Inference",
        "id": "92-variable-type-inference"
      }
    ]
  },
  "internals/validation": {
    "title": "Pre-Dataflow Validation Layer",
    "content": "# Pre-Dataflow Validation Layer\n\n> Status: Planned Architecture - The design below is not yet implemented. It describes the target validation system.\n\n## Problem Statement\n\nDifferential Dataflow (DD) operates on streaming deltas with multiplicities. It has no native concept of type enforcement. Once data enters DD, it flows through computation - \"rejecting\" invalid data would require expensive rollback.\n\n**Solution:** Validate data types *before* they enter the dataflow. Create a validation gate that enforces schemas at the boundary.\n\n**Scope:** Type validation only.\n\n## Architecture Overview\n\n```\n                    +-------------------------------------+\n                    |         VALIDATION LAYER            |\n                    |                                     |\n  +user(1, \"alice\") |  +----------+      +----------+    |\n  ----------------->|  |  Schema  |----->|   Type   |    |\n                    |  |  Lookup  |      |  Checker |    |\n                    |  +----------+      +----------+    |\n                    |                          |         |\n                    |                          v         |\n                    |        Ok(tuple) or Err(type error)|\n                    +----------------------+-------------+\n                                           |\n                         +-----------------+--------------+\n                         |                                |\n                         v                                v\n                    +---------+                      +---------+\n                    |   DD    |                      |  Error  |\n                    | Ingest  |                      | Response|\n                    +---------+                      +---------+\n```\n\n## What We Validate\n\n| Check | Example | When |\n|-------|---------|------|\n| Arity | `user` expects 2 columns, got 3 | Insert |\n| Type match | Column 1 expects `int`, got `\"alice\"` | Insert |\n| Type compatibility | Existing data matches new schema | Schema registration |\n\n## Components\n\n### 1. Schema Registry\n\nStores relation schemas with type information.\n\n```rust\npub struct SchemaRegistry {\n    /// Persistent schemas (loaded from disk, saved on change)\n    persistent: HashMap<String, RelationSchema>,\n\n    /// Session schemas (memory only, cleared on disconnect)\n    session: HashMap<String, RelationSchema>,\n}\n\npub struct RelationSchema {\n    pub name: String,\n    pub columns: Vec<ColumnDef>,\n}\n\npub struct ColumnDef {\n    pub name: String,\n    pub dtype: DataType,\n}\n\n#[derive(Clone, Debug, PartialEq)]\npub enum DataType {\n    Int,\n    Float,\n    String,\n    Bool,\n    Vector(usize),  // dimensionality\n}\n\nimpl SchemaRegistry {\n    /// Get effective schema for a relation.\n    /// Session schema shadows persistent if both exist.\n    pub fn get(&self, relation: &str) -> Option<&RelationSchema> {\n        self.session.get(relation)\n            .or_else(|| self.persistent.get(relation))\n    }\n\n    /// Register a persistent schema.\n    pub fn register_persistent(\n        &mut self,\n        schema: RelationSchema,\n        existing_data: Option<&[Vec<Value>]>,\n    ) -> Result<(), SchemaError> {\n        // If data exists, validate it matches the schema\n        if let Some(tuples) = existing_data {\n            for tuple in tuples {\n                TypeChecker::check(&schema, tuple)?;\n            }\n        }\n\n        self.persistent.insert(schema.name.clone(), schema);\n        self.persist_to_disk()?;\n        Ok(())\n    }\n\n    /// Register a session schema.\n    pub fn register_session(\n        &mut self,\n        schema: RelationSchema,\n        existing_data: Option<&[Vec<Value>]>,\n    ) -> Result<(), SchemaError> {\n        // Same validation as persistent\n        if let Some(tuples) = existing_data {\n            for tuple in tuples {\n                TypeChecker::check(&schema, tuple)?;\n            }\n        }\n\n        self.session.insert(schema.name.clone(), schema);\n        Ok(())\n    }\n\n    /// Clear session schemas (on disconnect).\n    pub fn clear_session(&mut self) {\n        self.session.clear();\n    }\n}\n```\n\n### 2. Type Checker\n\nValidates that values match declared types.\n\n```rust\npub struct TypeChecker;\n\nimpl TypeChecker {\n    pub fn check(\n        schema: &RelationSchema,\n        tuple: &[Value],\n    ) -> Result<(), TypeError> {\n        // Arity check\n        if tuple.len() != schema.columns.len() {\n            return Err(TypeError::ArityMismatch {\n                relation: schema.name.clone(),\n                expected: schema.columns.len(),\n                got: tuple.len(),\n            });\n        }\n\n        // Type check each column\n        for (i, (value, col)) in tuple.iter().zip(&schema.columns).enumerate() {\n            if !value.matches_type(&col.dtype) {\n                return Err(TypeError::TypeMismatch {\n                    relation: schema.name.clone(),\n                    column: col.name.clone(),\n                    column_index: i,\n                    expected: col.dtype.clone(),\n                    got: value.type_name(),\n                    value: value.clone(),\n                });\n            }\n        }\n\n        Ok(())\n    }\n}\n\nimpl Value {\n    pub fn matches_type(&self, dtype: &DataType) -> bool {\n        match (self, dtype) {\n            (Value::Int(_), DataType::Int) => true,\n            (Value::Float(_), DataType::Float) => true,\n            (Value::String(_), DataType::String) => true,\n            (Value::Bool(_), DataType::Bool) => true,\n            (Value::Vector(v), DataType::Vector(dim)) => v.len() == *dim,\n            // Int can widen to Float\n            (Value::Int(_), DataType::Float) => true,\n            _ => false,\n        }\n    }\n\n    pub fn type_name(&self) -> &'static str {\n        match self {\n            Value::Int(_) => \"int\",\n            Value::Float(_) => \"float\",\n            Value::String(_) => \"string\",\n            Value::Bool(_) => \"bool\",\n            Value::Vector(v) => \"vector\",\n        }\n    }\n}\n```\n\n### 3. Validation Layer\n\nOrchestrates validation for all operations.\n\n```rust\npub struct ValidationLayer {\n    schema_registry: SchemaRegistry,\n}\n\nimpl ValidationLayer {\n    /// Validate an insert operation. Called before DD ingest.\n    pub fn validate_insert(\n        &self,\n        relation: &str,\n        tuple: &[Value],\n    ) -> Result<(), ValidationError> {\n        // If no schema, allow anything (schema-less mode)\n        let Some(schema) = self.schema_registry.get(relation) else {\n            return Ok(());\n        };\n\n        // Type check\n        TypeChecker::check(schema, tuple)?;\n\n        Ok(())\n    }\n\n    /// Validate a batch insert.\n    pub fn validate_batch(\n        &self,\n        relation: &str,\n        tuples: &[Vec<Value>],\n    ) -> Result<(), ValidationError> {\n        let Some(schema) = self.schema_registry.get(relation) else {\n            return Ok(());\n        };\n\n        for tuple in tuples {\n            TypeChecker::check(schema, tuple)?;\n        }\n\n        Ok(())\n    }\n\n    /// Register a schema, validating against existing data.\n    pub fn register_schema(\n        &mut self,\n        schema: RelationSchema,\n        persistent: bool,\n        existing_data: Option<&[Vec<Value>]>,\n    ) -> Result<(), SchemaError> {\n        if persistent {\n            self.schema_registry.register_persistent(schema, existing_data)\n        } else {\n            self.schema_registry.register_session(schema, existing_data)\n        }\n    }\n}\n```\n\n## Schema Persistence\n\nSchemas follow the same session vs persistent pattern as rules:\n\n```datalog\n// Persistent schema - saved with knowledge graph\n+user(id: int, name: string)\n\n// Session schema - temporary, current connection only\nuser(id: int, name: string)\n```\n\n### Lifecycle Comparison\n\n| Aspect | Session Schema | Persistent Schema |\n|--------|---------------|-------------------|\n| Syntax | `rel(col: type)` | `+rel(col: type)` |\n| Storage | Memory only | Persisted to disk |\n| Lifetime | Until disconnect | Survives restart |\n| Use case | Scripts, testing | Production data |\n\n## Ingestion Modes\n\nInputLayer supports two ingestion patterns: schema-first (traditional) and data-first (exploratory).\n\n### Schema-First Ingestion\n\nDefine the schema before inserting data. All inserts are validated immediately.\n\n```datalog\n.kg create mydb\n.kg use mydb\n\n// 1. Define schema\n+user(id: int, email: string, active: bool)\n\n// 2. Insert data - each insert is validated\n+user(1, \"alice@x.com\", true)        // OK\n+user(2, \"bob@x.com\", false)         // OK\n+user(\"bad\", \"charlie@x.com\", true)  // ERROR: \"bad\" is not int\n```\n\n**Advantages:**\n- Catch type errors immediately at insert time\n- Clear contract for what data looks like\n- Better for production systems\n\n**Bulk loading with schema-first:**\n```datalog\n+product(sku: string, name: string, price: float)\n\n// Bulk insert - all tuples validated before any are inserted\n+product[\n    (\"SKU001\", \"Widget\", 9.99),\n    (\"SKU002\", \"Gadget\", 19.99),\n    (\"SKU003\", \"Gizmo\", 29.99)\n]\n```\n\n**File loading with schema-first:**\n```datalog\n+sensor_reading(timestamp: int, sensor_id: string, value: float)\n\n// Load validates each row against schema\n.load sensors.idl\n```\n\n### Data-First Ingestion\n\nInsert data without a schema (schema-less mode), then optionally add a schema later.\n\n```datalog\n.kg use mydb\n\n// 1. Insert data freely - no validation\n+user(1, \"alice@x.com\")\n+user(2, \"bob@x.com\")\n+user(3, \"charlie@x.com\")\n\n// 2. Later, add schema - validates existing data\n+user(id: int, email: string)\n// Success: all existing data matches schema\n```\n\n**Advantages:**\n- Quick prototyping and exploration\n- Import data first, figure out types later\n- Flexible for ad-hoc analysis\n\n**Schema-less behavior:**\n```datalog\n// Without schema, anything goes\n+mixed(1, \"text\")\n+mixed(\"also text\", 42)\n+mixed(true, [1.0, 2.0, 3.0])\n// All accepted - no type enforcement\n```\n\n**Adding schema to existing data:**\n```datalog\n// Existing data\n+event(1, \"click\", 1704067200)\n+event(2, \"view\", 1704067260)\n\n// Try to add schema\n+event(id: int, type: string, timestamp: int)\n// Success: existing data matches\n\n// But if data doesn't match...\n+log(1, \"info\", \"message\")\n+log(\"bad\", \"error\", \"another\")  // String in first column\n\n+log(id: int, level: string, msg: string)\n// ERROR: Existing data violates schema\n//   Row (\"bad\", \"error\", \"another\"): column 'id' expected int, got string\n```\n\n### Schema Rejection on Type Mismatch\n\nIf existing data doesn't match the proposed schema, the schema definition is **rejected** (not the data):\n\n```datalog\n+user(1, \"alice\")\n+user(\"not-an-int\", \"bob\")     // String in first column - allowed without schema\n\n+user(id: int, name: string)\n// Error: Cannot register schema for 'user'\n//   Existing data violates type requirements\n//   Row (\"not-an-int\", \"bob\"): column 'id' expected int, got string \"abc\"\n//   Fix the data before registering the schema\n```\n\nThe user must fix the data first:\n\n```datalog\n-user(\"not-an-int\", \"bob\")     // Remove bad row\n+user(id: int, name: string)   // Now schema can be registered\n```\n\n### Mixed Workflow\n\nYou can switch between modes as needed:\n\n```datalog\n// Start schema-less for exploration\n+experiment(1, \"trial-a\", 0.5)\n+experiment(2, \"trial-b\", 0.7)\n\n// Query and analyze\n?experiment(Id, Name, Score), Score > 0.6\n\n// Happy with structure? Lock it down with persistent schema\n+experiment(id: int, name: string, score: float)\n\n// Future inserts are now validated\n+experiment(3, \"trial-c\", 0.8)     // OK\n+experiment(\"bad\", \"trial-d\", 0.9) // ERROR\n```\n\n### Session Schema for Testing\n\nUse session schemas to temporarily validate without persisting:\n\n```datalog\n// Production has no schema (legacy data)\n// But you want to validate a batch before inserting\n\n// Define session schema (not persisted)\nuser(id: int, email: string)\n\n// Test your data\n+user(1, \"test@x.com\")  // Validated against session schema\n\n// Clear session when done\n.session clear\n```\n\n### Implementation Flow\n\n```\nInsert Request\n      |\n      v\n+-----------------+\n| Schema exists?  |\n+--------+--------+\n         |\n    +----+----+\n    |         |\n    v         v\n   Yes        No\n    |         |\n    v         |\n+--------+    |\n|Validate|    |\n| types  |    |\n+---+----+    |\n    |         |\n    v         v\n  Pass?    Insert\n    |      (no validation)\n  +-+-+\n  |   |\n  v   v\n Yes  No\n  |   |\n  v   v\nInsert Error\n\n\nSchema Registration Request\n           |\n           v\n  +-----------------+\n  | Data exists for |\n  | this relation?  |\n  +--------+--------+\n           |\n      +----+----+\n      |         |\n      v         v\n     Yes        No\n      |         |\n      v         |\n+-----------+   |\n| Validate  |   |\n| all rows  |   |\n+-----+-----+   |\n      |         |\n      v         v\n   All pass?  Register\n      |       schema\n    +-+-+\n    |   |\n    v   v\n   Yes  No\n    |   |\n    v   v\nRegister Reject\nschema   schema\n```\n\n## Integration\n\n### Statement Executor\n\n```rust\nimpl StatementExecutor {\n    pub fn execute_insert(\n        &mut self,\n        relation: &str,\n        tuples: Vec<Vec<Value>>,\n    ) -> Result<InsertResult, ExecuteError> {\n        // Validate all tuples first\n        self.validation.validate_batch(relation, &tuples)?;\n\n        // Insert into DD\n        self.dataflow.insert(relation, &tuples)?;\n\n        Ok(InsertResult { count: tuples.len() })\n    }\n\n    pub fn execute_schema(\n        &mut self,\n        schema: RelationSchema,\n        persistent: bool,\n    ) -> Result<(), ExecuteError> {\n        // Get existing data for this relation\n        let existing = self.dataflow.scan(&schema.name)?;\n        let existing_ref: Option<&[Vec<Value>]> = if existing.is_empty() {\n            None\n        } else {\n            Some(&existing)\n        };\n\n        // Register schema (validates existing data)\n        self.validation.register_schema(schema, persistent, existing_ref)?;\n\n        Ok(())\n    }\n}\n```\n\n### Database Load\n\n```rust\nimpl Database {\n    pub fn open(path: &Path) -> Result<Self, DbError> {\n        // 1. Load persistent schemas\n        let schemas = SchemaRegistry::load_from_disk(path)?;\n\n        // 2. Initialize validation layer\n        let validation = ValidationLayer::new(schemas);\n\n        // 3. Load data into DD\n        let dataflow = Dataflow::load_from_disk(path)?;\n\n        // Note: No constraint index rebuilding needed!\n\n        Ok(Database { dataflow, validation })\n    }\n}\n```\n\n### Storage Format\n\n```\ndata/\n  mydb/\n    rules/                  # Persistent rule definitions\n      catalog.json          # Rule catalog\n  persist/\n    shards/                 # Shard metadata\n    batches/                # Compacted batch files (Parquet)\n    wal/                    # Write-ahead log\n  metadata/                 # System metadata\n```\n\n```json\n{\n  \"version\": 1,\n  \"schemas\": {\n    \"user\": {\n      \"columns\": [\n        {\"name\": \"id\", \"type\": \"int\"},\n        {\"name\": \"email\", \"type\": \"string\"}\n      ]\n    },\n    \"embedding\": {\n      \"columns\": [\n        {\"name\": \"id\", \"type\": \"int\"},\n        {\"name\": \"vec\", \"type\": {\"vector\": 128}}\n      ]\n    }\n  }\n}\n```\n\n## Syntax\n\nThe schema syntax:\n\n```ebnf\nschema      ::= prefix? predicate \"(\" column_list \")\" \".\" ;\nprefix      ::= \"+\" ;\ncolumn_list ::= column (\",\" column)* ;\ncolumn      ::= name \":\" type ;\ntype        ::= \"int\" | \"float\" | \"string\" | \"bool\" | vector_type ;\nvector_type ::= \"vector\" \"[\" integer \"]\" ;\n```\n\nExamples:\n```datalog\n+user(id: int, name: string, active: bool)\n+embedding(id: int, vec: vector[128])\nproduct(sku: string, price: float)  // Session schema\n```\n\n## Error Messages\n\nClear, actionable error messages:\n\n```\nError: Type mismatch in relation 'user'\n  Column 'id' (index 0): expected int, got string\n  Value: \"not-a-number\"\n\nError: Arity mismatch in relation 'user'\n  Expected 2 columns, got 3\n  Schema: user(id: int, name: string)\n\nError: Cannot register schema for 'user'\n  Existing data violates type requirements\n  Row 5: column 'id' expected int, got string \"abc\"\n  Fix the data before registering the schema\n```\n\n## Migration Path\n\n### Type Validation\n- Implement `SchemaRegistry` and `TypeChecker`\n- Validate types on insert\n- Schema persistence\n\n### Derived Data Types\n- Infer output types for rules\n- Warn when rule output doesn't match view schema\n\n### Type Inference (Optional)\n- Infer schemas from data patterns\n- Suggest schemas based on inserted data\n\n## Summary\n\n| Feature | Status |\n|---------|--------|\n| Type validation (int, float, string, bool, vector) | Included |\n| Arity checking | Included |\n| Session vs persistent schemas | Included |\n| Schema-first workflow | Included |\n| Data-first workflow | Included |\n| Reject schema if data violates types | Included |\n\nThis keeps the system simple and DD-friendly while providing useful type safety.",
    "toc": [
      {
        "level": 2,
        "text": "Problem Statement",
        "id": "problem-statement"
      },
      {
        "level": 2,
        "text": "Architecture Overview",
        "id": "architecture-overview"
      },
      {
        "level": 2,
        "text": "What We Validate",
        "id": "what-we-validate"
      },
      {
        "level": 2,
        "text": "Components",
        "id": "components"
      },
      {
        "level": 3,
        "text": "1. Schema Registry",
        "id": "1-schema-registry"
      },
      {
        "level": 3,
        "text": "2. Type Checker",
        "id": "2-type-checker"
      },
      {
        "level": 3,
        "text": "3. Validation Layer",
        "id": "3-validation-layer"
      },
      {
        "level": 2,
        "text": "Schema Persistence",
        "id": "schema-persistence"
      },
      {
        "level": 3,
        "text": "Lifecycle Comparison",
        "id": "lifecycle-comparison"
      },
      {
        "level": 2,
        "text": "Ingestion Modes",
        "id": "ingestion-modes"
      },
      {
        "level": 3,
        "text": "Schema-First Ingestion",
        "id": "schema-first-ingestion"
      },
      {
        "level": 3,
        "text": "Data-First Ingestion",
        "id": "data-first-ingestion"
      },
      {
        "level": 3,
        "text": "Schema Rejection on Type Mismatch",
        "id": "schema-rejection-on-type-mismatch"
      },
      {
        "level": 3,
        "text": "Mixed Workflow",
        "id": "mixed-workflow"
      },
      {
        "level": 3,
        "text": "Session Schema for Testing",
        "id": "session-schema-for-testing"
      },
      {
        "level": 3,
        "text": "Implementation Flow",
        "id": "implementation-flow"
      },
      {
        "level": 2,
        "text": "Integration",
        "id": "integration"
      },
      {
        "level": 3,
        "text": "Statement Executor",
        "id": "statement-executor"
      },
      {
        "level": 3,
        "text": "Database Load",
        "id": "database-load"
      },
      {
        "level": 3,
        "text": "Storage Format",
        "id": "storage-format"
      },
      {
        "level": 2,
        "text": "Syntax",
        "id": "syntax"
      },
      {
        "level": 2,
        "text": "Error Messages",
        "id": "error-messages"
      },
      {
        "level": 2,
        "text": "Migration Path",
        "id": "migration-path"
      },
      {
        "level": 3,
        "text": "Type Validation",
        "id": "type-validation"
      },
      {
        "level": 3,
        "text": "Derived Data Types",
        "id": "derived-data-types"
      },
      {
        "level": 3,
        "text": "Type Inference (Optional)",
        "id": "type-inference-optional"
      },
      {
        "level": 2,
        "text": "Summary",
        "id": "summary"
      }
    ]
  },
  "reference/aggregations": {
    "title": "Aggregations Reference",
    "content": "# Aggregations Reference\n\nInputLayer supports 9 aggregation functions for computing summary values over groups of data.\n\n## Basic Aggregations\n\n### `count`\n\nCount the number of results.\n\n**Syntax:**\n```datalog\ncount<Variable>\n```\n\n**Example:**\n```datalog\n// Count all users\n+user_count(count<Id>) <- user(Id, _)\n\n// Count users per department\n+dept_count(Dept, count<Id>) <- user(Id, _, Dept)\n```\n\n**Returns:** Integer count of distinct bindings for the variable.\n\n---\n\n### `count_distinct`\n\nCount distinct values of a variable.\n\n**Syntax:**\n```datalog\ncount_distinct<Variable>\n```\n\n**Example:**\n```datalog\n// Count unique departments\n+unique_depts(count_distinct<Dept>) <- user(_, _, Dept)\n```\n\n**Returns:** Integer count of unique values.\n\n---\n\n### `sum`\n\nSum numeric values.\n\n**Syntax:**\n```datalog\nsum<Variable>\n```\n\n**Example:**\n```datalog\n// Total salary per department\n+dept_salary(Dept, sum<Salary>) <- employee(_, _, Dept, Salary)\n```\n\n**Returns:** Sum as Integer or Float (matches input type).\n\n---\n\n### `min`\n\nFind minimum value.\n\n**Syntax:**\n```datalog\nmin<Variable>\n```\n\n**Example:**\n```datalog\n// Lowest price per category\n+min_price(Category, min<Price>) <- product(_, Category, Price)\n```\n\n**Returns:** Minimum value (works with numbers and strings).\n\n---\n\n### `max`\n\nFind maximum value.\n\n**Syntax:**\n```datalog\nmax<Variable>\n```\n\n**Example:**\n```datalog\n// Highest score per game\n+high_score(Game, max<Score>) <- scores(_, Game, Score)\n```\n\n**Returns:** Maximum value (works with numbers and strings).\n\n---\n\n### `avg`\n\nCompute average of numeric values.\n\n**Syntax:**\n```datalog\navg<Variable>\n```\n\n**Example:**\n```datalog\n// Average rating per product\n+avg_rating(Product, avg<Rating>) <- reviews(_, Product, Rating)\n```\n\n**Returns:** Float average.\n\n---\n\n## Ranking Aggregations\n\n### `top_k`\n\nSelect the top K results ordered by a variable.\n\n**Syntax:**\n```datalog\ntop_k<K, OrderVariable>           // Ascending order (lowest K)\ntop_k<K, OrderVariable, desc>     // Descending order (highest K)\n```\n\n**Parameters:**\n- `K` - Number of results to return (integer)\n- `OrderVariable` - Variable to order by\n- `desc` - Optional: use descending order\n\n**Example:**\n```datalog\n// Top 10 highest scores\n+top_scores(Name, top_k<10, Score, desc>) <- scores(Name, Score)\n\n// Top 5 nearest neighbors by distance\n+nearest(Id, top_k<5, Dist>) <-\n    query_vec(QV),\n    vectors(Id, V),\n    Dist = euclidean(QV, V)\n```\n\n**Returns:** Up to K results with the ordering value.\n\n---\n\n### `top_k_threshold`\n\nSelect top K results, but only if they meet a minimum threshold.\n\n**Syntax:**\n```datalog\ntop_k_threshold<K, Threshold, OrderVariable>           // Ascending (default)\ntop_k_threshold<K, Threshold, OrderVariable, desc>     // Descending\n```\n\n**Parameters:**\n- `K` - Maximum number of results (integer)\n- `Threshold` - Minimum (or maximum for desc) value to include (float)\n- `OrderVariable` - Variable to order by (with optional `:desc`/`:asc` suffix)\n\n**Example:**\n```datalog\n// Top 10 products, but only if rating >= 4.0\n+top_rated(Product, top_k_threshold<10, 4.0, Rating, desc>) <-\n    reviews(Product, Rating)\n\n// Nearest 5 neighbors within distance 0.5\n+near_enough(Id, top_k_threshold<5, 0.5, Dist>) <-\n    query_vec(QV),\n    vectors(Id, V),\n    Dist = euclidean(QV, V)\n```\n\n**Returns:** Up to K results that meet the threshold.\n\n---\n\n### `within_radius`\n\nReturn all results within a distance threshold (range query).\n\n**Syntax:**\n```datalog\nwithin_radius<MaxDistance, DistanceVariable>\n```\n\n**Parameters:**\n- `MaxDistance` - Maximum distance to include (float)\n- `DistanceVariable` - Variable containing the distance (with optional `:asc`/`:desc` suffix)\n\n**Example:**\n```datalog\n// All vectors within distance 0.3\n+nearby(Id, within_radius<0.3, Dist>) <-\n    query_vec(QV),\n    vectors(Id, V),\n    Dist = cosine(QV, V)\n\n// Vectors within distance 0.5\n+close_vectors(Id, within_radius<0.5, D>) <-\n    query_vec(QV),\n    vectors(Id, V),\n    D = euclidean(QV, V)\n```\n\n**Returns:** All results where distance <= MaxDistance.\n\n---\n\n## Aggregation Rules\n\n### Grouping\n\nVariables in the head that are NOT aggregated become grouping keys:\n\n```datalog\n// Group by Department, aggregate Salary\n+dept_stats(Dept, sum<Salary>, avg<Salary>) <-\n    employee(_, _, Dept, Salary)\n```\n\n### Multiple Aggregations\n\nMultiple aggregations can be combined in one rule:\n\n```datalog\n+stats(count<Id>, min<Value>, max<Value>, avg<Value>) <-\n    data(Id, Value)\n```\n\n### With Filters\n\nAggregations work with filter conditions:\n\n```datalog\n// Only count active users\n+active_count(count<Id>) <- user(Id, Active), Active = true\n\n// Sum only positive values\n+positive_sum(sum<V>) <- values(V), V > 0\n```\n\n---\n\n## Quick Reference\n\n| Function | Syntax | Returns |\n|----------|--------|---------|\n| `count` | `count<X>` | Integer count |\n| `count_distinct` | `count_distinct<X>` | Integer unique count |\n| `sum` | `sum<X>` | Sum (Int or Float) |\n| `min` | `min<X>` | Minimum value |\n| `max` | `max<X>` | Maximum value |\n| `avg` | `avg<X>` | Float average |\n| `top_k` | `top_k<K, X>` or `top_k<K, X, desc>` | Top K results |\n| `top_k_threshold` | `top_k_threshold<K, T, X>` | Top K meeting threshold |\n| `within_radius` | `within_radius<Max, D>` | All within distance |",
    "toc": [
      {
        "level": 2,
        "text": "Basic Aggregations",
        "id": "basic-aggregations"
      },
      {
        "level": 3,
        "text": "count",
        "id": "count"
      },
      {
        "level": 3,
        "text": "countdistinct",
        "id": "countdistinct"
      },
      {
        "level": 3,
        "text": "sum",
        "id": "sum"
      },
      {
        "level": 3,
        "text": "min",
        "id": "min"
      },
      {
        "level": 3,
        "text": "max",
        "id": "max"
      },
      {
        "level": 3,
        "text": "avg",
        "id": "avg"
      },
      {
        "level": 2,
        "text": "Ranking Aggregations",
        "id": "ranking-aggregations"
      },
      {
        "level": 3,
        "text": "topk",
        "id": "topk"
      },
      {
        "level": 3,
        "text": "topkthreshold",
        "id": "topkthreshold"
      },
      {
        "level": 3,
        "text": "withinradius",
        "id": "withinradius"
      },
      {
        "level": 2,
        "text": "Aggregation Rules",
        "id": "aggregation-rules"
      },
      {
        "level": 3,
        "text": "Grouping",
        "id": "grouping"
      },
      {
        "level": 3,
        "text": "Multiple Aggregations",
        "id": "multiple-aggregations"
      },
      {
        "level": 3,
        "text": "With Filters",
        "id": "with-filters"
      },
      {
        "level": 2,
        "text": "Quick Reference",
        "id": "quick-reference"
      }
    ]
  },
  "reference/commands": {
    "title": "Meta Commands Reference",
    "content": "# Meta Commands Reference\n\nMeta commands control the InputLayer environment. They start with a `.` prefix.\n\n## Knowledge Graph Commands\n\n### `.kg`\n\nShow current knowledge graph.\n\n```\n.kg\n```\n\n**Output:**\n```\nCurrent knowledge graph: mykg\n```\n\n### `.kg list`\n\nList all knowledge graphs.\n\n```\n.kg list\n```\n\n**Output:**\n```\nKnowledge graphs:\n  default (current)\n  mykg\n  analytics\n```\n\n### `.kg create <name>`\n\nCreate a new knowledge graph.\n\n```\n.kg create analytics\n```\n\n### `.kg use <name>`\n\nSwitch to a knowledge graph.\n\n```\n.kg use analytics\n```\n\n**Note:** Switching knowledge graphs clears session rules and transient data.\n\n### `.kg drop <name>`\n\nDelete a knowledge graph and all its data.\n\n```\n.kg drop old_knowledge_graph\n```\n\n**Warning:** This permanently deletes all relations, rules, and data.\n\n## Access Control Commands\n\n### `.kg acl list [kg_name]`\n\nList access control entries for a knowledge graph. If no name is given, lists ACLs for the current KG.\n\n```\n.kg acl list\n.kg acl list analytics\n```\n\n### `.kg acl grant <kg> <user> <role>`\n\nGrant a user access to a knowledge graph with a specific role.\n\n```\n.kg acl grant analytics alice viewer\n.kg acl grant shared_data bob editor\n```\n\n**KG Roles:** `owner`, `editor`, `viewer`\n\n### `.kg acl revoke <kg> <user>`\n\nRevoke a user's access to a knowledge graph.\n\n```\n.kg acl revoke analytics alice\n```\n\n## User Management Commands\n\n### `.user list`\n\nList all users and their roles.\n\n```\n.user list\n```\n\n### `.user create <username> <password> <role>`\n\nCreate a new user with the given password and role.\n\n```\n.user create alice s3cret viewer\n.user create bob hunter2 admin\n```\n\n**Roles:** `admin`, `editor`, `viewer`\n\n### `.user drop <username>`\n\nDelete a user.\n\n```\n.user drop alice\n```\n\n### `.user password <username> <new_password>`\n\nChange a user's password.\n\n```\n.user password alice new-s3cret\n```\n\n### `.user role <username> <new_role>`\n\nChange a user's role.\n\n```\n.user role alice admin\n```\n\n---\n\n## Relation Commands\n\n### `.rel`\n\nList all relations with row counts.\n\n```\n.rel\n```\n\n**Output:**\n```\nRelations:\n  edge (arity: 2, columns: [col0: any, col1: any], tuples: 150)\n  person (arity: 3, columns: [id: int, name: string, age: int], tuples: 25)\n  department (arity: 2, columns: [col0: any, col1: any], tuples: 5)\n```\n\n### `.rel drop <name>`\n\nDelete a relation and all its data and schema.\n\n```\n.rel drop old_events\n```\n\n**Warning:** This permanently deletes the relation's tuples, metadata, schema, and persist storage. If a persistent rule exists with the same name as the relation, that rule is also removed. Other rules that reference the dropped relation are **not** automatically deleted — they will fail at query time. A `schema_change` notification is emitted.\n\n### `.rel <name>`\n\nDescribe a relation's schema and show sample data.\n\n```\n.rel person\n```\n\n**Output:**\n```\nRelation: person\n```\n\n| A | B | C |\n|---|---|---|\n| 1 | \"alice\" | 30 |\n| 2 | \"bob\" | 25 |\n\n*2 of 25 total rows*\n\n## Rule Commands\n\n### `.rule`\n\nList all persistent rules.\n\n```\n.rule\n```\n\n**Output:**\n```\nRules:\n  reachable (2 clause(s))\n  can_access (3 clause(s))\n```\n\n### `.rule list`\n\nSame as `.rule` - list all persistent rules.\n\n```\n.rule list\n```\n\n### `.rule <name>`\n\nQuery a rule and show computed results.\n\n```\n.rule reachable\n```\n\n**Output:**\n\n| X | Y |\n|---|---|\n| 1 | 2 |\n| 1 | 3 |\n| 1 | 4 |\n\n*3 rows*\n\n### `.rule def <name>`\n\nShow the definition of a rule.\n\n```\n.rule def reachable\n```\n\n**Output:**\n```\nRule: reachable\nClauses:\n  1. reachable(X, Y) <- edge(X, Y)\n  2. reachable(X, Z) <- reachable(X, Y), edge(Y, Z)\n```\n\n### `.rule drop <name>`\n\nDelete a rule entirely (removes all clauses).\n\n```\n.rule drop reachable\n```\n\n### `.rule drop prefix <prefix>`\n\nDelete all rules whose names start with the given prefix.\n\n```\n.rule drop prefix temp_\n```\n\n**Note:** An empty prefix is rejected to prevent accidentally dropping all rules.\n\n### `.rule remove <name> <index>`\n\nRemove a specific clause from a rule by index (1-based).\n\n```\n.rule remove reachable 2\n```\n\n**Output:**\n```\nClause 2 removed from rule 'reachable'\n```\n\n**Note:** If the last clause is removed, the entire rule is deleted:\n```\nClause 1 removed from rule 'simple'. Rule completely deleted (no clauses remaining)\n```\n\n**Errors:**\n- If clause index is out of bounds: `Clause index 5 out of bounds. Rule 'reachable' has 2 clause(s)`\n- If rule doesn't exist: `Rule 'nonexistent' does not exist`\n\n### `.rule clear <name>`\n\nClear all clauses from a rule for re-registration.\n\n```\n.rule clear reachable\n```\n\n### `.rule edit <name> <index> <clause>`\n\nEdit a specific clause in a rule.\n\n```\n.rule edit reachable 2 +reachable(X, Z) <- edge(X, Y), reachable(Y, Z)\n```\n\n**Note:** Index is 1-based.\n\n## Session Commands\n\nSession rules are transient and not persisted.\n\n### `.session`\n\nList current session rules.\n\n```\n.session\n```\n\n**Output:**\n```\nSession rules (2):\n  1. temp(X) <- edge(X, _)\n  2. filtered(X, Y) <- temp(X), edge(X, Y)\n```\n\n### `.session clear`\n\nClear all session rules.\n\n```\n.session clear\n```\n\n### `.session drop <n>`\n\nRemove a specific session rule by index.\n\n```\n.session drop 1\n```\n\n**Note:** Index is 1-based.\n\n## Load Command\n\nThe `.load` command reads and executes all statements from a `.idl` file sequentially.\n\n### `.load <file>`\n\nLoad and execute a file.\n\n```\n.load schema.idl\n```\n\n**Behavior:**\n- Reads the file from disk\n- Parses and executes each statement in order\n- Continues through non-fatal errors (parse errors, server errors) so that cleanup commands (e.g., `.kg drop`) always run\n- Only connection errors abort the script\n\n**Note:** The parser accepts `--replace` and `--merge` flags, but these modes are not yet implemented — all three modes currently execute identically (sequential statement execution).\n\n### Supported File Formats\n\n| Extension | Description |\n|-----------|-------------|\n| `.idl` | Datalog script (statements) |\n\n## System Commands\n\n### `.status`\n\nShow system status.\n\n```\n.status\n```\n\n**Output:**\n```\nKnowledge graph: mykg\nRelations: 5\nRules: 3\nSession rules: 2\nData directory: ./data\n```\n\n### `.clear prefix <prefix>`\n\nClear all facts from relations whose names start with the given prefix.\n\n```\n.clear prefix temp_\n```\n\n**Note:** An empty prefix is rejected. This clears data only - schemas and rules are preserved.\n\n### `.explain <query>`\n\nShow the query plan for a query without executing it. Displays the logical plan and applied optimizations.\n\n```\n.explain ?edge(X, Y)\n.explain ?path(1, X)\n```\n\n### `.compact`\n\nCompact storage by consolidating WAL and batch files.\n\n```\n.compact\n```\n\n### `.help`\n\nShow help message.\n\n```\n.help\n```\n\n### `.quit` / `.exit` / `.q`\n\nExit the REPL.\n\n```\n.quit\n```\n\n---\n\n## Index Commands\n\nCommands for managing HNSW (Hierarchical Navigable Small World) indexes for fast vector similarity search.\n\n### `.index` / `.index list`\n\nList all indexes in the current knowledge graph.\n\n```\n.index\n.index list\n```\n\n**Output:**\n```\nIndexes:\n  embeddings_idx on embeddings(vector) [hnsw, cosine]\n  docs_idx on documents(embedding) [hnsw, euclidean]\n```\n\n### `.index create`\n\nCreate a new HNSW index on a vector column.\n\n**Syntax:**\n```\n.index create <name> on <relation>(<column>) [type <index_type>] [metric <distance_metric>] [m <max_connections>] [ef_construction <beam_width>] [ef_search <search_beam>]\n```\n\n**Parameters:**\n- `name` - Unique name for the index\n- `relation` - Relation containing the vector column\n- `column` - Column name containing vectors\n- `type` - Index type (default: `hnsw`)\n- `metric` - Distance metric: `cosine`, `euclidean`, `dot_product`, `manhattan` (default: `cosine`)\n- `m` - Max connections per node (default: 16, higher = better recall, more memory)\n- `ef_construction` - Beam width during construction (default: 200, higher = better quality, slower build)\n- `ef_search` - Beam width during search (default: 50, higher = better recall, slower search)\n\n**Examples:**\n\nBasic index with defaults:\n```\n.index create my_idx on embeddings(vector)\n```\n\nIndex with cosine distance:\n```\n.index create doc_idx on documents(embedding) metric cosine\n```\n\nTuned index for high recall:\n```\n.index create high_recall_idx on items(vec) metric euclidean m 32 ef_construction 200 ef_search 100\n```\n\n### `.index drop`\n\nDelete an index.\n\n```\n.index drop <name>\n```\n\n**Example:**\n```\n.index drop embeddings_idx\n```\n\n### `.index stats`\n\nShow statistics for an index including size, build time, and configuration.\n\n```\n.index stats <name>\n```\n\n**Example:**\n```\n.index stats embeddings_idx\n```\n\n**Output:**\n```\nIndex: embeddings_idx\n  Relation: embeddings\n  Column: vector\n  Type: hnsw\n  Metric: cosine\n  Vectors: 10,000\n  Dimensions: 384\n  M: 16\n  ef_construction: 200\n  ef_search: 50\n  Memory: 15.2 MB\n```\n\n### `.index rebuild`\n\nForce rebuild an index. Useful after bulk inserts or if index becomes stale.\n\n```\n.index rebuild <name>\n```\n\n**Example:**\n```\n.index rebuild embeddings_idx\n```\n\n**Note:** Rebuilding large indexes may take significant time. The index remains available during rebuild.\n\n---\n\n## Error Handling\n\n### Invalid Command\n\n```\n.foo\n```\n\n**Output:**\n```\nUnknown meta command: .foo\n```\n\n### Missing Arguments\n\n```\n.kg create\n```\n\n**Output:**\n```\nUsage: .kg create <name>\n```\n\n### Rule Not Found\n\n```\n.rule nonexistent\n```\n\n**Output:**\n```\nRule 'nonexistent' not found\n```",
    "toc": [
      {
        "level": 2,
        "text": "Knowledge Graph Commands",
        "id": "knowledge-graph-commands"
      },
      {
        "level": 3,
        "text": ".kg",
        "id": "kg"
      },
      {
        "level": 3,
        "text": ".kg list",
        "id": "kg-list"
      },
      {
        "level": 3,
        "text": ".kg create <name>",
        "id": "kg-create-name"
      },
      {
        "level": 3,
        "text": ".kg use <name>",
        "id": "kg-use-name"
      },
      {
        "level": 3,
        "text": ".kg drop <name>",
        "id": "kg-drop-name"
      },
      {
        "level": 2,
        "text": "Access Control Commands",
        "id": "access-control-commands"
      },
      {
        "level": 3,
        "text": ".kg acl list [kgname]",
        "id": "kg-acl-list-kgname"
      },
      {
        "level": 3,
        "text": ".kg acl grant <kg> <user> <role>",
        "id": "kg-acl-grant-kg-user-role"
      },
      {
        "level": 3,
        "text": ".kg acl revoke <kg> <user>",
        "id": "kg-acl-revoke-kg-user"
      },
      {
        "level": 2,
        "text": "User Management Commands",
        "id": "user-management-commands"
      },
      {
        "level": 3,
        "text": ".user list",
        "id": "user-list"
      },
      {
        "level": 3,
        "text": ".user create <username> <password> <role>",
        "id": "user-create-username-password-role"
      },
      {
        "level": 3,
        "text": ".user drop <username>",
        "id": "user-drop-username"
      },
      {
        "level": 3,
        "text": ".user password <username> <newpassword>",
        "id": "user-password-username-newpassword"
      },
      {
        "level": 3,
        "text": ".user role <username> <newrole>",
        "id": "user-role-username-newrole"
      },
      {
        "level": 2,
        "text": "Relation Commands",
        "id": "relation-commands"
      },
      {
        "level": 3,
        "text": ".rel",
        "id": "rel"
      },
      {
        "level": 3,
        "text": ".rel drop <name>",
        "id": "rel-drop-name"
      },
      {
        "level": 3,
        "text": ".rel <name>",
        "id": "rel-name"
      },
      {
        "level": 2,
        "text": "Rule Commands",
        "id": "rule-commands"
      },
      {
        "level": 3,
        "text": ".rule",
        "id": "rule"
      },
      {
        "level": 3,
        "text": ".rule list",
        "id": "rule-list"
      },
      {
        "level": 3,
        "text": ".rule <name>",
        "id": "rule-name"
      },
      {
        "level": 3,
        "text": ".rule def <name>",
        "id": "rule-def-name"
      },
      {
        "level": 3,
        "text": ".rule drop <name>",
        "id": "rule-drop-name"
      },
      {
        "level": 3,
        "text": ".rule drop prefix <prefix>",
        "id": "rule-drop-prefix-prefix"
      },
      {
        "level": 3,
        "text": ".rule remove <name> <index>",
        "id": "rule-remove-name-index"
      },
      {
        "level": 3,
        "text": ".rule clear <name>",
        "id": "rule-clear-name"
      },
      {
        "level": 3,
        "text": ".rule edit <name> <index> <clause>",
        "id": "rule-edit-name-index-clause"
      },
      {
        "level": 2,
        "text": "Session Commands",
        "id": "session-commands"
      },
      {
        "level": 3,
        "text": ".session",
        "id": "session"
      },
      {
        "level": 3,
        "text": ".session clear",
        "id": "session-clear"
      },
      {
        "level": 3,
        "text": ".session drop <n>",
        "id": "session-drop-n"
      },
      {
        "level": 2,
        "text": "Load Command",
        "id": "load-command"
      },
      {
        "level": 3,
        "text": ".load <file>",
        "id": "load-file"
      },
      {
        "level": 3,
        "text": "Supported File Formats",
        "id": "supported-file-formats"
      },
      {
        "level": 2,
        "text": "System Commands",
        "id": "system-commands"
      },
      {
        "level": 3,
        "text": ".status",
        "id": "status"
      },
      {
        "level": 3,
        "text": ".clear prefix <prefix>",
        "id": "clear-prefix-prefix"
      },
      {
        "level": 3,
        "text": ".explain <query>",
        "id": "explain-query"
      },
      {
        "level": 3,
        "text": ".compact",
        "id": "compact"
      },
      {
        "level": 3,
        "text": ".help",
        "id": "help"
      },
      {
        "level": 3,
        "text": ".quit / .exit / .q",
        "id": "quit-exit-q"
      },
      {
        "level": 2,
        "text": "Index Commands",
        "id": "index-commands"
      },
      {
        "level": 3,
        "text": ".index / .index list",
        "id": "index-index-list"
      },
      {
        "level": 3,
        "text": ".index create",
        "id": "index-create"
      },
      {
        "level": 3,
        "text": ".index drop",
        "id": "index-drop"
      },
      {
        "level": 3,
        "text": ".index stats",
        "id": "index-stats"
      },
      {
        "level": 3,
        "text": ".index rebuild",
        "id": "index-rebuild"
      },
      {
        "level": 2,
        "text": "Error Handling",
        "id": "error-handling"
      },
      {
        "level": 3,
        "text": "Invalid Command",
        "id": "invalid-command"
      },
      {
        "level": 3,
        "text": "Missing Arguments",
        "id": "missing-arguments"
      },
      {
        "level": 3,
        "text": "Rule Not Found",
        "id": "rule-not-found"
      }
    ]
  },
  "reference/functions": {
    "title": "InputLayer Builtin Functions Reference",
    "content": "# InputLayer Builtin Functions Reference\n\n---\n\n## Overview\n\nInputLayer provides 55 builtin functions for vector operations, temporal processing, quantization, string manipulation, and math utilities.\n\n---\n\n## Table of Contents\n\n1. [Distance Functions](#1-distance-functions)\n2. [Vector Operations](#2-vector-operations)\n3. [LSH Functions](#3-lsh-locality-sensitive-hashing-functions)\n4. [Quantization Functions](#4-quantization-functions)\n5. [Int8 Distance Functions](#5-int8-distance-functions)\n6. [Temporal Functions](#6-temporal-functions)\n7. [Math Functions](#7-math-functions)\n8. [String Functions](#8-string-functions)\n9. [Scalar Min/Max Functions](#9-scalar-minmax-functions)\n\n---\n\n## 1. Distance Functions\n\nDistance functions for f32 vectors. All return `Float64`.\n\n### euclidean(v1, v2)\n\nEuclidean (L2) distance between two vectors.\n\n```datalog\n// Syntax\nD = euclidean(V1, V2)\n\n// Example\nsimilar(Id1, Id2, Dist) <-\n    vectors(Id1, V1),\n    vectors(Id2, V2),\n    Id1 < Id2,\n    Dist = euclidean(V1, V2),\n    Dist < 1.0\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v1 | Vector (f32) | First vector |\n| v2 | Vector (f32) | Second vector |\n| **Returns** | Float64 | Euclidean distance (>= 0) |\n\n---\n\n### cosine(v1, v2)\n\nCosine distance (1 - cosine similarity) between two vectors.\n\n```datalog\n// Syntax\nD = cosine(V1, V2)\n\n// Example - Find similar documents\nsimilar_docs(Id1, Id2) <-\n    doc_embedding(Id1, V1),\n    doc_embedding(Id2, V2),\n    D = cosine(V1, V2),\n    D < 0.1  // Very similar (close to 0)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v1 | Vector (f32) | First vector |\n| v2 | Vector (f32) | Second vector |\n| **Returns** | Float64 | Cosine distance (range [0, 2]) |\n\n**Note**: Returns 0 for identical directions, 1 for orthogonal, 2 for opposite directions.\n\n---\n\n### dot(v1, v2)\n\nDot product of two vectors.\n\n```datalog\n// Syntax\nScore = dot(V1, V2)\n\n// Example - Compute relevance scores\nrelevance(QueryId, DocId, Score) <-\n    query_vector(QueryId, Q),\n    doc_vector(DocId, D),\n    Score = dot(Q, D)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v1 | Vector (f32) | First vector |\n| v2 | Vector (f32) | Second vector |\n| **Returns** | Float64 | Dot product |\n\n---\n\n### manhattan(v1, v2)\n\nManhattan (L1) distance between two vectors. Good for sparse vectors.\n\n```datalog\n// Syntax\nD = manhattan(V1, V2)\n\n// Example\nnearby(Id1, Id2) <-\n    location(Id1, V1),\n    location(Id2, V2),\n    D = manhattan(V1, V2),\n    D < 10.0\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v1 | Vector (f32) | First vector |\n| v2 | Vector (f32) | Second vector |\n| **Returns** | Float64 | Manhattan distance (>= 0) |\n\n---\n\n## 2. Vector Operations\n\nOperations for manipulating vectors.\n\n### normalize(v)\n\nNormalize vector to unit length.\n\n```datalog\n// Syntax\nNormalized = normalize(V)\n\n// Example - Normalize before cosine similarity\nnormalized_embedding(Id, NormV) <-\n    raw_embedding(Id, V),\n    NormV = normalize(V)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v | Vector (f32) | Input vector |\n| **Returns** | Vector (f32) | Unit vector (length = 1) |\n\n**Note**: Returns zero vector if input is zero vector.\n\n---\n\n### vec_dim(v)\n\nGet the dimension (length) of a float32 vector.\n\n```datalog\n// Syntax\nDim = vec_dim(V)\n\n// Example - Filter by dimension\nvalid_embedding(Id, V) <-\n    embedding(Id, V),\n    Dim = vec_dim(V),\n    Dim = 128\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v | Vector (f32) | Input vector |\n| **Returns** | Int64 | Number of dimensions |\n\n---\n\n### vec_add(v1, v2)\n\nElement-wise addition of two vectors.\n\n```datalog\n// Syntax\nSum = vec_add(V1, V2)\n\n// Example - Combine embeddings\ncombined(Id, SumV) <-\n    text_embedding(Id, T),\n    image_embedding(Id, I),\n    SumV = vec_add(T, I)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v1 | Vector (f32) | First vector |\n| v2 | Vector (f32) | Second vector |\n| **Returns** | Vector (f32) | Element-wise sum |\n\n**Note**: Vectors must have same dimension.\n\n---\n\n### vec_scale(v, scalar)\n\nScale vector by a scalar value.\n\n```datalog\n// Syntax\nScaled = vec_scale(V, S)\n\n// Example - Apply weight\nweighted(Id, ScaledV) <-\n    embedding(Id, V),\n    weight(Id, W),\n    ScaledV = vec_scale(V, W)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v | Vector (f32) | Input vector |\n| scalar | Float64 | Scale factor |\n| **Returns** | Vector (f32) | Scaled vector |\n\n---\n\n## 3. LSH (Locality Sensitive Hashing) Functions\n\nFunctions for approximate nearest neighbor search.\n\n### lsh_bucket(v, table_idx, num_hyperplanes)\n\nCompute LSH bucket ID for a float32 vector.\n\n```datalog\n// Syntax\nBucket = lsh_bucket(V, TableIdx, NumHyperplanes)\n\n// Example - Build LSH index\nlsh_index(Id, Table, Bucket) <-\n    embedding(Id, V),\n    Table = 0,\n    Bucket = lsh_bucket(V, Table, 8)  // 8 hyperplanes = 256 buckets\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v | Vector (f32) | Input vector |\n| table_idx | Int64 | Hash table index (for multiple tables) |\n| num_hyperplanes | Int64 | Number of hyperplanes (controls granularity) |\n| **Returns** | Int64 | Bucket ID |\n\n**Note**: More hyperplanes = more buckets = higher precision but lower recall.\n\n---\n\n### lsh_probes(bucket, num_hp, num_probes)\n\nGenerate multi-probe sequence for a bucket by Hamming distance.\n\n```datalog\n// Syntax\nProbes = lsh_probes(Bucket, NumHyperplanes, NumProbes)\n\n// Example - Get probe sequence\nprobe_buckets(OrigBucket, Probes) <-\n    query_bucket(OrigBucket),\n    Probes = lsh_probes(OrigBucket, 8, 4)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| bucket | Int64 | Original bucket ID |\n| num_hp | Int64 | Number of hyperplanes used |\n| num_probes | Int64 | Number of probe buckets to generate |\n| **Returns** | Vector | Probe bucket IDs |\n\n---\n\n### lsh_multi_probe(v, table_idx, num_hp, num_probes)\n\nCompute LSH bucket and probes in one call for float32 vectors.\n\n```datalog\n// Syntax\nBuckets = lsh_multi_probe(V, TableIdx, NumHyperplanes, NumProbes)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v | Vector (f32) | Query vector |\n| table_idx | Int64 | Hash table index |\n| num_hp | Int64 | Number of hyperplanes |\n| num_probes | Int64 | Number of probes |\n| **Returns** | Vector | Bucket IDs to probe |\n\n---\n\n## 4. Quantization Functions\n\nFunctions for int8 vector quantization (memory-efficient storage).\n\n### quantize_linear(v)\n\nLinear quantization: maps [min, max] to [-128, 127].\n\n```datalog\n// Syntax\nQV = quantize_linear(V)\n\n// Example - Compress embeddings\ncompressed(Id, QV) <-\n    embedding(Id, V),\n    QV = quantize_linear(V)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v | Vector (f32) | Input vector |\n| **Returns** | VectorInt8 | Quantized vector |\n\n---\n\n### quantize_symmetric(v)\n\nSymmetric quantization: maps [-max_abs, max_abs] to [-127, 127], preserving zero.\n\n```datalog\nQV = quantize_symmetric(V)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v | Vector (f32) | Input vector |\n| **Returns** | VectorInt8 | Quantized vector |\n\n**Note**: Better preserves zero values; recommended for normalized vectors.\n\n---\n\n### dequantize(v)\n\nConvert int8 vector back to f32.\n\n```datalog\nFV = dequantize(QV)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v | VectorInt8 | Quantized vector |\n| **Returns** | Vector (f32) | Dequantized vector |\n\n**Note**: Lossy conversion - original precision not fully recovered.\n\n---\n\n### dequantize_scaled(v, scale)\n\nDequantize with explicit scale factor.\n\n```datalog\nFV = dequantize_scaled(QV, Scale)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v | VectorInt8 | Quantized vector |\n| scale | Float64 | Scale factor |\n| **Returns** | Vector (f32) | Dequantized vector |\n\n---\n\n## 5. Int8 Distance Functions\n\n### Native (Fast) Int8 Distance\n\nDirect computation on int8 values for maximum speed.\n\n### euclidean_int8(v1, v2)\n\nEuclidean distance for int8 vectors.\n\n```datalog\nD = euclidean_int8(QV1, QV2)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v1 | VectorInt8 | First quantized vector |\n| v2 | VectorInt8 | Second quantized vector |\n| **Returns** | Float64 | Euclidean distance |\n\n---\n\n### cosine_int8(v1, v2)\n\nCosine distance for int8 vectors.\n\n```datalog\nD = cosine_int8(QV1, QV2)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v1 | VectorInt8 | First quantized vector |\n| v2 | VectorInt8 | Second quantized vector |\n| **Returns** | Float64 | Cosine distance [0, 2] |\n\n---\n\n### dot_int8(v1, v2)\n\nDot product for int8 vectors.\n\n```datalog\nScore = dot_int8(QV1, QV2)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v1 | VectorInt8 | First quantized vector |\n| v2 | VectorInt8 | Second quantized vector |\n| **Returns** | Float64 | Dot product |\n\n---\n\n### manhattan_int8(v1, v2)\n\nManhattan distance for int8 vectors.\n\n```datalog\nD = manhattan_int8(QV1, QV2)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| v1 | VectorInt8 | First quantized vector |\n| v2 | VectorInt8 | Second quantized vector |\n| **Returns** | Float64 | Manhattan distance |\n\n---\n\n## 6. Temporal Functions\n\nFunctions for time-based queries and temporal reasoning.\n\n### time_now()\n\nGet current Unix timestamp in milliseconds.\n\n```datalog\nNow = time_now()\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| (none) | - | No parameters |\n| **Returns** | Int64 | Unix timestamp (milliseconds) |\n\n---\n\n### time_diff(t1, t2)\n\nCompute difference between two timestamps.\n\n```datalog\nDiff = time_diff(T1, T2)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| t1 | Int64 | First timestamp |\n| t2 | Int64 | Second timestamp |\n| **Returns** | Int64 | Difference (t1 - t2) in milliseconds |\n\n---\n\n### time_add(ts, duration_ms)\n\nAdd duration to timestamp.\n\n```datalog\nNewTime = time_add(Ts, DurationMs)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| ts | Int64 | Base timestamp |\n| duration_ms | Int64 | Duration to add (milliseconds) |\n| **Returns** | Int64 | New timestamp |\n\n---\n\n### time_sub(ts, duration_ms)\n\nSubtract duration from timestamp.\n\n```datalog\nNewTime = time_sub(Ts, DurationMs)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| ts | Int64 | Base timestamp |\n| duration_ms | Int64 | Duration to subtract (milliseconds) |\n| **Returns** | Int64 | New timestamp |\n\n---\n\n### time_decay(ts, now, half_life_ms)\n\nExponential time decay. Formula: `0.5^(age/half_life)`.\n\n```datalog\nWeight = time_decay(Ts, Now, HalfLifeMs)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| ts | Int64 | Event timestamp |\n| now | Int64 | Current timestamp |\n| half_life_ms | Int64 | Half-life in milliseconds |\n| **Returns** | Float64 | Decay factor [0, 1] |\n\n**Note**: Returns 1.0 at ts=now, 0.5 at age=half_life, approaches 0 for old events.\n\n---\n\n### time_decay_linear(ts, now, max_age_ms)\n\nLinear time decay. Formula: `max(0, 1 - age/max_age)`.\n\n```datalog\nWeight = time_decay_linear(Ts, Now, MaxAgeMs)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| ts | Int64 | Event timestamp |\n| now | Int64 | Current timestamp |\n| max_age_ms | Int64 | Maximum age for decay |\n| **Returns** | Float64 | Decay factor [0, 1] |\n\n**Note**: Returns 1.0 at ts=now, 0.5 at age=max_age/2, 0.0 at age>=max_age.\n\n---\n\n### time_before(t1, t2)\n\nCheck if t1 is before t2.\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| t1 | Int64 | First timestamp |\n| t2 | Int64 | Second timestamp |\n| **Returns** | Bool | true if t1 < t2 |\n\n---\n\n### time_after(t1, t2)\n\nCheck if t1 is after t2.\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| t1 | Int64 | First timestamp |\n| t2 | Int64 | Second timestamp |\n| **Returns** | Bool | true if t1 > t2 |\n\n---\n\n### time_between(ts, start, end)\n\nCheck if timestamp is within range [start, end] (inclusive).\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| ts | Int64 | Timestamp to check |\n| start | Int64 | Window start |\n| end | Int64 | Window end |\n| **Returns** | Bool | true if start <= ts <= end |\n\n---\n\n### within_last(ts, now, duration_ms)\n\nCheck if timestamp is within the last duration from now.\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| ts | Int64 | Timestamp to check |\n| now | Int64 | Current timestamp |\n| duration_ms | Int64 | Window size in milliseconds |\n| **Returns** | Bool | true if (now - duration_ms) <= ts <= now |\n\n---\n\n### intervals_overlap(s1, e1, s2, e2)\n\nCheck if two time intervals overlap.\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| s1 | Int64 | First interval start |\n| e1 | Int64 | First interval end |\n| s2 | Int64 | Second interval start |\n| e2 | Int64 | Second interval end |\n| **Returns** | Bool | true if intervals overlap |\n\n---\n\n### interval_contains(s1, e1, s2, e2)\n\nCheck if interval [s1, e1] fully contains interval [s2, e2].\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| s1 | Int64 | Outer interval start |\n| e1 | Int64 | Outer interval end |\n| s2 | Int64 | Inner interval start |\n| e2 | Int64 | Inner interval end |\n| **Returns** | Bool | true if [s1,e1] contains [s2,e2] |\n\n---\n\n### interval_duration(start, end)\n\nGet the duration of an interval.\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| start | Int64 | Interval start |\n| end | Int64 | Interval end |\n| **Returns** | Int64 | Duration (end - start) in milliseconds |\n\n---\n\n### point_in_interval(ts, start, end)\n\nCheck if a point is within an interval [start, end] (inclusive).\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| ts | Int64 | Point timestamp |\n| start | Int64 | Interval start |\n| end | Int64 | Interval end |\n| **Returns** | Bool | true if start <= ts <= end |\n\n---\n\n## 7. Math Functions\n\nGeneral-purpose math functions. All accept Int64 or Float64 inputs (coerced to f64 internally unless noted).\n\n### abs(x)\n\nGeneric absolute value. Preserves input type.\n\n```datalog\nA = abs(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Int64 or Float64 | Input value |\n| **Returns** | Same as input | Absolute value |\n\n---\n\n### abs_int64(x)\n\nAbsolute value of an integer.\n\n```datalog\nAbsVal = abs_int64(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Int64 | Input integer |\n| **Returns** | Int64 | Absolute value |\n\n**Note**: Uses saturating arithmetic for `i64::MIN`.\n\n---\n\n### abs_float64(x)\n\nAbsolute value of a float.\n\n```datalog\nAbsVal = abs_float64(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 | Input float |\n| **Returns** | Float64 | Absolute value |\n\n---\n\n### sqrt(x)\n\nSquare root.\n\n```datalog\nR = sqrt(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 or Int64 | Input value (>= 0) |\n| **Returns** | Float64 | Square root |\n\n**Note**: Returns Null if x < 0.\n\n---\n\n### pow(base, exp)\n\nPower function.\n\n```datalog\nR = pow(Base, Exp)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| base | Float64 or Int64 | Base value |\n| exp | Float64 or Int64 | Exponent |\n| **Returns** | Float64 | base^exp |\n\n---\n\n### log(x)\n\nNatural logarithm (base e).\n\n```datalog\nR = log(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 or Int64 | Input value (> 0) |\n| **Returns** | Float64 | Natural logarithm |\n\n**Note**: Returns Null if x <= 0.\n\n---\n\n### exp(x)\n\nExponential function (e^x).\n\n```datalog\nR = exp(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 or Int64 | Input value |\n| **Returns** | Float64 | e^x |\n\n---\n\n### sin(x)\n\nSine function (radians).\n\n```datalog\nR = sin(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 or Int64 | Angle in radians |\n| **Returns** | Float64 | Sine of x |\n\n---\n\n### cos(x)\n\nCosine function (radians).\n\n```datalog\nR = cos(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 or Int64 | Angle in radians |\n| **Returns** | Float64 | Cosine of x |\n\n---\n\n### tan(x)\n\nTangent function (radians).\n\n```datalog\nR = tan(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 or Int64 | Angle in radians |\n| **Returns** | Float64 | Tangent of x |\n\n---\n\n### floor(x)\n\nRound down to nearest integer.\n\n```datalog\nR = floor(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 or Int64 | Input value |\n| **Returns** | Int64 | Largest integer <= x |\n\n---\n\n### ceil(x)\n\nRound up to nearest integer.\n\n```datalog\nR = ceil(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 or Int64 | Input value |\n| **Returns** | Int64 | Smallest integer >= x |\n\n---\n\n### sign(x)\n\nSign function. Returns -1, 0, or 1.\n\n```datalog\nS = sign(X)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| x | Float64 or Int64 | Input value |\n| **Returns** | Int64 | -1 (negative), 0 (zero), or 1 (positive) |\n\n**Note**: For float NaN, returns 0.\n\n---\n\n## 8. String Functions\n\nFunctions for string manipulation.\n\n### len(s)\n\nGet string length (byte count).\n\n```datalog\nL = len(S)\n\n// Example\nlong_names(Name, L) <-\n    names(Name),\n    L = len(Name),\n    L > 10\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| s | String | Input string |\n| **Returns** | Int64 | Byte length of string |\n\n---\n\n### upper(s)\n\nConvert string to uppercase (Unicode-aware).\n\n```datalog\nU = upper(S)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| s | String | Input string |\n| **Returns** | String | Uppercase string |\n\n---\n\n### lower(s)\n\nConvert string to lowercase (Unicode-aware).\n\n```datalog\nL = lower(S)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| s | String | Input string |\n| **Returns** | String | Lowercase string |\n\n---\n\n### trim(s)\n\nRemove leading and trailing whitespace.\n\n```datalog\nT = trim(S)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| s | String | Input string |\n| **Returns** | String | Trimmed string |\n\n---\n\n### substr(s, start, len)\n\nExtract a substring.\n\n```datalog\nSub = substr(S, Start, Len)\n\n// Example - Get first 3 characters\nprefix(Name, P) <-\n    names(Name),\n    P = substr(Name, 0, 3)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| s | String | Input string |\n| start | Int64 | Start byte index (0-based) |\n| len | Int64 | Maximum length to extract |\n| **Returns** | String | Extracted substring |\n\n**Note**: Returns empty string if start > string length. Clamps to string bounds.\n\n---\n\n### replace(s, find, replacement)\n\nReplace all occurrences of a substring.\n\n```datalog\nR = replace(S, Find, Replacement)\n\n// Example\ncleaned(R) <-\n    raw(\"hello-world\"),\n    R = replace(\"hello-world\", \"-\", \" \")\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| s | String | Input string |\n| find | String | Substring to find |\n| replacement | String | Replacement string |\n| **Returns** | String | String with all occurrences replaced |\n\n---\n\n### concat(s1, s2, ...)\n\nConcatenate multiple values into a string. Variable arity (2+ arguments).\n\n```datalog\nR = concat(S1, S2)\n\n// Example - Build full name\nfull_name(First, Last, Full) <-\n    person(First, Last),\n    Full = concat(First, \" \", Last)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| s1, s2, ... | String/Int64/Float64 | Values to concatenate |\n| **Returns** | String | Concatenated string |\n\n**Note**: Non-string values are automatically converted to their string representation.\n\n---\n\n## 9. Scalar Min/Max Functions\n\nScalar comparison functions returning the minimum or maximum of two values.\n\n### min_val(a, b)\n\nReturn the smaller of two values.\n\n```datalog\nM = min_val(A, B)\n\n// Example - Clamp to range\nclamped(X, C) <-\n    values(X),\n    C = max_val(0, min_val(X, 100))\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| a | Int64/Float64/String | First value |\n| b | Int64/Float64/String | Second value |\n| **Returns** | Same as input | Smaller of the two values |\n\n**Note**: Do not confuse with the `min` aggregation. `min_val` is a scalar function comparing two values. Mixed numeric types (Int64 + Float64) return Float64. Strings compare lexicographically.\n\n---\n\n### max_val(a, b)\n\nReturn the larger of two values.\n\n```datalog\nM = max_val(A, B)\n```\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| a | Int64/Float64/String | First value |\n| b | Int64/Float64/String | Second value |\n| **Returns** | Same as input | Larger of the two values |\n\n**Note**: Do not confuse with the `max` aggregation. `max_val` is a scalar function comparing two values. Mixed numeric types (Int64 + Float64) return Float64. Strings compare lexicographically.\n\n---\n\n## Appendix: Function Quick Reference\n\n| Function | Parameters | Returns | Category |\n|----------|-----------|---------|----------|\n| `euclidean` | (v1, v2) | Float64 | Distance |\n| `cosine` | (v1, v2) | Float64 | Distance |\n| `dot` | (v1, v2) | Float64 | Distance |\n| `manhattan` | (v1, v2) | Float64 | Distance |\n| `normalize` | (v) | Vector | Vector Ops |\n| `vec_dim` | (v) | Int64 | Vector Ops |\n| `vec_add` | (v1, v2) | Vector | Vector Ops |\n| `vec_scale` | (v, s) | Vector | Vector Ops |\n| `lsh_bucket` | (v, table, hp) | Int64 | LSH |\n| `lsh_probes` | (bucket, hp, n) | Vector | LSH |\n| `lsh_multi_probe` | (v, table, hp, n) | Vector | LSH |\n| `quantize_linear` | (v) | VectorInt8 | Quantization |\n| `quantize_symmetric` | (v) | VectorInt8 | Quantization |\n| `dequantize` | (qv) | Vector | Quantization |\n| `dequantize_scaled` | (qv, s) | Vector | Quantization |\n| `euclidean_int8` | (qv1, qv2) | Float64 | Int8 Distance |\n| `cosine_int8` | (qv1, qv2) | Float64 | Int8 Distance |\n| `dot_int8` | (qv1, qv2) | Float64 | Int8 Distance |\n| `manhattan_int8` | (qv1, qv2) | Float64 | Int8 Distance |\n| `time_now` | () | Int64 | Temporal |\n| `time_diff` | (t1, t2) | Int64 | Temporal |\n| `time_add` | (ts, dur) | Int64 | Temporal |\n| `time_sub` | (ts, dur) | Int64 | Temporal |\n| `time_decay` | (ts, now, hl) | Float64 | Temporal |\n| `time_decay_linear` | (ts, now, max) | Float64 | Temporal |\n| `time_before` | (t1, t2) | Bool | Temporal |\n| `time_after` | (t1, t2) | Bool | Temporal |\n| `time_between` | (ts, s, e) | Bool | Temporal |\n| `within_last` | (ts, now, dur) | Bool | Temporal |\n| `intervals_overlap` | (s1, e1, s2, e2) | Bool | Temporal |\n| `interval_contains` | (s1, e1, s2, e2) | Bool | Temporal |\n| `interval_duration` | (s, e) | Int64 | Temporal |\n| `point_in_interval` | (ts, s, e) | Bool | Temporal |\n| `abs` | (x) | same type | Math |\n| `abs_int64` | (x) | Int64 | Math |\n| `abs_float64` | (x) | Float64 | Math |\n| `sqrt` | (x) | Float64 | Math |\n| `pow` | (base, exp) | Float64 | Math |\n| `log` | (x) | Float64 | Math |\n| `exp` | (x) | Float64 | Math |\n| `sin` | (x) | Float64 | Math |\n| `cos` | (x) | Float64 | Math |\n| `tan` | (x) | Float64 | Math |\n| `floor` | (x) | Int64 | Math |\n| `ceil` | (x) | Int64 | Math |\n| `sign` | (x) | Int64 | Math |\n| `len` | (s) | Int64 | String |\n| `upper` | (s) | String | String |\n| `lower` | (s) | String | String |\n| `trim` | (s) | String | String |\n| `substr` | (s, start, len) | String | String |\n| `replace` | (s, find, repl) | String | String |\n| `concat` | (s1, s2, ...) | String | String |\n| `min_val` | (a, b) | same type | Min/Max |\n| `max_val` | (a, b) | same type | Min/Max |",
    "toc": [
      {
        "level": 2,
        "text": "Overview",
        "id": "overview"
      },
      {
        "level": 2,
        "text": "Table of Contents",
        "id": "table-of-contents"
      },
      {
        "level": 2,
        "text": "1. Distance Functions",
        "id": "1-distance-functions"
      },
      {
        "level": 3,
        "text": "euclidean(v1, v2)",
        "id": "euclideanv1-v2"
      },
      {
        "level": 3,
        "text": "cosine(v1, v2)",
        "id": "cosinev1-v2"
      },
      {
        "level": 3,
        "text": "dot(v1, v2)",
        "id": "dotv1-v2"
      },
      {
        "level": 3,
        "text": "manhattan(v1, v2)",
        "id": "manhattanv1-v2"
      },
      {
        "level": 2,
        "text": "2. Vector Operations",
        "id": "2-vector-operations"
      },
      {
        "level": 3,
        "text": "normalize(v)",
        "id": "normalizev"
      },
      {
        "level": 3,
        "text": "vecdim(v)",
        "id": "vecdimv"
      },
      {
        "level": 3,
        "text": "vecadd(v1, v2)",
        "id": "vecaddv1-v2"
      },
      {
        "level": 3,
        "text": "vecscale(v, scalar)",
        "id": "vecscalev-scalar"
      },
      {
        "level": 2,
        "text": "3. LSH (Locality Sensitive Hashing) Functions",
        "id": "3-lsh-locality-sensitive-hashing-functions"
      },
      {
        "level": 3,
        "text": "lshbucket(v, tableidx, numhyperplanes)",
        "id": "lshbucketv-tableidx-numhyperplanes"
      },
      {
        "level": 3,
        "text": "lshprobes(bucket, numhp, numprobes)",
        "id": "lshprobesbucket-numhp-numprobes"
      },
      {
        "level": 3,
        "text": "lshmultiprobe(v, tableidx, numhp, numprobes)",
        "id": "lshmultiprobev-tableidx-numhp-numprobes"
      },
      {
        "level": 2,
        "text": "4. Quantization Functions",
        "id": "4-quantization-functions"
      },
      {
        "level": 3,
        "text": "quantizelinear(v)",
        "id": "quantizelinearv"
      },
      {
        "level": 3,
        "text": "quantizesymmetric(v)",
        "id": "quantizesymmetricv"
      },
      {
        "level": 3,
        "text": "dequantize(v)",
        "id": "dequantizev"
      },
      {
        "level": 3,
        "text": "dequantizescaled(v, scale)",
        "id": "dequantizescaledv-scale"
      },
      {
        "level": 2,
        "text": "5. Int8 Distance Functions",
        "id": "5-int8-distance-functions"
      },
      {
        "level": 3,
        "text": "Native (Fast) Int8 Distance",
        "id": "native-fast-int8-distance"
      },
      {
        "level": 3,
        "text": "euclideanint8(v1, v2)",
        "id": "euclideanint8v1-v2"
      },
      {
        "level": 3,
        "text": "cosineint8(v1, v2)",
        "id": "cosineint8v1-v2"
      },
      {
        "level": 3,
        "text": "dotint8(v1, v2)",
        "id": "dotint8v1-v2"
      },
      {
        "level": 3,
        "text": "manhattanint8(v1, v2)",
        "id": "manhattanint8v1-v2"
      },
      {
        "level": 2,
        "text": "6. Temporal Functions",
        "id": "6-temporal-functions"
      },
      {
        "level": 3,
        "text": "timenow()",
        "id": "timenow"
      },
      {
        "level": 3,
        "text": "timediff(t1, t2)",
        "id": "timedifft1-t2"
      },
      {
        "level": 3,
        "text": "timeadd(ts, durationms)",
        "id": "timeaddts-durationms"
      },
      {
        "level": 3,
        "text": "timesub(ts, durationms)",
        "id": "timesubts-durationms"
      },
      {
        "level": 3,
        "text": "timedecay(ts, now, halflifems)",
        "id": "timedecayts-now-halflifems"
      },
      {
        "level": 3,
        "text": "timedecaylinear(ts, now, maxagems)",
        "id": "timedecaylinearts-now-maxagems"
      },
      {
        "level": 3,
        "text": "timebefore(t1, t2)",
        "id": "timebeforet1-t2"
      },
      {
        "level": 3,
        "text": "timeafter(t1, t2)",
        "id": "timeaftert1-t2"
      },
      {
        "level": 3,
        "text": "timebetween(ts, start, end)",
        "id": "timebetweents-start-end"
      },
      {
        "level": 3,
        "text": "withinlast(ts, now, durationms)",
        "id": "withinlastts-now-durationms"
      },
      {
        "level": 3,
        "text": "intervalsoverlap(s1, e1, s2, e2)",
        "id": "intervalsoverlaps1-e1-s2-e2"
      },
      {
        "level": 3,
        "text": "intervalcontains(s1, e1, s2, e2)",
        "id": "intervalcontainss1-e1-s2-e2"
      },
      {
        "level": 3,
        "text": "intervalduration(start, end)",
        "id": "intervaldurationstart-end"
      },
      {
        "level": 3,
        "text": "pointininterval(ts, start, end)",
        "id": "pointinintervalts-start-end"
      },
      {
        "level": 2,
        "text": "7. Math Functions",
        "id": "7-math-functions"
      },
      {
        "level": 3,
        "text": "abs(x)",
        "id": "absx"
      },
      {
        "level": 3,
        "text": "absint64(x)",
        "id": "absint64x"
      },
      {
        "level": 3,
        "text": "absfloat64(x)",
        "id": "absfloat64x"
      },
      {
        "level": 3,
        "text": "sqrt(x)",
        "id": "sqrtx"
      },
      {
        "level": 3,
        "text": "pow(base, exp)",
        "id": "powbase-exp"
      },
      {
        "level": 3,
        "text": "log(x)",
        "id": "logx"
      },
      {
        "level": 3,
        "text": "exp(x)",
        "id": "expx"
      },
      {
        "level": 3,
        "text": "sin(x)",
        "id": "sinx"
      },
      {
        "level": 3,
        "text": "cos(x)",
        "id": "cosx"
      },
      {
        "level": 3,
        "text": "tan(x)",
        "id": "tanx"
      },
      {
        "level": 3,
        "text": "floor(x)",
        "id": "floorx"
      },
      {
        "level": 3,
        "text": "ceil(x)",
        "id": "ceilx"
      },
      {
        "level": 3,
        "text": "sign(x)",
        "id": "signx"
      },
      {
        "level": 2,
        "text": "8. String Functions",
        "id": "8-string-functions"
      },
      {
        "level": 3,
        "text": "len(s)",
        "id": "lens"
      },
      {
        "level": 3,
        "text": "upper(s)",
        "id": "uppers"
      },
      {
        "level": 3,
        "text": "lower(s)",
        "id": "lowers"
      },
      {
        "level": 3,
        "text": "trim(s)",
        "id": "trims"
      },
      {
        "level": 3,
        "text": "substr(s, start, len)",
        "id": "substrs-start-len"
      },
      {
        "level": 3,
        "text": "replace(s, find, replacement)",
        "id": "replaces-find-replacement"
      },
      {
        "level": 3,
        "text": "concat(s1, s2, ...)",
        "id": "concats1-s2-"
      },
      {
        "level": 2,
        "text": "9. Scalar Min/Max Functions",
        "id": "9-scalar-minmax-functions"
      },
      {
        "level": 3,
        "text": "minval(a, b)",
        "id": "minvala-b"
      },
      {
        "level": 3,
        "text": "maxval(a, b)",
        "id": "maxvala-b"
      },
      {
        "level": 2,
        "text": "Appendix: Function Quick Reference",
        "id": "appendix-function-quick-reference"
      }
    ]
  },
  "reference/index": {
    "title": "Reference Overview",
    "content": "\n# Reference\n\nComplete reference documentation for InputLayer.\n\n- [Commands](/docs/reference/commands) — All REPL and meta commands\n- [Functions](/docs/reference/functions) — Built-in functions for math, strings, vectors, and more\n- [Syntax](/docs/reference/syntax) — Datalog syntax reference\n- [Syntax Cheatsheet](/docs/reference/syntax-cheatsheet) — Quick reference card\n- [Aggregations](/docs/reference/aggregations) — Aggregate functions (count, sum, avg, etc.)",
    "toc": []
  },
  "reference/syntax-cheatsheet": {
    "title": "InputLayer Datalog Cheatsheet",
    "content": "# InputLayer Datalog Cheatsheet\n\nA unified Datalog-native syntax for InputLayer, designed for intuitive data manipulation and querying.\n\n## Quick Reference\n\n| Operator | Meaning | Persisted | DD Semantics |\n|----------|---------|-----------|--------------|\n| `+` | Insert fact or persistent rule | Yes | diff = +1 |\n| `-` | Delete fact or drop rule | Yes | diff = -1 |\n| `<-` (no `+`) | Session rule (transient) | No | Ad-hoc computation |\n| `?` | Query | No | Ad-hoc query |\n\n## Key Terminology\n\n| Term | Description |\n|------|-------------|\n| **Fact** | Base data stored in a relation (e.g., `+edge(1, 2)`) |\n| **Rule** | Derived relation defined by a Datalog rule (persistent) |\n| **Session Rule** | Transient rule that exists only for current session |\n| **Schema** | Type definition for a relation's columns |\n| **Query** | One-shot question against facts and rules |\n| **Knowledge Graph** | Isolated namespace containing relations, rules, and indexes |\n\n## Meta Commands\n\nMeta commands start with `.` and control the system:\n\n```\n.kg                  Show current knowledge graph\n.kg list             List all knowledge graphs\n.kg create <name>    Create knowledge graph\n.kg use <name>       Switch to knowledge graph\n.kg drop <name>      Drop knowledge graph (cannot drop current)\n\n.rel                 List relations (base facts)\n.rel <name>          Describe relation schema\n.rel drop <name>     Drop a relation and its data\n\n.rule                List persistent rules\n.rule <name>         Query rule (show computed data)\n.rule def <name>     Show rule definition\n.rule drop <name>    Drop all clauses of a rule\n.rule remove <name> <n>  Remove clause #n from rule (1-based)\n.rule clear <name>   Clear all clauses for re-registration\n\n.session             List session rules\n.session clear       Clear all session rules\n.session drop <n>    Remove session rule #n\n\n.index               List all indexes\n.index list          List all indexes (explicit)\n.index create <name> on <relation>(<column>) [options]\n.index drop <name>   Drop an index\n.index stats <name>  Show index statistics\n.index rebuild <name>  Rebuild an index\n\n.load <file>         Load and execute a .idl file\n\n.user list           List all users\n.user create <name> <password> <role>  Create user\n.user drop <name>    Delete user\n.user password <name> <password>       Change password\n.user role <name> <role>               Change role\n\n.apikey create <label>   Create API key\n.apikey list             List API keys\n.apikey revoke <label>   Revoke API key\n\n.kg acl list [<kg>]              List ACLs\n.kg acl grant <kg> <user> <role> Grant access\n.kg acl revoke <kg> <user>       Revoke access\n\n.explain <query>     Show query plan without executing\n.compact             Compact WAL and consolidate batch files\n.status              Show system status\n.help                Show this help\n.quit                Exit client\n```\n\n### Index Creation Options\n\n```\n.index create vec_idx on docs(embedding) type hnsw metric cosine m 16 ef_construction 200\n```\n\nOptions: `type <hnsw>`, `metric <cosine|euclidean|dot_product|manhattan>`, `m <N>`, `ef_construction <N>`, `ef_search <N>`\n\n`.idx` is an alias for `.index`.\n\n## Data Manipulation\n\n### Insert Facts (`+`)\n\nSingle fact:\n```datalog\n+edge(1, 2)\n```\n\nBulk insert:\n```datalog\n+edge[(1, 2), (2, 3), (3, 4)]\n```\n\n### Delete Facts (`-`)\n\nSingle fact:\n```datalog\n-edge(1, 2)\n```\n\nConditional delete (query-based):\n```datalog\n-edge(X, Y) <- edge(X, Y), X > 5\n```\n\n### Updates (Delete then Insert)\n\nTo update data, delete the old value then insert the new:\n```datalog\n// Delete old value\n-counter(1, 0)\n// Insert new value\n+counter(1, 5)\n```\n\n## Persistent Rules (`+head <- body`)\n\nPersistent rules are saved to disk and incrementally maintained by Differential Dataflow.\n\nSimple rule:\n```datalog\n+path(X, Y) <- edge(X, Y)\n```\n\nRecursive rule (transitive closure):\n```datalog\n+path(X, Y) <- edge(X, Y)\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n```\n\nRule with filter:\n```datalog\n+adult(Name, Age) <- person(Name, Age), Age >= 18\n```\n\nRule with computed head variable:\n```datalog\n+doubled(X, Y) <- nums(X), Y = X * 2\n```\n\nRules are saved to `{kg_dir}/rules/catalog.json` and automatically loaded on knowledge graph startup.\n\n## Session Rules (`<-`)\n\nSession rules are executed immediately but not persisted. They're useful for ad-hoc analysis:\n\n```datalog\nresult(X, Y) <- edge(X, Y), X < Y\n```\n\nSession rules can reference persistent rules:\n```datalog\nreachable_from_one(X) <- path(1, X)\n```\n\nMultiple session rules accumulate and evaluate together:\n```datalog\nfoo(X, Y) <- bar(X, Y)\nfoo(X, Z) <- foo(X, Y), foo(Y, Z)  // Adds to previous rule\n```\n\n## Queries (`?`)\n\nQuery a relation or rule:\n\nSimple query:\n```datalog\n? edge(1, X)\n```\n\nQuery with constraints:\n```datalog\n? person(Name, Age), Age > 30\n```\n\nQuery a derived relation:\n```datalog\n? path(1, X)\n```\n\n## Schema Declarations\n\nDefine typed relations:\n\n```datalog\n+employee(id: int, name: string, dept_id: int)\n+user(id: int, email: string, name: string)\n```\n\n## Aggregations\n\n```datalog\n+total_sales(Dept, sum<Amount>) <- sales(Dept, _, Amount)\n+employee_count(Dept, count<Id>) <- employee(Id, _, Dept)\n+max_salary(Dept, max<Salary>) <- employee(_, Salary, Dept)\n+min_age(min<Age>) <- person(_, Age)\n+avg_score(avg<Score>) <- test_results(_, Score)\n```\n\nSupported aggregates: `count`, `sum`, `min`, `max`, `avg`, `count_distinct`, `top_k`, `top_k_threshold`.\n\n### TopK Example\n```datalog\n+top_scores(Name, top_k<3, Score, desc>) <- scores(Name, Score)\n```\n\n## Builtin Functions\n\n### Distance Functions\n```datalog\nDist = euclidean(V1, V2)    // Euclidean (L2) distance\nDist = cosine(V1, V2)       // Cosine distance (1 - similarity)\nScore = dot(V1, V2)         // Dot product\nDist = manhattan(V1, V2)    // Manhattan (L1) distance\n```\n\n### Vector Operations\n```datalog\nNormV = normalize(V)        // Unit vector\nDim = vec_dim(V)            // Vector dimension\nSum = vec_add(V1, V2)       // Element-wise addition\nScaled = vec_scale(V, S)    // Scalar multiplication\n```\n\n### Math Functions\n```datalog\nA = abs(X)                  // Absolute value\nR = sqrt(X)                 // Square root\nR = pow(Base, Exp)          // Power\nR = log(X)                  // Natural logarithm\nR = exp(X)                  // Exponential (e^x)\nR = sin(X)                  // Sine (radians)\nR = cos(X)                  // Cosine (radians)\nR = tan(X)                  // Tangent (radians)\nR = floor(X)                // Floor\nR = ceil(X)                 // Ceiling\nS = sign(X)                 // Sign (-1, 0, 1)\n```\n\n### String Functions\n```datalog\nL = len(S)                  // String length\nU = upper(S)                // To uppercase\nL = lower(S)                // To lowercase\nT = trim(S)                 // Trim whitespace\nSub = substr(S, Start, Len) // Substring\nR = replace(S, Find, Repl)  // Replace all\nR = concat(S1, S2)          // Concatenate\n```\n\n### Temporal Functions\n```datalog\nNow = time_now()            // Current Unix timestamp (ms)\nDiff = time_diff(T1, T2)   // Timestamp difference\nNew = time_add(Ts, Dur)     // Add duration\nNew = time_sub(Ts, Dur)     // Subtract duration\nW = time_decay(Ts, Now, HL) // Exponential decay\n```\n\n### Scalar Min/Max\n```datalog\nM = min_val(A, B)           // Smaller of two values\nM = max_val(A, B)           // Larger of two values\n```\n\nSee [functions](functions) for the complete reference (55 functions).\n\n## Vector Operations Example\n\n```datalog\n// Insert vectors\n+vectors[(1, [1.0, 0.0, 0.0]), (2, [0.0, 1.0, 0.0])]\n\n// Query with distance computation\n? vectors(Id1, V1), vectors(Id2, V2), Id1 < Id2,\n   Dist = euclidean(V1, V2), Dist < 1.0\n\n// Persistent rule with vector computation\n+similarity(Id1, Id2, Score) <-\n    vectors(Id1, V1), vectors(Id2, V2),\n    Id1 < Id2,\n    Score = cosine(V1, V2)\n```\n\n## Examples\n\n### Social Graph\n\n```datalog\n// Create knowledge graph\n.kg create social\n.kg use social\n\n// Add edges\n+follows[(1, 2), (2, 3), (3, 4), (1, 4)]\n\n// Define reachability rule (persistent)\n+reach(X, Y) <- follows(X, Y)\n+reach(X, Z) <- reach(X, Y), follows(Y, Z)\n\n// Query who user 1 can reach\n? reach(1, X)\n```\n\n### Access Control (RBAC)\n\n```datalog\n.kg create acl\n.kg use acl\n\n// Facts: users, roles, permissions\n+user_role[(\"alice\", \"admin\"), (\"bob\", \"viewer\")]\n+role_permission[(\"admin\", \"read\"), (\"admin\", \"write\"), (\"viewer\", \"read\")]\n\n// Rule: user has permission if they have a role with that permission\n+has_permission(User, Perm) <-\n  user_role(User, Role),\n  role_permission(Role, Perm)\n\n// Query: what can alice do?\n? has_permission(\"alice\", Perm)\n```\n\n### Policy-First RAG\n\n```datalog\n.kg create rag\n.kg use rag\n\n// Facts\n+member[(\"alice\", \"engineering\"), (\"bob\", \"sales\")]\n+doc[(101, \"Design Doc\"), (102, \"Sales Pitch\")]\n+acl[(\"engineering\", 101), (\"sales\", 102)]\n+emb[(101, [1.0, 0.0]), (102, [0.0, 1.0])]\n\n// Rule: user can access docs via group membership\n+can_access(User, DocId) <- member(User, Group), acl(Group, DocId)\n\n// Query: what can alice retrieve, with similarity score?\n? can_access(\"alice\", DocId), emb(DocId, V),\n   Sim = cosine(V, [0.9, 0.1]), Sim > 0.5\n```\n\n## Differential Dataflow Semantics\n\nInputLayer is built on Differential Dataflow (DD), which uses a diff-based model:\n\n- `+fact` sends `(fact, time, +1)` to DD\n- `-fact` sends `(fact, time, -1)` to DD\n- Rules are incrementally maintained using DD's `iterate()` operator\n- Queries are executed using DD's dataflow operators\n\nThe persistence layer stores `(data, time, diff)` triples, enabling:\n- Efficient incremental updates\n- Crash recovery with WAL (Write-Ahead Log)\n\n## Architecture\n\n```\nStatement Parser     ->  Statement enum (facts, rules, queries)\n       |\nStorage Engine       ->  Multi-knowledge-graph management\n       |\nRule Catalog         ->  Persistent rule definitions (JSON)\n       |\nDatalog Engine       ->  DD-based execution\n       |\nPersist Layer        ->  WAL + batched Parquet storage\n```\n\nSee [architecture](../internals/architecture) for the complete architecture reference.\n\n## File Locations\n\n- Config: `./config.toml` or `--config <path>`\n- Data: `{data_dir}/{knowledge_graph}/`\n- Rules: `{data_dir}/{knowledge_graph}/rules/catalog.json`\n- Persist: `{data_dir}/persist/{knowledge_graph}:{relation}/`",
    "toc": [
      {
        "level": 2,
        "text": "Quick Reference",
        "id": "quick-reference"
      },
      {
        "level": 2,
        "text": "Key Terminology",
        "id": "key-terminology"
      },
      {
        "level": 2,
        "text": "Meta Commands",
        "id": "meta-commands"
      },
      {
        "level": 3,
        "text": "Index Creation Options",
        "id": "index-creation-options"
      },
      {
        "level": 2,
        "text": "Data Manipulation",
        "id": "data-manipulation"
      },
      {
        "level": 3,
        "text": "Insert Facts (+)",
        "id": "insert-facts-"
      },
      {
        "level": 3,
        "text": "Delete Facts (-)",
        "id": "delete-facts-"
      },
      {
        "level": 3,
        "text": "Updates (Delete then Insert)",
        "id": "updates-delete-then-insert"
      },
      {
        "level": 2,
        "text": "Persistent Rules (+head <- body)",
        "id": "persistent-rules-head-body"
      },
      {
        "level": 2,
        "text": "Session Rules (<-)",
        "id": "session-rules-"
      },
      {
        "level": 2,
        "text": "Queries (?)",
        "id": "queries-"
      },
      {
        "level": 2,
        "text": "Schema Declarations",
        "id": "schema-declarations"
      },
      {
        "level": 2,
        "text": "Aggregations",
        "id": "aggregations"
      },
      {
        "level": 3,
        "text": "TopK Example",
        "id": "topk-example"
      },
      {
        "level": 2,
        "text": "Builtin Functions",
        "id": "builtin-functions"
      },
      {
        "level": 3,
        "text": "Distance Functions",
        "id": "distance-functions"
      },
      {
        "level": 3,
        "text": "Vector Operations",
        "id": "vector-operations"
      },
      {
        "level": 3,
        "text": "Math Functions",
        "id": "math-functions"
      },
      {
        "level": 3,
        "text": "String Functions",
        "id": "string-functions"
      },
      {
        "level": 3,
        "text": "Temporal Functions",
        "id": "temporal-functions"
      },
      {
        "level": 3,
        "text": "Scalar Min/Max",
        "id": "scalar-minmax"
      },
      {
        "level": 2,
        "text": "Vector Operations Example",
        "id": "vector-operations-example"
      },
      {
        "level": 2,
        "text": "Examples",
        "id": "examples"
      },
      {
        "level": 3,
        "text": "Social Graph",
        "id": "social-graph"
      },
      {
        "level": 3,
        "text": "Access Control (RBAC)",
        "id": "access-control-rbac"
      },
      {
        "level": 3,
        "text": "Policy-First RAG",
        "id": "policy-first-rag"
      },
      {
        "level": 2,
        "text": "Differential Dataflow Semantics",
        "id": "differential-dataflow-semantics"
      },
      {
        "level": 2,
        "text": "Architecture",
        "id": "architecture"
      },
      {
        "level": 2,
        "text": "File Locations",
        "id": "file-locations"
      }
    ]
  },
  "reference/syntax": {
    "title": "Syntax Reference",
    "content": "# Syntax Reference\n\nComplete syntax reference for InputLayer's Datalog dialect.\n\n## Grammar Overview\n\n```\nprogram     ::= statement*\nstatement   ::= fact | rule | query | meta_command | schema_decl\n\nfact        ::= '+' atom\n              | '+' relation '[' tuple_list ']'\n              | '-' atom\n\nrule        ::= '+' atom '<-' body           // Persistent rule\n              | atom '<-' body               // Session rule\n\nquery       ::= '?' body\n\nbody        ::= goal (',' goal)*\ngoal        ::= atom | constraint | negated_atom\n\natom        ::= relation '(' term_list ')'\nnegated_atom::= '!' atom\n\nterm        ::= variable | constant | expression\nterm_list   ::= term (',' term)*\n\nconstraint  ::= term op term\nop          ::= '=' | '!=' | '<' | '<=' | '>' | '>='\n```\n\n## Lexical Elements\n\n### Identifiers\n\n| Type | Pattern | Examples |\n|------|---------|----------|\n| Relation | `[a-z][a-z0-9_]*` | `edge`, `my_relation`, `path2` |\n| Variable | `[A-Z][a-zA-Z0-9_]*` | `X`, `Name`, `PersonId` |\n| Placeholder | `_` | Matches any value, discarded |\n\n### Literals\n\n#### Integers\n```datalog\n42        // Decimal\n-17       // Negative\n0         // Zero\n```\n\n#### Floats\n```datalog\n3.14      // Decimal float\n-0.5      // Negative float\n1.0e10    // Scientific notation\n```\n\n#### Strings\n```datalog\n\"hello\"           // Basic string\n\"hello world\"     // With spaces\n\"line1\\nline2\"    // Escape sequences\n\"say \\\"hi\\\"\"      // Escaped quotes\n```\n\n**Escape sequences:**\n| Sequence | Meaning |\n|----------|---------|\n| `\\n` | Newline |\n| `\\t` | Tab |\n| `\\\\` | Backslash |\n| `\\\"` | Quote |\n\n#### Vectors\n```datalog\n[1.0, 2.0, 3.0]           // Float vector\n[0.1, 0.2, 0.3, 0.4]      // Embedding\n```\n\n### Comments\n\n```datalog\n// Single line comment\n\n/* Multi-line\n   block comment */\n\n+edge(1, 2)  // Inline comment\n```\n\n## Statements\n\n### Fact Insertion (`+`)\n\nInsert base data into relations.\n\n**Single fact:**\n```datalog\n+edge(1, 2)\n+person(\"alice\", 30)\n+location(\"NYC\", 40.7, -74.0)\n```\n\n**Bulk insert:**\n```datalog\n+edge[(1, 2), (2, 3), (3, 4), (4, 5)]\n+person[(\"alice\", 30), (\"bob\", 25), (\"carol\", 35)]\n```\n\n### Fact Deletion (`-`)\n\nRemove base data from relations.\n\n**Single fact:**\n```datalog\n-edge(1, 2)\n```\n\n**Conditional delete (based on another relation):**\n```datalog\n// Delete all edges where source node is in the 'banned' relation\n-edge(X, Y) <- banned(X)\n\n// Delete all edges where X is greater than 5\n-edge(X, Y) <- edge(X, Y), X > 5\n\n// Delete edges that form triangles\n-edge(X, Y) <- edge(X, Y), edge(Y, Z), edge(Z, X)\n```\n\n**Note:** Conditional delete finds all tuples matching the condition and removes them from the target relation. The target relation is automatically included in the query body to bind all head variables.\n\n### Persistent Rules (`+head <- body`)\n\nDefine derived relations that persist across sessions.\n\n```datalog\n// Simple derivation\n+adult(Name, Age) <- person(Name, Age), Age >= 18\n\n// Join\n+works_in(Name, Building) <-\n  employee(Name, Dept),\n  department(Dept, Building)\n\n// Recursive\n+path(X, Y) <- edge(X, Y)\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n\n// With negation\n+orphan(X) <- person(X), !parent(_, X)\n\n// With aggregation\n+dept_size(Dept, count<Emp>) <- employee(Emp, Dept)\n```\n\n### Session Rules (no `+` prefix)\n\nTransient rules that exist only for the current session.\n\n```datalog\n// Not persisted\ntemp(X, Y) <- edge(X, Y), X < Y\ndebug(X) <- some_complex_condition(X)\n```\n\n### Queries (`?`)\n\nAsk questions about the data.\n\n```datalog\n// Simple query\n? edge(X, Y)\n\n// With constants\n? edge(1, X)\n\n// With constraints\n? person(Name, Age), Age > 25\n\n// Join query\n? person(Id, Name, _, _), purchase(Id, Item, _)\n```\n\n### Schema Declarations\n\nDefine typed schemas for relations.\n\n```datalog\n// Basic schema\n+employee(id: int, name: string, dept: string)\n\n// All types\n+example(\n  a: int,\n  b: float,\n  c: string,\n  d: vector\n)\n```\n\n**Supported types:**\n| Type | Description | Example Values |\n|------|-------------|----------------|\n| `int` | 64-bit integer | `42`, `-17`, `0` |\n| `float` | 64-bit float | `3.14`, `-0.5` |\n| `string` | UTF-8 string | `\"hello\"`, `\"world\"` |\n| `bool` | Boolean | `true`, `false` |\n| `timestamp` | Unix timestamp (ms) | Aliases: `time`, `datetime` |\n| `vector` | Float array | `[0.1, 0.2, 0.3]` |\n\n## Expressions\n\n### Arithmetic\n\n```datalog\n+computed(X, Y) <- input(A, B), X = A + B, Y = A * B\n```\n\n| Operator | Description |\n|----------|-------------|\n| `+` | Addition |\n| `-` | Subtraction |\n| `*` | Multiplication |\n| `/` | Division |\n| `%` | Modulo |\n\n### Comparison\n\n```datalog\n? person(Name, Age), Age >= 18, Age < 65\n```\n\n| Operator | Description |\n|----------|-------------|\n| `=` | Equal |\n| `!=` | Not equal |\n| `<` | Less than |\n| `<=` | Less or equal |\n| `>` | Greater than |\n| `>=` | Greater or equal |\n\n### Built-in Functions\n\n**Vector functions:**\n\nInputLayer provides built-in vector distance and similarity functions:\n\n```datalog\n// Compute cosine similarity between embeddings\n? embedding(Id1, V1), embedding(Id2, V2), Id1 < Id2,\n   Sim = cosine(V1, V2), Sim > 0.9\n\n// Compute Euclidean distance between points\n? point(Id1, V1), point(Id2, V2), Id1 < Id2,\n   Dist = euclidean(V1, V2), Dist < 1.0\n```\n\n| Function | Description |\n|----------|-------------|\n| `euclidean(v1, v2)` | Euclidean distance |\n| `cosine(v1, v2)` | Cosine similarity |\n| `dot(v1, v2)` | Dot product |\n| `manhattan(v1, v2)` | Manhattan distance |\n\nVector functions work in both query bodies and rule heads (via computed head variables). See [functions](functions) for the complete list (55 functions including vector, temporal, math, string, LSH, and quantization).\n\n## Aggregations\n\nCompute aggregate values over groups.\n\n### Syntax\n\n```datalog\n+result(GroupBy1, GroupBy2, agg<AggColumn>) <-\n  source(GroupBy1, GroupBy2, AggColumn, _)\n```\n\nVariables in the head that are not aggregated become group-by columns.\n\n### Aggregate Functions\n\n| Function | Description | Example |\n|----------|-------------|---------|\n| `count<X>` | Count rows | `count<Id>` |\n| `sum<X>` | Sum values | `sum<Amount>` |\n| `min<X>` | Minimum | `min<Age>` |\n| `max<X>` | Maximum | `max<Score>` |\n| `avg<X>` | Average | `avg<Salary>` |\n\n### Examples\n\n```datalog\n// Count per group\n+city_count(City, count<Id>) <- person(Id, _, _, City)\n\n// Sum\n+total_sales(Product, sum<Amount>) <- sale(_, Product, Amount)\n\n// Multiple aggregates (separate rules)\n+stats_min(min<Age>) <- person(_, _, Age, _)\n+stats_max(max<Age>) <- person(_, _, Age, _)\n\n// Global aggregate (no group-by)\n+total(sum<Amount>) <- purchase(_, _, Amount)\n```\n\n## Negation\n\nExpress \"does not exist\" conditions.\n\n### Syntax\n\n```datalog\n!atom(args)     // Negated atom\n```\n\n### Rules\n\n1. **Safety**: Variables in negation must appear positively elsewhere\n2. **Stratification**: No circular dependencies through negation\n\n### Examples\n\n```datalog\n// People without purchases\n+non_buyer(Id, Name) <-\n  person(Id, Name, _, _),\n  !purchase(Id, _, _)\n\n// Nodes with no outgoing edges\n+sink(X) <- node(X), !edge(X, _)\n\n// Set difference\n+only_in_a(X) <- a(X), !b(X)\n```\n\n### Invalid (Unsafe)\n\n```datalog\n// WRONG - X only appears in negation\n+bad(X) <- !some_rel(X)\n\n// CORRECT - X appears positively\n+good(X) <- domain(X), !some_rel(X)\n```\n\n## Meta Commands\n\nCommands that control the REPL environment.\n\n### Knowledge Graph Commands\n\n```datalog\n.kg                     // Show current knowledge graph\n.kg list                // List all knowledge graphs\n.kg create <name>       // Create knowledge graph\n.kg use <name>          // Switch to knowledge graph\n.kg drop <name>         // Delete knowledge graph\n```\n\n### Relation Commands\n\n```datalog\n.rel                    // List relations with data\n.rel <name>             // Show schema and sample data\n.rel drop <name>        // Drop a relation and its data\n```\n\n### Rule Commands\n\n```datalog\n.rule                   // List all rules\n.rule <name>            // Query a rule\n.rule def <name>        // Show rule definition\n.rule drop <name>       // Delete all clauses of a rule\n.rule remove <name> <n> // Remove clause #n (1-based)\n.rule clear <name>      // Clear for re-registration\n.rule edit <name> <n> <clause>  // Edit clause\n```\n\n### Session Commands\n\n```datalog\n.session                // List session rules\n.session clear          // Clear all session rules\n.session drop <n>       // Remove session rule #n\n```\n\n### File Commands\n\n```datalog\n.load <file>            // Execute a .idl file\n```\n\n### Index Commands\n\n```datalog\n.index                  // List all indexes\n.index list             // List all indexes (explicit)\n.index create <name> on <relation>(<column>) [options]\n.index drop <name>      // Drop an index\n.index stats <name>     // Show index statistics\n.index rebuild <name>   // Rebuild an index\n```\n\n### User & Auth Commands\n\n```datalog\n.user list              // List all users\n.user create <name> <password> <role>  // Create user\n.user drop <name>       // Delete user\n.user password <name> <password>       // Change password\n.user role <name> <role>               // Change role\n.apikey create <label>  // Create API key\n.apikey list            // List API keys\n.apikey revoke <label>  // Revoke API key\n.kg acl list [<kg>]     // List ACLs\n.kg acl grant <kg> <user> <role>  // Grant access\n.kg acl revoke <kg> <user>        // Revoke access\n```\n\n### System Commands\n\n```datalog\n.status                 // System status\n.compact                // Compact storage\n.explain <query>        // Show query plan\n.help                   // Help message\n.quit                   // Exit REPL\n.exit                   // Exit REPL (alias)\n```\n\n## Recursion\n\n### Basic Pattern\n\n```datalog\n// Base case\n+derived(X, Y) <- base(X, Y)\n\n// Recursive case\n+derived(X, Z) <- derived(X, Y), base(Y, Z)\n```\n\n### Transitive Closure\n\n```datalog\n+reachable(X, Y) <- edge(X, Y)\n+reachable(X, Z) <- reachable(X, Y), edge(Y, Z)\n```\n\n### Mutual Recursion\n\n```datalog\n+odd(X) <- edge(Start, X), start(Start)\n+odd(X) <- even(Y), edge(Y, X)\n+even(X) <- odd(Y), edge(Y, X)\n```\n\n### Restrictions\n\n- Negation through recursion is not allowed (non-stratifiable)\n- Must have a base case that terminates\n\n## Complete Examples\n\n### Social Network\n\n```datalog\n// Schema\n+person(id: int, name: string, age: int)\n+follows(follower: int, followed: int)\n\n// Data\n+person[(1, \"alice\", 30), (2, \"bob\", 25), (3, \"carol\", 35)]\n+follows[(1, 2), (2, 3), (1, 3)]\n\n// Rules\n+mutual_follow(A, B) <-\n  follows(A, B),\n  follows(B, A),\n  A < B\n\n+influencer(Id, count<Follower>) <-\n  follows(Follower, Id)\n\n// Query\n? influencer(Id, Count), Count > 10\n```\n\n### Graph Analysis\n\n```datalog\n// Transitive closure\n+path(X, Y) <- edge(X, Y)\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n\n// Cycle detection\n+in_cycle(X) <- path(X, X)\n\n// Connected components (undirected)\n+bidir(X, Y) <- edge(X, Y)\n+bidir(X, Y) <- edge(Y, X)\n+connected(X, Y) <- bidir(X, Y)\n+connected(X, Z) <- connected(X, Y), bidir(Y, Z)\n\n// Sink nodes (no outgoing)\n+sink(X) <- node(X), !edge(X, _)\n\n// Source nodes (no incoming)\n+source(X) <- node(X), !edge(_, X)\n```\n\n### Bill of Materials\n\n```datalog\n// Part hierarchy\n+contains(assembly: int, part: int, qty: int)\n\n// All parts needed (recursive)\n+requires(Asm, Part) <- contains(Asm, Part, _)\n+requires(Asm, Part) <-\n  contains(Asm, Sub, _),\n  requires(Sub, Part)\n\n// Total quantity calculation\n+total_qty(Asm, Part, sum<Qty>) <-\n  contains(Asm, Part, Qty)\n```\n\n## Reserved Words\n\nThe following are reserved and cannot be used as relation names:\n\n```\ntrue, false\ncount, sum, min, max, avg\nint, float, string, vector\n```\n\n## File Format\n\nInputLayer files use the `.idl` extension and contain valid Datalog statements:\n\n```datalog\n// my_program.idl\n\n// Schema declarations\n+node(id: int, label: string)\n+edge(src: int, dst: int, weight: float)\n\n// Data\n+node[(1, \"a\"), (2, \"b\"), (3, \"c\")]\n+edge[(1, 2, 1.0), (2, 3, 2.0)]\n\n// Rules\n+path(X, Y, W) <- edge(X, Y, W)\n+path(X, Z, W) <- path(X, Y, W1), edge(Y, Z, W2), W = W1 + W2\n```\n\nLoad with:\n```datalog\n.load my_program.idl\n```",
    "toc": [
      {
        "level": 2,
        "text": "Grammar Overview",
        "id": "grammar-overview"
      },
      {
        "level": 2,
        "text": "Lexical Elements",
        "id": "lexical-elements"
      },
      {
        "level": 3,
        "text": "Identifiers",
        "id": "identifiers"
      },
      {
        "level": 3,
        "text": "Literals",
        "id": "literals"
      },
      {
        "level": 3,
        "text": "Comments",
        "id": "comments"
      },
      {
        "level": 2,
        "text": "Statements",
        "id": "statements"
      },
      {
        "level": 3,
        "text": "Fact Insertion (+)",
        "id": "fact-insertion-"
      },
      {
        "level": 3,
        "text": "Fact Deletion (-)",
        "id": "fact-deletion-"
      },
      {
        "level": 3,
        "text": "Persistent Rules (+head <- body)",
        "id": "persistent-rules-head-body"
      },
      {
        "level": 3,
        "text": "Session Rules (no + prefix)",
        "id": "session-rules-no-prefix"
      },
      {
        "level": 3,
        "text": "Queries (?)",
        "id": "queries-"
      },
      {
        "level": 3,
        "text": "Schema Declarations",
        "id": "schema-declarations"
      },
      {
        "level": 2,
        "text": "Expressions",
        "id": "expressions"
      },
      {
        "level": 3,
        "text": "Arithmetic",
        "id": "arithmetic"
      },
      {
        "level": 3,
        "text": "Comparison",
        "id": "comparison"
      },
      {
        "level": 3,
        "text": "Built-in Functions",
        "id": "built-in-functions"
      },
      {
        "level": 2,
        "text": "Aggregations",
        "id": "aggregations"
      },
      {
        "level": 3,
        "text": "Syntax",
        "id": "syntax"
      },
      {
        "level": 3,
        "text": "Aggregate Functions",
        "id": "aggregate-functions"
      },
      {
        "level": 3,
        "text": "Examples",
        "id": "examples"
      },
      {
        "level": 2,
        "text": "Negation",
        "id": "negation"
      },
      {
        "level": 3,
        "text": "Syntax",
        "id": "syntax"
      },
      {
        "level": 3,
        "text": "Rules",
        "id": "rules"
      },
      {
        "level": 3,
        "text": "Examples",
        "id": "examples"
      },
      {
        "level": 3,
        "text": "Invalid (Unsafe)",
        "id": "invalid-unsafe"
      },
      {
        "level": 2,
        "text": "Meta Commands",
        "id": "meta-commands"
      },
      {
        "level": 3,
        "text": "Knowledge Graph Commands",
        "id": "knowledge-graph-commands"
      },
      {
        "level": 3,
        "text": "Relation Commands",
        "id": "relation-commands"
      },
      {
        "level": 3,
        "text": "Rule Commands",
        "id": "rule-commands"
      },
      {
        "level": 3,
        "text": "Session Commands",
        "id": "session-commands"
      },
      {
        "level": 3,
        "text": "File Commands",
        "id": "file-commands"
      },
      {
        "level": 3,
        "text": "Index Commands",
        "id": "index-commands"
      },
      {
        "level": 3,
        "text": "User & Auth Commands",
        "id": "user-auth-commands"
      },
      {
        "level": 3,
        "text": "System Commands",
        "id": "system-commands"
      },
      {
        "level": 2,
        "text": "Recursion",
        "id": "recursion"
      },
      {
        "level": 3,
        "text": "Basic Pattern",
        "id": "basic-pattern"
      },
      {
        "level": 3,
        "text": "Transitive Closure",
        "id": "transitive-closure"
      },
      {
        "level": 3,
        "text": "Mutual Recursion",
        "id": "mutual-recursion"
      },
      {
        "level": 3,
        "text": "Restrictions",
        "id": "restrictions"
      },
      {
        "level": 2,
        "text": "Complete Examples",
        "id": "complete-examples"
      },
      {
        "level": 3,
        "text": "Social Network",
        "id": "social-network"
      },
      {
        "level": 3,
        "text": "Graph Analysis",
        "id": "graph-analysis"
      },
      {
        "level": 3,
        "text": "Bill of Materials",
        "id": "bill-of-materials"
      },
      {
        "level": 2,
        "text": "Reserved Words",
        "id": "reserved-words"
      },
      {
        "level": 2,
        "text": "File Format",
        "id": "file-format"
      }
    ]
  },
  "spec/basic-concepts": {
    "title": "Core Concepts",
    "content": "# Core Concepts\n\nThis section explains the fundamental building blocks of InputLayer Datalog.\n\n## Relations (Tables)\n\nA relation is a table that stores facts. Each relation has:\n- A **name** (lowercase, like `edge` or `person`)\n- A **schema** (the columns and their types)\n- **Facts** (the rows)\n\n```datalog\n// Create a relation by inserting facts\n+person(\"alice\", 30)\n+person(\"bob\", 25)\n```\n\nThis creates a `person` relation with 2 columns (name and age) and 2 rows.\n\n## Facts (Rows)\n\nA fact is a single row in a relation. Facts contain only values (no variables).\n\n```datalog\n// Insert facts\n+edge(1, 2)           // edge has two integer columns\n+user(\"alice\", true)  // user has string and boolean columns\n```\n\nFacts are stored persistently. When you restart InputLayer, your facts are still there.\n\n## Rules (Derived Tables)\n\nRules define new relations based on existing data. A rule has:\n- A **head** (the relation being defined)\n- A **body** (the conditions for deriving facts)\n\n```datalog\n// \"path exists from X to Y if edge exists from X to Y\"\n+path(X, Y) <- edge(X, Y)\n\n// \"path exists from X to Z if path exists from X to Y and edge from Y to Z\"\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n```\n\nThe head is `path(X, Y)`. The body is everything after `<-`.\n\nRules don't store data directly - they compute results from stored facts. When facts change, rule results update automatically.\n\n## Views\n\nA view is a relation defined by rules. In the example above, `path` is a view.\n\nViews are virtual tables:\n- They don't store data directly\n- Results are computed when queried\n- Updates are incremental (only changed parts recompute)\n\n## Queries\n\nA query asks a question about your data. It returns matching facts.\n\n```datalog\n// Find all edges starting from node 1\n?edge(1, X)\n\n// Find all people older than 25\n?person(Name, Age), Age > 25\n```\n\nQueries return results but don't modify any data.\n\n## Variables\n\nVariables start with an uppercase letter. They match any value.\n\n| Pattern | Matches |\n|---------|---------|\n| `edge(X, Y)` | All edges (both columns bound to variables) |\n| `edge(1, Y)` | Edges starting from node 1 |\n| `edge(X, X)` | Self-loops (same value in both columns) |\n| `edge(_, Y)` | All edges (ignore first column) |\n\nThe underscore `_` is a special variable that means \"I don't care about this value.\"\n\n## Constants\n\nConstants are literal values:\n\n| Type | Examples |\n|------|----------|\n| **Integer** | `1`, `-42`, `1000000` |\n| **Float** | `3.14`, `-0.5`, `1e10` |\n| **String** | `\"hello\"`, `\"alice\"`, `\"with spaces\"` |\n| **Boolean** | `true`, `false` |\n| **Vector** | `[1.0, 2.0, 3.0]` |\n\n## Atoms (Predicate Calls)\n\nAn atom is a relation name with arguments:\n\n```datalog\nedge(1, 2)              // Atom with two constants\nperson(Name, Age)       // Atom with two variables\nfriend(X, \"alice\")      // Atom with variable and constant\n```\n\nIn a rule body, atoms specify patterns to match. In a rule head, atoms specify what to produce.\n\n## Schema (Column Types)\n\nYou can declare a relation's schema before inserting data:\n\n```datalog\n// Declare schema with types\n+employee(id: int, name: string, salary: int)\n\n// Now insert data\n+employee(1, \"alice\", 50000)\n+employee(2, \"bob\", 60000)\n```\n\nSchema declarations are optional. Without a schema, InputLayer accepts any value types (schema-less mode).\n\n### Session vs Persistent Schemas\n\nLike rules, schemas can be session or persistent:\n\n```datalog\n// Persistent schema - saved with knowledge graph\n+user(id: int, email: string, name: string)\n\n// Session schema - only for current connection\nuser(id: int, email: string, name: string)\n```\n\n## Program Structure\n\nAn InputLayer session consists of:\n\n1. **Stored facts** - data in relations\n2. **Persistent rules** - saved rule definitions\n3. **Session rules** - temporary rules for this session\n4. **Queries** - questions you ask\n\n```datalog\n// 1. Insert facts (stored)\n+edge[(1, 2), (2, 3), (3, 4)]\n\n// 2. Define persistent rule (saved)\n+path(X, Y) <- edge(X, Y)\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n\n// 3. Define session rule (temporary)\nlong_path(X, Y) <- path(X, Y), X < Y\n\n// 4. Query\n?path(1, X)\n```\n\n## Knowledge Graphs\n\nA knowledge graph is a named container for relations and rules. It's like a database in SQL systems.\n\n```datalog\n.kg create social        // Create a knowledge graph\n.kg use social           // Switch to it\n+follows(1, 2)           // Data goes in 'social'\n.kg create work          // Create another\n.kg use work             // Switch to it\n+reports_to(1, 2)        // Data goes in 'work'\n```\n\nEach knowledge graph is isolated. Relations in `social` are separate from relations in `work`.",
    "toc": [
      {
        "level": 2,
        "text": "Relations (Tables)",
        "id": "relations-tables"
      },
      {
        "level": 2,
        "text": "Facts (Rows)",
        "id": "facts-rows"
      },
      {
        "level": 2,
        "text": "Rules (Derived Tables)",
        "id": "rules-derived-tables"
      },
      {
        "level": 2,
        "text": "Views",
        "id": "views"
      },
      {
        "level": 2,
        "text": "Queries",
        "id": "queries"
      },
      {
        "level": 2,
        "text": "Variables",
        "id": "variables"
      },
      {
        "level": 2,
        "text": "Constants",
        "id": "constants"
      },
      {
        "level": 2,
        "text": "Atoms (Predicate Calls)",
        "id": "atoms-predicate-calls"
      },
      {
        "level": 2,
        "text": "Schema (Column Types)",
        "id": "schema-column-types"
      },
      {
        "level": 3,
        "text": "Session vs Persistent Schemas",
        "id": "session-vs-persistent-schemas"
      },
      {
        "level": 2,
        "text": "Program Structure",
        "id": "program-structure"
      },
      {
        "level": 2,
        "text": "Knowledge Graphs",
        "id": "knowledge-graphs"
      }
    ]
  },
  "spec/errors": {
    "title": "Error Reference",
    "content": "# Error Reference\n\nThis section documents common errors you may encounter when using InputLayer.\n\n## Syntax Errors\n\n### Invalid Statement\n\n```\nError: Invalid statement: unexpected token\n```\n\nCheck that your statement follows correct syntax.\n\n### Parse Error\n\n```\nError: Parse error at line N\n```\n\nCommon causes:\n- Unbalanced parentheses\n- Invalid characters\n\n## Relation Errors\n\n### Undefined Relation\n\n```\nError: Undefined relation 'foo'\n```\n\nThe relation doesn't exist. Insert facts or check spelling:\n\n```datalog\n+foo(1, 2)        // Creates relation\n?foo(X, Y)        // Now works\n```\n\n### Arity Mismatch\n\n```\nError: Arity mismatch for 'edge': expected 2, got 3\n```\n\nYou provided the wrong number of columns:\n\n```datalog\n+edge(1, 2)       // edge has 2 columns\n+edge(1, 2, 3)    // ERROR: 3 values for 2-column relation\n```\n\n### Type Mismatch\n\n```\nError: Type mismatch: expected int, got string\n```\n\nValue types don't match the schema:\n\n```datalog\n+person(name: string, age: int)\n+person(30, \"alice\")   // ERROR: reversed types\n```\n\n### Insert into View\n\n```\nError: Cannot insert into view 'path'\n```\n\nYou can't insert facts into a derived relation (view):\n\n```datalog\n+path(X, Y) <- edge(X, Y)   // path is a view\n+path(1, 2)                  // ERROR: can't insert into view\n```\n\n## Rule Errors\n\n### Unsafe Variable\n\n```\nUnsafe rule 'bad': Head variable(s) X not bound by any positive body atom.\nAll head variables must appear in at least one positive body predicate.\n```\n\nAll head variables must appear in a positive body literal:\n\n```datalog\n// BAD: X not bound in body\n+bad(X) <- edge(A, B)\n\n// GOOD: X bound by edge\n+good(X) <- edge(X, _)\n```\n\n### Unsafe Negation Variable\n\n```\nUnsafe negation in rule 'bad': Variable(s) X in negated atom !excluded(...)\nmust be bound by a positive body atom. Range restriction violation.\n```\n\nVariables in negations must also appear in positive literals:\n\n```datalog\n// BAD: X only in negation\n+bad(X) <- !excluded(X)\n\n// GOOD: X bound first\n+good(X) <- items(X), !excluded(X)\n```\n\n### Unstratifiable Negation\n\n```\nUnstratified negation: Rule 'a' negates itself (!a in body). Self-negation is not supported.\n```\n\nA relation can't negatively depend on itself:\n\n```datalog\n// BAD: a depends on !a\n+a(X) <- b(X), !a(X)\n\n// GOOD: negate a different relation\n+a(X) <- b(X), !c(X)\n```\n\n## Arithmetic Errors\n\n### Division by Zero\n\n```\nError: Division by zero\n```\n\nCheck your data for zero divisors:\n\n```datalog\n// May error if Y = 0\n+ratio(X, R) <- data(X, Y), R = X / Y\n\n// Safe: filter out zeros\n+ratio(X, R) <- data(X, Y), Y != 0, R = X / Y\n```\n\n### Overflow\n\n```\nError: Integer overflow\n```\n\nResult exceeds 64-bit integer range.\n\n## Knowledge Graph Errors\n\n### Knowledge Graph Not Found\n\n```\nError: Knowledge graph 'foo' not found\n```\n\nThe knowledge graph doesn't exist:\n\n```\n.kg create foo    // Create it first\n.kg use foo       // Then use it\n```\n\n### Cannot Drop Current\n\n```\nError: Cannot drop current knowledge graph\n```\n\nSwitch to a different knowledge graph first:\n\n```\n.kg use other     // Switch away\n.kg drop target   // Now drop works\n```\n\n## Aggregation Errors\n\n### Invalid Aggregation Variable\n\n```\nError: Aggregation variable 'X' not found in body\n```\n\nThe aggregated variable must appear in the rule body:\n\n```datalog\n// BAD: Z not in body\n+bad(sum<Z>) <- data(X, Y)\n\n// GOOD: X appears in body\n+good(sum<X>) <- data(X, _)\n```\n\n## Connection Errors\n\n### Server Unavailable\n\n```\nError: Could not connect to server at http://127.0.0.1:8080\n```\n\nThe InputLayer server isn't running. Start it with:\n\n```bash\ninputlayer-server\n```\n\n### Request Timeout\n\n```\nError: Request timed out\n```\n\nThe query took too long. Try:\n- Adding filters to reduce data\n- Breaking into smaller queries\n- Increasing timeout in config",
    "toc": [
      {
        "level": 2,
        "text": "Syntax Errors",
        "id": "syntax-errors"
      },
      {
        "level": 3,
        "text": "Invalid Statement",
        "id": "invalid-statement"
      },
      {
        "level": 3,
        "text": "Parse Error",
        "id": "parse-error"
      },
      {
        "level": 2,
        "text": "Relation Errors",
        "id": "relation-errors"
      },
      {
        "level": 3,
        "text": "Undefined Relation",
        "id": "undefined-relation"
      },
      {
        "level": 3,
        "text": "Arity Mismatch",
        "id": "arity-mismatch"
      },
      {
        "level": 3,
        "text": "Type Mismatch",
        "id": "type-mismatch"
      },
      {
        "level": 3,
        "text": "Insert into View",
        "id": "insert-into-view"
      },
      {
        "level": 2,
        "text": "Rule Errors",
        "id": "rule-errors"
      },
      {
        "level": 3,
        "text": "Unsafe Variable",
        "id": "unsafe-variable"
      },
      {
        "level": 3,
        "text": "Unsafe Negation Variable",
        "id": "unsafe-negation-variable"
      },
      {
        "level": 3,
        "text": "Unstratifiable Negation",
        "id": "unstratifiable-negation"
      },
      {
        "level": 2,
        "text": "Arithmetic Errors",
        "id": "arithmetic-errors"
      },
      {
        "level": 3,
        "text": "Division by Zero",
        "id": "division-by-zero"
      },
      {
        "level": 3,
        "text": "Overflow",
        "id": "overflow"
      },
      {
        "level": 2,
        "text": "Knowledge Graph Errors",
        "id": "knowledge-graph-errors"
      },
      {
        "level": 3,
        "text": "Knowledge Graph Not Found",
        "id": "knowledge-graph-not-found"
      },
      {
        "level": 3,
        "text": "Cannot Drop Current",
        "id": "cannot-drop-current"
      },
      {
        "level": 2,
        "text": "Aggregation Errors",
        "id": "aggregation-errors"
      },
      {
        "level": 3,
        "text": "Invalid Aggregation Variable",
        "id": "invalid-aggregation-variable"
      },
      {
        "level": 2,
        "text": "Connection Errors",
        "id": "connection-errors"
      },
      {
        "level": 3,
        "text": "Server Unavailable",
        "id": "server-unavailable"
      },
      {
        "level": 3,
        "text": "Request Timeout",
        "id": "request-timeout"
      }
    ]
  },
  "spec/grammar": {
    "title": "Grammar Reference",
    "content": "# Grammar Reference\n\nThis section describes the complete syntax of InputLayer Datalog.\n\n## Character Encoding\n\nInputLayer programs are UTF-8 encoded text. You can use Unicode characters in string literals.\n\n## Whitespace\n\nSpaces, tabs, and newlines separate tokens. Extra whitespace is ignored:\n\n```datalog\n+edge(1, 2)           // Fine\n+edge( 1 , 2 )        // Also fine\n+edge(1,2)            // Also fine\n```\n\nWhitespace inside strings is preserved:\n\n```datalog\n+message(\"hello world\")  // String contains space\n```\n\n## Comments\n\nTwo styles of comments are supported:\n\n```datalog\n// Line comment: everything after // is ignored\n\n/* Block comment:\n   spans multiple lines\n   until closing */\n\n+edge(1, 2)  // Inline comment after statement\n```\n\nComments cannot be nested:\n```datalog\n/* outer /* inner */ still in outer? */  // Error!\n```\n\n## Statement Types\n\n| Statement | Syntax | Purpose |\n|-----------|--------|---------|\n| Insert fact | `+relation(args)` | Add data |\n| Bulk insert | `+relation[(tuple), ...]` | Add multiple facts |\n| Delete fact | `-relation(args)` | Remove data |\n| Conditional delete | `-relation(vars) <- body` | Remove matching data |\n| Persistent rule | `+head(vars) <- body` | Define saved rule |\n| Session rule | `head(vars) <- body` | Define temporary rule |\n| Query | `?pattern` | Retrieve data |\n| Schema | `+relation(col: type, ...)` | Declare structure |\n| Meta command | `.command args` | System operations |\n\n## Grammar Sections\n\nThe grammar is organized into these sections:\n\n- **[Rules](rules)** - Session and persistent rules\n- **[Queries](queries)** - Querying data\n- **[Types](types)** - Integers, strings, floats, vectors\n- **[Syntax](../reference/syntax)** - Complete syntax reference\n\n## EBNF Notation\n\nThis document uses EBNF notation for grammar rules:\n\n```ebnf\nsymbol ::= expression ;\n```\n\n| Notation | Meaning |\n|----------|---------|\n| `\"text\"` | Literal text |\n| `A B` | A followed by B |\n| `A \\| B` | A or B |\n| `A?` | Optional (zero or one) |\n| `A*` | Zero or more |\n| `A+` | One or more |\n| `( A B )` | Grouping |\n| `[a-z]` | Character range |\n\n## Quick Syntax Reference\n\n### Facts\n\n```ebnf\ninsert_fact  ::= \"+\" predicate \"(\" term (\",\" term)* \")\" ;\nbulk_insert  ::= \"+\" predicate \"[\" tuple (\",\" tuple)* \"]\" ;\ndelete_fact  ::= \"-\" predicate \"(\" term (\",\" term)* \")\" ;\ntuple        ::= \"(\" term (\",\" term)* \")\" ;\n```\n\n### Rules\n\n```ebnf\npersistent_rule ::= \"+\" head \"<-\" body ;\nsession_rule    ::= head \"<-\" body ;\nhead            ::= predicate \"(\" term (\",\" term)* \")\" ;\nbody            ::= literal (\",\" literal)* ;\n```\n\n### Queries\n\n```ebnf\nquery ::= \"?\" body ;\n```\n\n### Terms\n\n```ebnf\nterm      ::= variable | constant | placeholder ;\nvariable  ::= [A-Z] [a-zA-Z0-9_]* ;\nplaceholder ::= \"_\" ;\nconstant  ::= integer | float | string | boolean | vector ;\n```\n\n### Predicates\n\n```ebnf\npredicate ::= [a-z] [a-zA-Z0-9_]* ;\n```\n\nPredicates (relation names) must start with a lowercase letter.\n\n## Extensions\n\nInputLayer extends standard Datalog with:\n\n- **Aggregations** - count, sum, min, max, avg, top_k\n- **Vectors** - Array literals and 55 builtin functions\n- **Meta Commands** - System operations (`.kg`, `.rel`, `.rule`)\n- **Typed Schemas** - Type declarations for relations\n\nSee [Reference Documentation](../reference/) for complete details.",
    "toc": [
      {
        "level": 2,
        "text": "Character Encoding",
        "id": "character-encoding"
      },
      {
        "level": 2,
        "text": "Whitespace",
        "id": "whitespace"
      },
      {
        "level": 2,
        "text": "Comments",
        "id": "comments"
      },
      {
        "level": 2,
        "text": "Statement Types",
        "id": "statement-types"
      },
      {
        "level": 2,
        "text": "Grammar Sections",
        "id": "grammar-sections"
      },
      {
        "level": 2,
        "text": "EBNF Notation",
        "id": "ebnf-notation"
      },
      {
        "level": 2,
        "text": "Quick Syntax Reference",
        "id": "quick-syntax-reference"
      },
      {
        "level": 3,
        "text": "Facts",
        "id": "facts"
      },
      {
        "level": 3,
        "text": "Rules",
        "id": "rules"
      },
      {
        "level": 3,
        "text": "Queries",
        "id": "queries"
      },
      {
        "level": 3,
        "text": "Terms",
        "id": "terms"
      },
      {
        "level": 3,
        "text": "Predicates",
        "id": "predicates"
      },
      {
        "level": 2,
        "text": "Extensions",
        "id": "extensions"
      }
    ]
  },
  "spec/index": {
    "title": "Specification Overview",
    "content": "\n# InputLayer Specification\n\nFormal specification of InputLayer's Datalog dialect.\n\n- [Basic Concepts](/docs/spec/basic-concepts) — Foundations: relations, tuples, and knowledge graphs\n- [Types](/docs/spec/types) — Type system: int, float, string, bool, list, vectors\n- [Rules](/docs/spec/rules) — Rule syntax, semantics, and evaluation\n- [Queries](/docs/spec/queries) — Query syntax and execution model\n- [Grammar](/docs/spec/grammar) — Formal grammar definition\n- [Errors](/docs/spec/errors) — Error codes and diagnostics",
    "toc": []
  },
  "spec/queries": {
    "title": "Queries",
    "content": "# Queries\n\nQueries retrieve data from InputLayer. They return matching facts without modifying any data.\n\n## Basic Syntax\n\nQueries start with `?`:\n\n```datalog\n?edge(X, Y)\n```\n\n## Grammar\n\n```ebnf\nquery ::= \"?\" body ;\nbody  ::= literal (\",\" literal)* ;\n```\n\n## Query Patterns\n\n### Get All Data\n\nQuery all facts in a relation:\n\n```datalog\n?edge(X, Y)\n```\n\nResult:\n\n| X | Y |\n|---|---|\n| 1 | 2 |\n| 2 | 3 |\n| 3 | 4 |\n\n*3 rows*\n\n### Filter by Value\n\nFix one column to filter results:\n\n```datalog\n// Edges starting from node 1\n?edge(1, Y)\n```\n\nResult:\n\n| 1 | Y |\n|---|---|\n| 1 | 2 |\n\n*1 rows*\n\n### Check Existence\n\nQuery with all constants to check if a fact exists:\n\n```datalog\n?edge(1, 2)\n```\n\nIf the fact exists, you get a result with the constant values as columns. If it doesn't exist, you get `No results.`\n\n### Ignore Columns\n\nUse `_` (wildcard) to ignore columns you don't need:\n\n```datalog\n// Get all source nodes (ignore destination)\n?edge(X, _)\n```\n\nResult:\n\n| X |\n|---|\n| 1 |\n| 2 |\n| 3 |\n\n*3 rows*\n\n## Queries with Conditions\n\n### Comparisons\n\nFilter results with comparison operators:\n\n```datalog\n?person(Name, Age), Age > 25\n```\n\n### Multiple Conditions\n\nCombine conditions with commas (AND):\n\n```datalog\n?person(Name, Age), Age >= 18, Age <= 65\n```\n\n### Negation\n\nFind facts that don't match a pattern:\n\n```datalog\n// People who are not managers\n?employee(Id, Name), !manager(Id)\n```\n\n## Joins\n\n### Two-Way Join\n\nJoin two relations:\n\n```datalog\n?employee(Id, Name), works_in(Id, DeptId), department(DeptId, DeptName)\n```\n\nThis finds employees with their department names.\n\n### Self-Join\n\nJoin a relation with itself:\n\n```datalog\n// Find pairs where A follows B and B follows A (mutual follows)\n?follows(A, B), follows(B, A), A < B\n```\n\nThe `A < B` avoids duplicate pairs.\n\n## Querying Views\n\nQuery derived relations (views) the same way as base relations:\n\n```datalog\n// Define a rule\n+path(X, Y) <- edge(X, Y)\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n\n// Query the derived data\n?path(1, X)\n```\n\n## Computed Values\n\n### Arithmetic\n\nCompute values in queries:\n\n```datalog\n?product(Name, Price, Qty), Total = Price * Qty\n```\n\n### Vector Functions\n\nCompute distances or similarities:\n\n```datalog\n?vectors(Id1, V1), vectors(Id2, V2),\n   Id1 < Id2,\n   Dist = euclidean(V1, V2)\n```\n\n## Aggregations in Queries\n\nFor aggregations, define a rule first then query it:\n\n```datalog\n+dept_count(Dept, count<Id>) <- works_in(Id, Dept)\n?dept_count(Dept, Count)\n```\n\n## Result Format\n\nQuery results are displayed as formatted tables with column headers:\n\n| Name | Age |\n|---|---|\n| \"alice\" | 30 |\n| \"bob\" | 25 |\n\n*2 rows*\n\nIf no matches are found:\n\n```\nNo results.\n```\n\n## Examples\n\n### Simple Lookup\n\n```datalog\n+person[(\"alice\", 30), (\"bob\", 25), (\"charlie\", 35)]\n\n?person(\"alice\", Age)\n```\n\nResult:\n\n| \"alice\" | Age |\n|---|---|\n| \"alice\" | 30 |\n\n*1 rows*\n\n### Filtered Search\n\n```datalog\n+employee[(1, \"alice\", 75000), (2, \"bob\", 65000), (3, \"charlie\", 80000)]\n\n?employee(Id, Name, Salary), Salary > 70000\n```\n\nResult:\n\n| Id | Name | Salary |\n|---|---|---|\n| 1 | \"alice\" | 75000 |\n| 3 | \"charlie\" | 80000 |\n\n*2 rows*\n\n### Complex Query\n\n```datalog\n// Data\n+follows[(1, 2), (2, 3), (3, 1), (2, 1)]\n\n// Find mutual follows\n?follows(A, B), follows(B, A), A < B\n```\n\nResult:\n\n| A | B |\n|---|---|\n| 1 | 2 |\n| 1 | 3 |\n\n*2 rows*\n\n### Query with Aggregation\n\n```datalog\n+sales[(\"north\", 100), (\"north\", 200), (\"south\", 150)]\n\n// Define aggregation rule\n+total_by_region(Region, sum<Amount>) <- sales(Region, Amount)\n\n// Query it\n?total_by_region(Region, Total)\n```\n\nResult:\n\n| Region | Total |\n|---|---|\n| \"north\" | 300 |\n| \"south\" | 150 |\n\n*2 rows*",
    "toc": [
      {
        "level": 2,
        "text": "Basic Syntax",
        "id": "basic-syntax"
      },
      {
        "level": 2,
        "text": "Grammar",
        "id": "grammar"
      },
      {
        "level": 2,
        "text": "Query Patterns",
        "id": "query-patterns"
      },
      {
        "level": 3,
        "text": "Get All Data",
        "id": "get-all-data"
      },
      {
        "level": 3,
        "text": "Filter by Value",
        "id": "filter-by-value"
      },
      {
        "level": 3,
        "text": "Check Existence",
        "id": "check-existence"
      },
      {
        "level": 3,
        "text": "Ignore Columns",
        "id": "ignore-columns"
      },
      {
        "level": 2,
        "text": "Queries with Conditions",
        "id": "queries-with-conditions"
      },
      {
        "level": 3,
        "text": "Comparisons",
        "id": "comparisons"
      },
      {
        "level": 3,
        "text": "Multiple Conditions",
        "id": "multiple-conditions"
      },
      {
        "level": 3,
        "text": "Negation",
        "id": "negation"
      },
      {
        "level": 2,
        "text": "Joins",
        "id": "joins"
      },
      {
        "level": 3,
        "text": "Two-Way Join",
        "id": "two-way-join"
      },
      {
        "level": 3,
        "text": "Self-Join",
        "id": "self-join"
      },
      {
        "level": 2,
        "text": "Querying Views",
        "id": "querying-views"
      },
      {
        "level": 2,
        "text": "Computed Values",
        "id": "computed-values"
      },
      {
        "level": 3,
        "text": "Arithmetic",
        "id": "arithmetic"
      },
      {
        "level": 3,
        "text": "Vector Functions",
        "id": "vector-functions"
      },
      {
        "level": 2,
        "text": "Aggregations in Queries",
        "id": "aggregations-in-queries"
      },
      {
        "level": 2,
        "text": "Result Format",
        "id": "result-format"
      },
      {
        "level": 2,
        "text": "Examples",
        "id": "examples"
      },
      {
        "level": 3,
        "text": "Simple Lookup",
        "id": "simple-lookup"
      },
      {
        "level": 3,
        "text": "Filtered Search",
        "id": "filtered-search"
      },
      {
        "level": 3,
        "text": "Complex Query",
        "id": "complex-query"
      },
      {
        "level": 3,
        "text": "Query with Aggregation",
        "id": "query-with-aggregation"
      }
    ]
  },
  "spec/rules": {
    "title": "Rules",
    "content": "# Rules\n\nRules define derived relations (views) by computing new facts from existing data. InputLayer has two types of rules:\n\n| Type | Syntax | Persistence |\n|------|--------|-------------|\n| **Session rule** | `head <- body` | Temporary, cleared on disconnect |\n| **Persistent rule** | `+head <- body` | Saved to disk, survives restarts |\n\n## Session Rules\n\nSession rules exist only for your current connection:\n\n```datalog\n// Define a session rule\nancestor(X, Y) <- parent(X, Y)\nancestor(X, Z) <- ancestor(X, Y), parent(Y, Z)\n\n// Use it immediately\n?ancestor(\"alice\", X)\n\n// Rule is gone when you disconnect\n```\n\nSession rules are useful for:\n- Ad-hoc analysis\n- Experimenting with rule definitions\n- One-time computations\n\n### Managing Session Rules\n\n```\n.session              // List all session rules\n.session clear        // Clear all session rules\n.session drop 1       // Drop session rule #1\n```\n\n## Persistent Rules\n\nPersistent rules are saved to disk and automatically loaded:\n\n```datalog\n// Create a persistent rule (saved)\n+path(X, Y) <- edge(X, Y)\n+path(X, Z) <- path(X, Y), edge(Y, Z)\n\n// Works immediately and after restart\n?path(1, X)\n```\n\nPersistent rules are stored in `{data_dir}/{kg}/rules/catalog.json`.\n\n### Multi-Clause Rules\n\nA view can be defined by multiple rules (clauses):\n\n```datalog\n// First clause: base case\n+ancestor(X, Y) <- parent(X, Y)\n\n// Second clause: recursive case\n+ancestor(X, Z) <- parent(X, Y), ancestor(Y, Z)\n```\n\nBoth clauses define `ancestor`. Results are the union of all matching clauses.\n\n### Deleting Persistent Rules\n\nRemove a persistent rule with `-`:\n\n```datalog\n-path(X, Y)    // Removes all rules defining 'path'\n```\n\nOr use meta commands:\n\n```\n.rule drop path        // Drop 'path' rule\n.rule clear path       // Clear for re-registration\n```\n\n## Rule Structure\n\nA rule has two parts:\n\n```\nhead(Variables) <- body\n     ^              ^\n  What to         What must\n  produce         be true\n```\n\n### Head\n\nThe head is an atom that defines what the rule produces:\n\n```datalog\n+path(X, Y) <- edge(X, Y)\n// ^^^^^^^^\n// Head: produces facts for 'path' relation\n```\n\nThe head must use a **lowercase** predicate name.\n\n### Body\n\nThe body specifies conditions using literals separated by commas (AND):\n\n```datalog\n+employee_in_dept(Name, Dept) <- employee(Id, Name), works_in(Id, Dept)\n//                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n//                                Body: both conditions must match\n```\n\n## Grammar\n\n```ebnf\nsession_rule    ::= head \"<-\" body ;\npersistent_rule ::= \"+\" head \"<-\" body ;\ndelete_rule     ::= \"-\" predicate \"(\" variables \")\" ;\nhead            ::= predicate \"(\" term (\",\" term)* \")\" ;\nbody            ::= literal (\",\" literal)* ;\n```\n\n## Body Literals\n\nThe body can contain:\n\n| Literal | Example | Meaning |\n|---------|---------|---------|\n| Positive atom | `edge(X, Y)` | Relation must have matching fact |\n| Negated atom | `!edge(X, Y)` | Relation must NOT have matching fact |\n| Comparison | `X > 10` | Condition must be true |\n| Arithmetic | `Z = X + Y` | Compute and bind value |\n| Aggregation | `count<X>` | Aggregate over variable |\n\n## Recursion\n\nRules can reference themselves (recursion):\n\n```datalog\n// Transitive closure - find all reachable nodes\n+reach(X, Y) <- edge(X, Y)\n+reach(X, Z) <- reach(X, Y), edge(Y, Z)\n```\n\nInputLayer handles recursion automatically. It keeps computing until no new facts are found (fixpoint).\n\n### Mutual Recursion\n\nRules can reference each other:\n\n```datalog\n+even(0)\n+odd(X) <- even(Y), succ(Y, X)\n+even(X) <- odd(Y), succ(Y, X)\n```\n\n## Variable Safety Rules\n\nAll variables in the **head** must appear in a **positive** body literal:\n\n```datalog\n// GOOD: X appears in edge(X, Y)\n+start_nodes(X) <- edge(X, Y)\n\n// BAD: X only in head, not in positive body literal\n+broken(X) <- edge(A, B)   // ERROR: X is unbound\n```\n\nVariables in **negations** or **comparisons** must also appear in a positive literal:\n\n```datalog\n// GOOD: X is bound by edge, then filtered\n+filtered(X) <- edge(X, Y), X > 5\n\n// BAD: X only in comparison\n+broken(X) <- Y > 5   // ERROR: X unbound, Y only in comparison\n```\n\n## Common Patterns\n\n### Filter\n\n```datalog\n+adults(Name, Age) <- person(Name, Age), Age >= 18\n```\n\n### Join\n\n```datalog\n+employee_dept(Name, DeptName) <-\n    employee(Id, Name),\n    works_in(Id, DeptId),\n    department(DeptId, DeptName)\n```\n\n### Set Difference\n\n```datalog\n+not_in_b(X) <- a(X), !b(X)\n```\n\n### Aggregation\n\n```datalog\n+dept_count(Dept, count<Id>) <- works_in(Id, Dept)\n```\n\n## Examples\n\n### Social Network\n\n```datalog\n// Facts\n+follows[(1, 2), (2, 3), (3, 4), (2, 4)]\n\n// Find who user 1 can reach (transitive)\n+can_reach(X, Y) <- follows(X, Y)\n+can_reach(X, Z) <- can_reach(X, Y), follows(Y, Z)\n\n// Query\n?can_reach(1, X)\n```\n\n### Access Control\n\n```datalog\n// Facts\n+user_role[(\"alice\", \"admin\"), (\"bob\", \"viewer\")]\n+role_perm[(\"admin\", \"read\"), (\"admin\", \"write\"), (\"viewer\", \"read\")]\n\n// Rule: user has permission via role\n+has_perm(User, Perm) <- user_role(User, Role), role_perm(Role, Perm)\n\n// Query\n?has_perm(\"alice\", Perm)\n```\n\n### Graph Analysis\n\n```datalog\n// Facts\n+edge[(1, 2), (2, 3), (3, 1), (4, 5)]\n\n// Nodes in same strongly connected component\n+same_scc(X, Y) <- reach(X, Y), reach(Y, X)\n+reach(X, Y) <- edge(X, Y)\n+reach(X, Z) <- reach(X, Y), edge(Y, Z)\n\n// Query\n?same_scc(1, X)\n```",
    "toc": [
      {
        "level": 2,
        "text": "Session Rules",
        "id": "session-rules"
      },
      {
        "level": 3,
        "text": "Managing Session Rules",
        "id": "managing-session-rules"
      },
      {
        "level": 2,
        "text": "Persistent Rules",
        "id": "persistent-rules"
      },
      {
        "level": 3,
        "text": "Multi-Clause Rules",
        "id": "multi-clause-rules"
      },
      {
        "level": 3,
        "text": "Deleting Persistent Rules",
        "id": "deleting-persistent-rules"
      },
      {
        "level": 2,
        "text": "Rule Structure",
        "id": "rule-structure"
      },
      {
        "level": 3,
        "text": "Head",
        "id": "head"
      },
      {
        "level": 3,
        "text": "Body",
        "id": "body"
      },
      {
        "level": 2,
        "text": "Grammar",
        "id": "grammar"
      },
      {
        "level": 2,
        "text": "Body Literals",
        "id": "body-literals"
      },
      {
        "level": 2,
        "text": "Recursion",
        "id": "recursion"
      },
      {
        "level": 3,
        "text": "Mutual Recursion",
        "id": "mutual-recursion"
      },
      {
        "level": 2,
        "text": "Variable Safety Rules",
        "id": "variable-safety-rules"
      },
      {
        "level": 2,
        "text": "Common Patterns",
        "id": "common-patterns"
      },
      {
        "level": 3,
        "text": "Filter",
        "id": "filter"
      },
      {
        "level": 3,
        "text": "Join",
        "id": "join"
      },
      {
        "level": 3,
        "text": "Set Difference",
        "id": "set-difference"
      },
      {
        "level": 3,
        "text": "Aggregation",
        "id": "aggregation"
      },
      {
        "level": 2,
        "text": "Examples",
        "id": "examples"
      },
      {
        "level": 3,
        "text": "Social Network",
        "id": "social-network"
      },
      {
        "level": 3,
        "text": "Access Control",
        "id": "access-control"
      },
      {
        "level": 3,
        "text": "Graph Analysis",
        "id": "graph-analysis"
      }
    ]
  },
  "spec/types": {
    "title": "Data Types",
    "content": "# Data Types\n\nInputLayer supports several data types for constants.\n\n## Type Summary\n\n| Type | Syntax | Examples |\n|------|--------|----------|\n| Integer | digits | `1`, `-42`, `1000000` |\n| Float | digits with decimal | `3.14`, `-0.5`, `1e10` |\n| String | double quotes | `\"hello\"`, `\"alice\"` |\n| Boolean | true/false | `true`, `false` |\n| Vector | brackets | `[1.0, 2.0, 3.0]` |\n| Timestamp | Unix milliseconds | `1704067200000` |\n\n**Schema hint**: `symbol` — interned string optimization (see [Symbols](#symbols) below).\n\n## Integers\n\n64-bit signed integers:\n\n```datalog\n42\n-17\n0\n1000000\n-9223372036854775808   // Minimum\n9223372036854775807    // Maximum\n```\n\n### Grammar\n\n```ebnf\ninteger ::= \"-\"? [0-9]+ ;\n```\n\n## Floats\n\n64-bit floating-point numbers:\n\n```datalog\n3.14\n-0.5\n0.0\n1.23e10\n-4.56e-7\n```\n\n### Grammar\n\n```ebnf\nfloat ::= \"-\"? [0-9]+ \".\" [0-9]+ ( (\"e\" | \"E\") \"-\"? [0-9]+ )? ;\n```\n\n## Strings\n\nUTF-8 text enclosed in double quotes:\n\n```datalog\n\"hello\"\n\"Alice Smith\"\n\"with spaces and punctuation!\"\n\"\"                    // Empty string\n```\n\n### Escape Sequences\n\n| Sequence | Meaning |\n|----------|---------|\n| `\\\"` | Double quote |\n| `\\\\` | Backslash |\n| `\\n` | Newline |\n| `\\t` | Tab |\n| `\\r` | Carriage return |\n\n```datalog\n+\"She said \\\"hello\\\"\"\n+\"Line 1\\nLine 2\"\n+\"Path: C:\\\\Users\\\\alice\"\n```\n\n### Grammar\n\n```ebnf\nstring ::= '\"' ( [^\"\\\\] | escape )* '\"' ;\nescape ::= '\\\\' ( '\"' | '\\\\' | 'n' | 't' | 'r' ) ;\n```\n\n## Booleans\n\nBoolean true or false:\n\n```datalog\ntrue\nfalse\n```\n\n### Grammar\n\n```ebnf\nboolean ::= \"true\" | \"false\" ;\n```\n\n## Vectors\n\nArrays of floating-point numbers:\n\n```datalog\n[1.0, 2.0, 3.0]\n[0.5, -0.5]\n[]                  // Empty vector\n```\n\nVectors are used for embeddings and similarity search:\n\n```datalog\n+vectors(1, [1.0, 0.0, 0.0])\n+vectors(2, [0.0, 1.0, 0.0])\n\n?vectors(Id1, V1), vectors(Id2, V2),\n   Dist = euclidean(V1, V2)\n```\n\n### Grammar\n\n```ebnf\nvector ::= \"[\" ( float ( \",\" float )* )? \"]\" ;\n```\n\n## Timestamps\n\nUnix timestamps in milliseconds since epoch (1970-01-01 00:00:00 UTC):\n\n```datalog\n1704067200000           // 2024-01-01 00:00:00 UTC\n```\n\nTimestamps are used with temporal functions:\n\n```datalog\n+events(1, 1704067200000)\n+events(2, 1704153600000)\n\n// Find events from last 24 hours\n?events(Id, Ts),\n   Now = time_now(),\n   within_last(Ts, Now, 86400000)\n```\n\nTimestamps are stored as 64-bit integers internally.\n\n### Schema Declaration\n\n```datalog\n+event_log(id: int, occurred_at: timestamp, message: string)\n```\n\nAliases: `timestamp`, `time`, `datetime`\n\n## Symbols\n\nSymbols are interned strings optimized for frequent comparisons (like identifiers or tags):\n\n```datalog\n+user(1, \"alice\", \"admin\")     // \"admin\" as string\n```\n\n### Schema Declaration\n\nUse the `symbol` type when a column contains a small set of repeated values:\n\n```datalog\n+user(id: int, name: string, role: symbol)\n```\n\nSymbols provide better performance for equality comparisons and use less memory when values repeat frequently.\n\n**Note**: At the data level, symbols appear as strings. The `symbol` type is a schema hint for optimization.\n\n## Type Coercion\n\nInputLayer performs limited automatic type coercion:\n\n| From | To | Automatic? |\n|------|-----|------------|\n| Integer | Float | Yes (in arithmetic) |\n| Others | Any | No |\n\n```datalog\n// Integer + Float = Float\n?data(X, Y), Z = X + 3.14  // X (int) coerced to float\n```\n\n## Type in Schemas\n\nWhen declaring schemas, use these type names:\n\n```datalog\n+employee(\n    id: int,\n    name: string,\n    salary: float,\n    active: bool\n)\n```\n\n| Type Keyword | Aliases | Accepts |\n|--------------|---------|---------|\n| `int` | `integer`, `i32`, `i64` | Integers |\n| `float` | `double`, `f64`, `number` | Floats |\n| `string` | `str`, `text` | Strings |\n| `bool` | `boolean` | true, false |\n| `vector` | `embedding`, `vec` | Vector arrays |\n| `timestamp` | `time`, `datetime` | Unix milliseconds |\n| `symbol` | - | Interned strings (schema hint) |\n\n## Examples\n\n### Mixed Types\n\n```datalog\n+product(1, \"Widget\", 19.99, true)\n+product(2, \"Gadget\", 29.99, false)\n```\n\n### Vectors for Similarity\n\n```datalog\n+embedding[(1, [0.1, 0.2, 0.3]), (2, [0.2, 0.3, 0.4])]\n\n?embedding(Id1, V1), embedding(Id2, V2),\n   Id1 < Id2,\n   Sim = cosine(V1, V2),\n   Sim > 0.9\n```\n\n### String Comparisons\n\n```datalog\n+person[(\"alice\", \"engineering\"), (\"bob\", \"sales\")]\n\n?person(Name, Dept), Dept = \"engineering\"\n```",
    "toc": [
      {
        "level": 2,
        "text": "Type Summary",
        "id": "type-summary"
      },
      {
        "level": 2,
        "text": "Integers",
        "id": "integers"
      },
      {
        "level": 3,
        "text": "Grammar",
        "id": "grammar"
      },
      {
        "level": 2,
        "text": "Floats",
        "id": "floats"
      },
      {
        "level": 3,
        "text": "Grammar",
        "id": "grammar"
      },
      {
        "level": 2,
        "text": "Strings",
        "id": "strings"
      },
      {
        "level": 3,
        "text": "Escape Sequences",
        "id": "escape-sequences"
      },
      {
        "level": 3,
        "text": "Grammar",
        "id": "grammar"
      },
      {
        "level": 2,
        "text": "Booleans",
        "id": "booleans"
      },
      {
        "level": 3,
        "text": "Grammar",
        "id": "grammar"
      },
      {
        "level": 2,
        "text": "Vectors",
        "id": "vectors"
      },
      {
        "level": 3,
        "text": "Grammar",
        "id": "grammar"
      },
      {
        "level": 2,
        "text": "Timestamps",
        "id": "timestamps"
      },
      {
        "level": 3,
        "text": "Schema Declaration",
        "id": "schema-declaration"
      },
      {
        "level": 2,
        "text": "Symbols",
        "id": "symbols"
      },
      {
        "level": 3,
        "text": "Schema Declaration",
        "id": "schema-declaration"
      },
      {
        "level": 2,
        "text": "Type Coercion",
        "id": "type-coercion"
      },
      {
        "level": 2,
        "text": "Type in Schemas",
        "id": "type-in-schemas"
      },
      {
        "level": 2,
        "text": "Examples",
        "id": "examples"
      },
      {
        "level": 3,
        "text": "Mixed Types",
        "id": "mixed-types"
      },
      {
        "level": 3,
        "text": "Vectors for Similarity",
        "id": "vectors-for-similarity"
      },
      {
        "level": 3,
        "text": "String Comparisons",
        "id": "string-comparisons"
      }
    ]
  }
}
